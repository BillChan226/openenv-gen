# Progress
## Completed Features
- [x] Build a React + Vite frontend that matches the Jir...
- [x] Implement PostgreSQL schema and seed data to suppo...
- [x] Create the design documentation that fully specifi...
- [x] Initialized project and memory bank
- [x] Design docs present: `design/api_contracts.md`, `design/components.md`, `design/spec.json`
- [x] `design/spec.json` updated to be self-contained (no JSON `$ref`) and to cover required routes + UX states

## In Progress
- [ ] (auto-updated during generation)

## Known Issues
- [ ] FAILED: Create Docker configuration to run database, backe...
- [ ] docker: Potential repo/config risk (not runtime-verified): backend healthcheck uses `wget` inside the backend container. The backend image is `node:20-alpine`; if `wget` is not present in that base image, the container healthcheck will fail even if the server is healthy. (Same risk for frontend nginx healthcheck, though nginx:alpine typically includes wget/busybox wget.) This needs confirmation by actually building/running.
- [ ] docker: BLOCKER (environment): Docker daemon is not running/accessible. `docker_build` fails with: `Cannot connect to the Docker daemon at unix:///Users/thb/.docker/run/docker.sock` and `docker version` reports it cannot connect to the Docker API socket. Because of this, I cannot verify that images build, containers start, healthchecks pass, or that the UI works end-to-end under Docker.
- [ ] docker: Potential functional risk (needs runtime confirmation): prod-like frontend uses nginx reverse proxy to `http://backend:8000/api/`. This assumes the frontend is built with `VITE_API_BASE_URL=/api` and that the backend serves routes under `/api/*`. If the backend routes differ, frontend API calls will fail.
- [ ] docker: Potential functional risk (needs runtime confirmation): db_init runs `psql -f /init/01_schema.sql -f /init/02_seed.sql` and comments claim idempotency. If the SQL is not actually idempotent, repeated `docker compose up` may fail (especially if volumes persist). Needs runtime verification.
- [ ] docker: Potential functional risk (needs runtime confirmation): backend.Dockerfile installs production deps only (`npm ci --omit=dev`) and runs `npm start`. If the backend start script relies on devDependencies (e.g., nodemon/ts-node) or requires a build step, the container will crash at runtime. This must be validated once Docker is available.
- [ ] docker: BLOCKER (environment): Docker daemon is not reachable from the verification environment. `docker_build` fails with: `Cannot connect to the Docker daemon at unix:///Users/thb/.docker/run/docker.sock` and `docker version` reports the same. Because of this, runtime acceptance criteria (build, up, /health, browser flow) cannot be executed.
- [ ] docker: Potential functional gap vs requirements: requirement mentions 'backend with migrations/seed on startup (or explicit init step)'. This implementation uses a separate `db_init` service running SQL scripts, not backend migrations. That may be acceptable, but must be confirmed that the backend does not also require its own migration step.
- [ ] docker: Potential repo-level risk to double-check once Docker is available: backend container runs `npm start` with production deps only (`npm ci --omit=dev`). If the backend's start script relies on devDependencies (e.g., nodemon/ts-node), the container will fail at runtime. Needs confirmation by actually building/running.
- [ ] docker: Hard blocker: Docker daemon not available in the execution environment. `docker compose ... build` fails with: `Cannot connect to the Docker daemon at unix:///Users/thb/.docker/run/docker.sock` and `docker version` reports it cannot connect to the Docker API. This prevents verifying build/run/health/UI flows.
- [ ] docker: Acceptance criteria not verifiable without runtime: no evidence that backend runs migrations/seed on startup beyond db_init running SQL scripts; cannot confirm backend actually connects and serves /health, nor that frontend can authenticate and navigate.
- [ ] docker: Path/layout inconsistency: docker/README.md says canonical compose files live under `jira/docker/docker-compose.yml`, but when running from the `jira/` repo root the compose files are actually at `docker/docker-compose.yml` and `docker/docker-compose.dev.yml` (as evidenced by the successful yaml.safe_load only when using those paths). This mismatch will confuse users and automation.
- [ ] docker: BLOCKER (environment): Docker daemon is not available. `docker_build()` fails with: Cannot connect to the Docker daemon at unix:///Users/thb/.docker/run/docker.sock. `docker version` also fails to connect. This prevents validating acceptance criteria that require building and running containers.
- [ ] docker: Potential functional gap (needs runtime confirmation): Requirement mentions 'backend with migrations/seed on startup (or explicit init step)'. Current setup relies on Postgres init scripts (schema/seed) executed only on first DB initialization. If the `postgres_data` volume already exists, seed/migrations will not re-run. There is no explicit backend init/migrate step in compose. This may cause inconsistent startup in dev if volume persists with partial state.
- [ ] docker: BLOCKER (environment): Docker is not available to run verification. `docker version` fails with: `failed to connect to the docker API at unix:///Users/thb/.docker/run/docker.sock ... no such file or directory`. Because of this, I cannot execute `docker compose up`, cannot confirm images build, cannot confirm healthchecks/startup ordering in practice, and cannot run browser-based end-to-end checks against the dockerized stack.
- [ ] docker: Potential missing env wiring: frontend build arg VITE_API_BASE_URL is passed in compose, but frontend codebase appears to rely on Vite proxy to /api (vite.config.js) rather than reading VITE_API_BASE_URL (grep found no usage). This may be fine, but if the app expects an env-based API base URL at runtime, it may not work as intended.
- [ ] docker: Potential config mismatch: docker-compose.yml sets FRONTEND_URL=http://localhost:3000 but the frontend is exposed on 5173 in both dev and prod-like compose. If backend CORS uses FRONTEND_URL, production-like stack may reject browser requests.
- [ ] docker: Potential functional gap vs requirements: backend service does not show an explicit migrations/seed-on-start step in docker/backend.Dockerfile or compose. It relies on Postgres init scripts (first-run only). If the backend requires migrations on every start (or when schema changes), this may be insufficient.
- [ ] docker: BLOCKER (environment): Docker daemon is not reachable in this verification environment. docker_build() failed with: "Cannot connect to the Docker daemon at unix:///Users/thb/.docker/run/docker.sock. Is the docker daemon running?". This prevents validating build/run/health/browser acceptance criteria.
- [ ] docker: Potential config risk to re-check once Docker is available: dev frontend maps host 5173 -> container 3000. This is only correct if the dev image actually runs Vite on port 3000 inside the container (many Vite setups default to 5173). If the container listens on 5173, this mapping will break frontend access.
- [ ] docker: BLOCKER (environment): `docker_build()` failed with: `Cannot connect to the Docker daemon at unix:///Users/thb/.docker/run/docker.sock. Is the docker daemon running?` This prevents any runtime verification of the Docker configuration and all acceptance criteria that require running containers.
- [ ] docker: Frontend dev compose uses `VITE_API_BASE_URL: /api` but there is no explicit Vite dev-server proxy configuration shown here. If the frontend code does not proxy `/api` to backend, browser calls to `/api` on port 5173 will 404. (Prod nginx.conf does proxy /api to backend, dev does not unless Vite config handles it.)
- [ ] docker: Port mapping mismatch with common expectation: backend is exposed as `3001:8000` (not `8000:8000`). This is fine if documented, but acceptance criteria says 'Backend /health returns 200 from outside the containers'—testers may assume `localhost:8000/health`. README should be explicit (or map 8000:8000).
- [ ] docker: Backend migrations/seed on startup not evidenced in docker config: backend container runs `npm start` only. Unless `npm start` internally runs migrations/seed, the requirement 'migrations/seed on startup (or explicit init step)' may not be met. Needs confirmation in backend package scripts or an explicit init service/command.
- [ ] docker: Potential spec gap vs requirements: No database Dockerfile was created (requirement mentions 'Dockerfiles for frontend, backend, and database initialization'). Current approach uses `postgres:16-alpine` with init scripts mounted from `../app/database/init`. If a DB Dockerfile is required, it is missing.
- [ ] docker: BLOCKER (environment): `docker_build` fails with: `Cannot connect to the Docker daemon at unix:///Users/thb/.docker/run/docker.sock. Is the docker daemon running?` This prevents executing the acceptance criteria that require building and running containers.
- [ ] docker: Because compose files are missing, the following requirements are unverified and currently unmet from an execution standpoint: postgres service with init scripts, backend startup ordering/migrations/seed, frontend dev/prod modes, healthchecks, port exposure, and end-to-end login/dashboard/project navigation against dockerized services.
- [ ] docker: Documentation mismatch: docker/README.md claims canonical compose files live under `jira/docker/docker-compose.yml` and `jira/docker/docker-compose.dev.yml`, but the `jira/` directory in this workspace only contains `jira/env/` and has no `jira/docker/` at all.
- [ ] docker: CRITICAL: No docker-compose.yml or docker-compose.dev.yml exists in the repo. `find . -maxdepth 4 -name 'docker-compose*.yml'` returns nothing, and `docker_up(build=true)` fails with 'docker-compose.yml not found'. This blocks all acceptance criteria.
- [x] docker: Compose file duplication removed; canonical compose files are under `jira/docker/` only.
- [x] docker: Removed obsolete `version:` key from compose files to avoid warnings.
- [ ] docker: BLOCKER (environment): `docker_build()` fails with: `Cannot connect to the Docker daemon at unix:///Users/thb/.docker/run/docker.sock. Is the docker daemon running?` This prevents verifying all acceptance criteria (build, compose up, /health 200, frontend login/dashboard/project views).
- [ ] FAILED: Implement the OpenEnv adapter to enable automated ...
- [ ] env: Smoke test could not reach backend at http://localhost:8000/health in `--no-start` mode (connection refused). Without a running environment, the adapter’s runtime API helper methods cannot be validated end-to-end.
- [ ] env: In `jira/env/client.py` and `jira/env/smoke_test.py`, the file viewer shows literal `[REDACTED]` tokens in code (e.g., setting AuthConfig password). If those are actually present in the real file content, they would be invalid Python; compilation currently passes, which suggests either (a) the viewer is redacting dynamically, or (b) there is inconsistent content between paths. This needs clarification because it affects runtime login fallback behavior.
- [ ] env: There are TWO parallel implementations of the env adapter: `env/` and `jira/env/`. The richer smoke test (with --timeout, create/update/comment flow) appears to be in `env/smoke_test.py`, while `jira/env/smoke_test.py` is a different/older CLI (no --timeout) and likely different behavior. This duplication makes the target (`jira/env/`) ambiguous and breaks expected usage.
- [ ] env: Potentially broken committed code: env/smoke_test.py and env/client.py display `password=[REDACTED]` in the source (around login/auth handling). That is not valid Python unless it is inside a string/comment, and it strongly suggests the file content is sanitized/incorrect. Even if py_compile succeeded here, the repository version likely needs to store the real variable name (e.g., `password=self.auth.password`) and never a placeholder token.
- [ ] env: Config/profile support gap: requirements state configuration is driven by config.yaml and supports local + docker deployments. However, env/smoke_test.py has no `--profile` argument, and the attempted run `python3 env/smoke_test.py --profile local ...` fails with `unrecognized arguments: --profile local`. If config.yaml contains multiple profiles, there is no CLI path to select them for smoke testing.
- [ ] env: Docker context verification blocked: after freeing ports and retrying, `docker compose ... up` failed with `Cannot connect to the Docker daemon at unix:///Users/thb/.docker/run/docker.sock. Is the docker daemon running?`. This prevents validating the acceptance criteria for docker deployments and service manager start/stop behavior.
- [ ] env: Because docker could not start, I could not validate: backend /health readiness via adapter, login flow, project/issue fetch, create/update/comment operations, or frontend behavior. No browser/UI checks were possible.
- [ ] env: env/smoke_test.py provides no CLI option to skip starting services (connect to already-running services). Attempting `--no-start` fails with `unrecognized arguments: --no-start`. This prevents using the adapter in environments where services are managed externally (a stated requirement: start/stop OR connect).
- [ ] env: env/smoke_test.py is not a real smoke test for the acceptance criteria. It only starts services and calls `client.health()`. It does not test: login, authenticated API calls, create issue, update status, add comment.
- [ ] env: docker-compose.yml maps Postgres to host port 5432 ("5432:5432"). On this machine, port 5432 is already in use by Docker Desktop (lsof shows `com.docke ... TCP *:postgresql (LISTEN)`), causing `docker compose up` to fail with: `Bind for 0.0.0.0:5432 failed: port is already allocated`. This blocks docker-context verification and violates the requirement that config works in docker contexts (it should not assume 5432 is free).
- [ ] env: BLOCKER for acceptance criteria: No successful run was possible to confirm: backend /health readiness, authenticated login, create issue, update status, add comment. These require a running backend; the adapter’s docker start path currently fails.
- [ ] env: CRITICAL: jira/env/smoke_test.py calls `JiraServiceManager(config_path=args.config)` and `sm.ensure_running()`, but jira/env/service_manager.py defines `JiraServiceManager.__init__(config: OpenEnvConfig)` and `start()/stop()/status()/wait_until_healthy()`. The smoke test is not aligned with the implemented adapter API, so even if docker compose were valid, the smoke test logic is incorrect/outdated.
- [ ] env: CRITICAL: `python3 -m jira.env.smoke_test --config jira/env/config.yaml` fails immediately trying to run docker compose: `service "backend" has neither an image nor a build context specified: invalid compose project`. This prevents any automated verification of /health, login, and CRUD flows in docker mode.
- [ ] env: The task report lists files under `env/...` but also claims files under `jira/env/...` and shows a failed compile for `jira/env/...`. This indicates packaging/path inconsistency that should be resolved (either move/duplicate package into `jira/env/` or update task target and any tooling accordingly).
- [ ] env: Directory mismatch vs requirements: TASK_INFO target is `jira/env/`, but the implemented adapter package is `env/` (repo root). As a result, `python3 -m py_compile jira/env/__init__.py ...` fails with FileNotFoundError. This is a hard FAIL against the stated target and will break any automation expecting `jira/env/`.
- [ ] env: P2: Docker-context configuration not verified in this run. env/config.yaml docker profile uses container DNS names (http://backend:8000, http://frontend:3000). This is fine for container-to-container, but from host-based test runner it will fail unless env overrides are implemented/used. Need explicit verification that overrides exist and work (or provide a host-accessible docker profile).
- [ ] env: P1: env/smoke_test.py cannot be run directly despite being a common usage pattern. `python3 env/smoke_test.py --profile local` fails: ImportError: attempted relative import with no known parent package. Only `python3 -m env.smoke_test` works. README/doc should either enforce module execution or smoke_test should support script execution (e.g., adjust sys.path or use absolute imports).
- [ ] env: P0: Issue status update does not actually change status. Repro: create issue, call PATCH /api/issues/{id} with {"status":"IN_PROGRESS"} (also tried "DONE", "IN PROGRESS", "IN-PROGRESS"); response includes activity metadata fields:['status'] but returned issue.status stays 'TODO', and GET /api/issues/{id} also shows 'TODO'. This breaks acceptance criterion: 'create an issue, update its status'. Likely backend ignores/validates status silently or maps statuses differently; adapter currently assumes status strings like IN_PROGRESS/DONE.
- [ ] env: Config portability concern (docker context): env/config.yaml uses `http://localhost:3000` and `http://localhost:8000` for both local and docker profiles. This may violate the 'no hard-coded localhost-only assumptions' requirement for non-local execution contexts (e.g., remote docker host, CI, container-to-container). The code supports env var overrides, but the default config is still localhost-bound.
- [ ] env: API ergonomics mismatch (minor): JiraEnvClient.create_issue only accepts a single `payload` dict; it does not accept keyword args like `projectKey=...`. This is not a functional failure, but it is easy to misuse and caused a TypeError during QA until corrected.
- [ ] env: Pathing/verification inconsistency: running `python3 -m py_compile jira/env/__init__.py ...` from within the `jira/` directory fails with FileNotFoundError because it resolves to `jira/jira/env/__init__.py`. The correct invocation from repo root is `python3 -m py_compile env/__init__.py env/client.py env/service_manager.py env/smoke_test.py` (or run the jira/env/... command from the parent directory). This caused the initial reported 'FAILED' compile result.
- [ ] env: Smoke test is not implemented; `env/smoke_test.py` is only a wrapper that imports `main` from `env.smoke_test` (itself), so there is no real smoke test logic to validate acceptance criteria.
- [ ] env: Service management (start/stop local processes or docker compose) is not implemented; `env/service_manager.py` is also just a wrapper.
- [ ] env: The required helper methods (health checks, login, fetching projects/issues, creating issues, updating status, comment operations) are not present anywhere in `env/` (confirmed by grep).
- [ ] env: Importing `env` fails immediately: `ImportError: cannot import name 'AuthConfig' from 'env.client' (/.../env/client.py)` because `env/__init__.py` expects `AuthConfig, EnvUrls, JiraEnvClient, OpenEnvError` to exist, but `env/client.py` defines none.
- [ ] env: CRITICAL: The env adapter is not implemented. Files under top-level `env/` are only 6-line wrappers that import from `env.client` / `env.service_manager`, but those modules resolve to the same wrapper files, causing missing symbols and import failure.
- [ ] env: MINOR (environment): Smoke test output shows urllib3 NotOpenSSLWarning due to LibreSSL; not a functional failure, but noisy for automation logs.
- [ ] env: MAJOR: The provided COMMAND_RESULTS claiming 'python3 -m py_compile jira/env/...' succeeded is not reproducible in this workspace because there is no jira/env directory. Running that command fails with FileNotFoundError. This indicates the adapter is not located at the required target path.
- [ ] env: CRITICAL: env/smoke_test.py imports from 'env.client' and 'env.service_manager' but its docstring suggests running 'python3 -m jira.env.smoke_test'. In this repo, the package is 'env', not 'jira.env'. This mismatch will confuse automation and breaks the stated TASK_INFO target (jira/env/).
- [ ] env: CRITICAL: env.client.JiraEnvClient.list_projects() only accepts response shapes: {'projects': [...]} or a raw list. The actual backend returns {'items': [...]} (observed in smoke test output). This causes smoke tests (and any consumer) to fail before issue creation/status/comment steps, violating acceptance criteria.
- [ ] env: Environment readiness and authenticated API flows were not verifiable because the smoke test runner cannot start due to the import/package error, and no running backend/frontend were provided to connect to.
- [ ] env: Potential misconfiguration/bug: An ad-hoc call to JiraEnvClient.health() raised `ValueError Timeout value connect was AuthConfig(...)` (suggesting a wrong type may be getting passed into requests timeout somewhere in some code path or due to an invocation mistake). This needs confirmation once services can actually be started and smoke tests run.
- [ ] env: CRITICAL: The provided command results show confusion between paths `jira/env/...` and `env/...`. The adapter appears to live at `jira/env/` but is only importable as `env.*` when running from the `jira/` directory. This breaks the requirement that the adapter be usable for automated interaction/testing.
- [ ] env: CRITICAL: Package/import mismatch. Files are located under jira/env/, but smoke_test.py imports using `from jira.env...`. The workspace does not contain a top-level Python package named `jira` (no jira/__init__.py), so `python3 env/smoke_test.py ...` and `import env.smoke_test` both fail with `ModuleNotFoundError: No module named 'jira'`.
- [ ] frontend: P1: Modal close control could not be located via data-testid='modal-close'. If the modal has a close button, it should be discoverable and testable; requirement also calls for keyboard (Escape) support (Escape worked to close backdrop visibility, but close button test remains unverified).
- [ ] frontend: P1: Toast notification did not appear after creating an issue (waited for [data-testid='toast'], .toast for 10s). Either toast is not implemented for this action, uses different selectors, or the create action failed silently. Requirement calls for toast notifications for create/update/delete.
- [ ] frontend: P0: On initial load and throughout flows, the frontend makes GET http://localhost:3000/api/users which returns 404. This produces a browser console error ('Failed to load resource') and a network error entry. This violates the requirement 'no failing network requests (except intentional auth)' and the PASS criteria.
- [ ] frontend: Create Issue modal: clicking the backdrop to close fails. Playwright reports the backdrop element exists and is visible/enabled, but the form subtree intercepts pointer events ("<form ...> intercepts pointer events"). This violates the requirement: 'Test clicking outside to close'. Fix suggestion: ensure backdrop is behind the modal content (z-index) and receives pointer events; modal content should not overlap the backdrop; or implement outside-click handler on overlay container rather than relying on a full-screen button that can be covered by the modal panel.
- [ ] frontend: After logout, navigating to /dashboard triggers GET /api/auth/me returning 401 (Unauthorized) and logs console errors. While 401 may be expected when logged out, acceptance criteria require no unexpected network errors during normal flows; the app should avoid firing /auth/me repeatedly or should handle it without console error noise.
- [ ] frontend: Create Project is broken: submitting the Create Project modal triggers POST http://localhost:3000/api/projects which returns 400 (Bad Request). UI shows validation message 'Project key already exists' and the dashboard remains with 'No projects'. This blocks end-to-end project creation and prevents testing downstream project/issue workflows.
- [ ] frontend: Console/network errors currently fail the 'no console errors / no failing network requests during normal flows' requirement. Even if 401s are 'intentional auth', POST /auth/login should not be 401 for the provided demo credentials.
- [ ] frontend: Auth/session mismatch symptoms: backend logs also show 'Invalid token' and 'Missing token' for authenticated endpoints (e.g., GET /api/projects). Frontend stores token in localStorage and sends Authorization: Bearer <token> plus credentials: include. If backend expects cookie-based auth or a different token format/secret, subsequent requests will 401 even after login.
- [ ] frontend: BLOCKER: Backend logs show repeated 'Invalid email or password' for /api/auth/login, indicating the seeded/demo user credentials expected by the frontend do not match backend data. Without a working demo account, all authenticated workflows are untestable.
- [ ] frontend: BLOCKER: Demo login flow fails. UI submits credentials but POST http://localhost:3000/api/auth/login returns 401 Unauthorized, and the app stays on /login (no redirect). This violates acceptance criteria ('Login works with pre-filled demo credentials').
- [ ] frontend: Potential cause to investigate: backend rate limiter misconfigured (too strict, not excluding localhost/dev), or frontend repeatedly calling /auth/me in a loop causing immediate throttling.
- [ ] frontend: Because auth endpoints return 429, the app cannot complete login, cannot establish session persistence, and cannot access any protected routes (dashboard/projects/etc.). This prevents verification of all remaining requirements (dashboard, project views, issue CRUD, comments, search, theme persistence, etc.).
- [ ] frontend: BLOCKER: API requests are being rate-limited (HTTP 429) for core auth endpoints. Observed in browser console/network: GET http://localhost:3000/api/auth/me -> 429 (twice) and POST http://localhost:3000/api/auth/login -> 429. Also reproducible via curl directly against backend: GET http://localhost:8000/api/auth/me -> 429.
- [ ] frontend: API auth mismatch for QA seed credentials: direct API test with test@example.com/password123 returned 401, while UI uses admin@example.com/Password123!. (Not necessarily a bug, but it conflicts with the test instruction that mentions test@example.com/password123.)
- [ ] frontend: Because of the request storm, I could not proceed to test required workflows (dashboard cards, create project, project board/list/summary, issue CRUD, comments CRUD, search, settings/theme) or enumerate/click all interactive elements as required.
- [ ] frontend: CRITICAL: The frontend is stuck in an aggressive refetch loop against GET /api/projects, producing 429 Too Many Requests and net::ERR_INSUFFICIENT_RESOURCES. This prevents normal app usage and even breaks the QA tooling (a11y tree and screenshots time out).
- [ ] FAILED: Build an Express.js backend API implementing authe...
- [ ] backend: Search endpoint method mismatch: POST /api/search returns 404 while GET /api/search works. If frontend uses POST (common when sending complex filters), it will fail. Either add POST support or ensure frontend uses GET consistently.
- [ ] backend: Issue creation is broken: POST /api/issues consistently returns HTTP 400 even with seemingly valid payloads (projectKey, summary, description, type, priority, labels, status, reporterId/assigneeId variants). This blocks core acceptance criteria: 'Issue list/create/detail/update/status-update endpoints work and persist to DB'. Need to inspect validation schema for create issue and document required fields; ensure it matches frontend payload and seeded enums.
- [ ] backend: Verification gap: /health endpoint was not tested in this run, and issue/project/issue-history endpoints were not exercised here. (This is a QA coverage failure for the overall task; see untested_elements.)
- [ ] backend: Logout/session invalidation is broken: POST /api/auth/logout returns 204 but subsequent GET /api/auth/me with the same auth_token still returns 200 with the user. This violates the requirement that logout is supported and implies session persistence/invalidation is not correctly implemented. If using JWT, you need a revocation/blacklist strategy or switch to server-side sessions; if using server sessions, ensure the session is destroyed/cleared and auth middleware rejects it afterward.
- [ ] backend: Because /me cannot be authenticated, all other acceptance criteria requiring authenticated access (projects list/create, issues CRUD/status update/history, comments CRUD with ownership enforcement, search, settings get/update) could not be exercised.
- [ ] backend: AUTH_TEST_MODE escape hatch appears non-functional for verification: auth middleware supports x-test-auth header / auth_token query/body when AUTH_TEST_MODE=true, but the verification tool cannot send custom headers (per tool signature shown) and the query-param attempt still returned 401. This blocks verification of all protected endpoints (projects/issues/comments/search/settings).
- [ ] backend: Authentication/session persistence is not verifiable with the provided test tooling: login sets an httpOnly cookie, but the verification tool (test_api) does not persist cookies between requests. As a result, /api/auth/me returns 401 even immediately after a successful login.
- [ ] backend: SET-003: Settings response shape is incomplete: GET returns {settings:{theme,notifications}} where notifications is boolean; requirement expects notification preferences (likely object) and profile fields. Add profile fields (name, avatar, etc.) and notifications object (email/inApp/etc.).
- [ ] backend: SET-002: Settings update method mismatch: only PUT /api/settings exists; PATCH /api/settings returns 404. Requirement says get/update; frontend commonly uses PATCH. Add PATCH (or ensure frontend uses PUT) and keep consistent error envelope (currently settings PUT returns {error:'validation_error', message...} which differs from ApiError format).
- [ ] backend: SET-001: /api/settings is not authenticated and is not persisted to DB. It uses a module-level variable `let settings = { theme, notifications }` (placeholder). Requirement calls for get/update user profile fields, theme preference, notification preferences, persisted per-user. Implement DB table (e.g., user_settings) keyed by user_id and enforce authRequired.
- [ ] backend: AUTH-002: Because only admin can log in, comment ownership enforcement cannot be validated. Expected behavior: non-author with valid session should receive 403 (forbidden) on PATCH/DELETE /api/comments/:id. Current tests with other users return 401 Invalid token because they cannot log in.
- [ ] backend: AUTH-001: /api/auth/login contains a hard-coded restriction: if email !== 'admin@example.com' it returns 401. This violates requirement to accept email/password for users (and prevents testing/using ownership-based authorization for comments, assignee filters, etc.). Fix: remove the hard-coded check and authenticate any seeded user with bcrypt password_hash.
- [ ] backend: ERR-001 (Inconsistent error shape): /api/search when q is missing returns error as a string field (error:'validation_error') rather than the consistent ApiError format used elsewhere ({error:{code,message,details}}). This violates the “consistent error response format” requirement.
- [ ] backend: SEARCH-002 (Filters missing): Attempting to pass filters (e.g., projectId=ACME&status=TODO) yields no filtered results because the route does not apply these query params at all. The SQL only searches tsv/key and does not constrain by project/status/assignee.
- [ ] backend: SEARCH-001 (Contract mismatch): Requirements: “global search endpoint searching by issue key/title/description with filters (project, status, assignee) and returns results with project context”. Actual: GET /api/search returns { query, results: { projects, issues, comments } } and does not accept/implement project/status/assignee filters. This will break any frontend expecting the required contract.
- [ ] backend: Because authenticated requests cannot be made in this environment, the majority of required endpoints (projects/issues/comments/search/settings) could not be exercised at all, so acceptance criteria cannot be met/verified.
- [ ] backend: The code includes a test-mode escape hatch in auth middleware (accept auth_token via query/body when NODE_ENV==='test') and a /api/auth/test-login endpoint gated by NODE_ENV==='test', but the running server appears to be in NODE_ENV='development' (default in env.js). Therefore the escape hatch is inactive and /api/auth/test-login returns 404.
- [ ] backend: Session persistence/usage is not verifiable with the provided test tool: login returns a JWT and sets an httpOnly cookie, but subsequent requests made by the verification tool do not include cookies and cannot send Authorization headers. As a result, /api/auth/me and all auth-protected endpoints return 401, blocking verification of Projects/Issues/Comments/Search/Settings.
- [ ] backend: Because auth could not be established for subsequent calls, the following acceptance criteria remain unverified: projects list (3+ seeded with issue counts), project create, issues list/create/detail/update/status-update + history, comments CRUD + ownership enforcement, search filters/results, settings get/update persistence, and consistent validation errors on those endpoints.
- [ ] backend: Design/API mismatch risk: login returns a JWT token in the JSON response, and auth middleware supports Bearer tokens, but the verification tool cannot send headers (the provided test_api signature in this environment only supports method/url/body). This prevents validating the Bearer-token path as well.
- [ ] backend: Critical: Authenticated requests cannot be performed with the current verification tooling because login sets an httpOnly cookie (access_token) but the test_api tool does not preserve/return cookies between requests. As a result, GET /api/auth/me returns 401 even immediately after a successful login, and all protected endpoints (e.g., /api/projects) also return 401.
- [ ] backend: ISSUE-002: Issue listing does not implement required filters (type, priority, labels) and required sorts (created, priority, status, key). It only filters by projectId/status/assigneeId/q and sorts by arbitrary field name from query with no validation.
- [ ] backend: ISSUE-001: app/backend/src/routes/issues.js uses an in-memory array (hardcoded demo issue/comments) rather than database. This violates 'No placeholder endpoints' and acceptance criteria requiring persistence to DB.
- [ ] backend: PROJ-001: POST /api/projects returns 404 (endpoint not implemented). Requirement: create project must work and persist to DB.
- [ ] backend: AUTH-002: test_api tool cannot pass Authorization headers (tool limitation), but code inspection confirms /me requires Bearer token via middleware (auth.js). This means /me will always 401 unless client sends Authorization header; currently no server-side session cookie support is visible.
- [ ] backend: AUTH-001: Session persistence across refresh is not implemented as required. Login returns a JWT in JSON, but there is no cookie/session mechanism; calling GET /api/auth/me without an Authorization header returns 401 (expected persistence across refresh). If JWT is the chosen strategy, frontend must store token and send it on refresh; requirement explicitly asks for session persistence, so either set an HttpOnly cookie on login or document/implement token persistence strategy and ensure /me works after refresh.
- [ ] backend: P0: Project 'get by key' is not implemented as specified. Route is GET /api/projects/:projectId and matches by id, not by key.
- [ ] backend: P0: Projects list does not return 3+ seeded projects and does not include issue counts (requirement: list projects with issue count). Current response contains only one 'Demo Project' and no issueCount field.
- [ ] backend: P0: Projects are stored in an in-memory array in src/routes/projects.js. This violates 'No placeholder endpoints' and 'real data from the database'. Data will reset on server restart and is not seeded from DB.
- [ ] backend: P0: src/routes/auth.js is explicitly a stub: login accepts any credentials and returns token 'demo-token'; no password validation; no JWT verification; no session persistence; /me only checks for Authorization header and returns a hardcoded user. This fails the requirement: login must succeed specifically for admin@example.com / Password123! and return a usable session; /me must return the logged-in user.
- [ ] backend: P0: /health returns 503 (observed via GET http://localhost:8000/health). Acceptance requires 200.
- [ ] backend: Because the server does not start, none of the required endpoints can be exercised: auth (/api/auth/login, /api/auth/logout, /api/auth/me), projects, issues (including filtering/sorting/pagination/status updates/history), comments CRUD with ownership enforcement, search, settings persistence, consistent error shapes, and CORS behavior.
- [ ] backend: CRITICAL: Server cannot start. app/backend/src/app.js imports routes that do not exist: ./routes/auth.js, ./routes/users.js, ./routes/projects.js, ./routes/issues.js, ./routes/search.js, ./routes/settings.js. Only src/routes/health.js exists. Startup error: ERR_MODULE_NOT_FOUND: Cannot find module '.../src/routes/auth.js' imported from src/app.js.
- [ ] database: Requirement says: 'Support comments CRUD with author ownership and timestamps; store markdown content'. Schema stores comment content in column `comment.body` (TEXT). While TEXT can contain markdown, the column is not explicitly named as markdown (e.g., body_markdown) and my attempted verification query for `body_markdown` failed with: ERROR: column c.body_markdown does not exist. If backend/API contracts expect `body_markdown`, this will break comment CRUD. Align schema/seed/API naming (either rename column to body_markdown or ensure backend uses `body`).
- [ ] database: Cannot execute 03_verify.sql to validate 'Basic queries ... return expected results' due to missing Postgres runtime access.
- [ ] database: Cannot run acceptance criterion 'Database initializes successfully from schema + seed scripts with no errors' because this repo contains only SQL scripts and no runnable Postgres environment definition (no docker-compose.yml / Dockerfile) and no local psql client is available in the execution environment.
- [ ] design: P1: Inconsistency between spec.json and api_contracts.md for issue/comment update endpoints. spec.json apiSummary lists PUT /issues/:issueId and PUT /comments/:commentId, but api_contracts.md documents PATCH /issues/:issueId and PATCH /comments/:commentId. This mismatch can cause implementation confusion and should be reconciled (choose PATCH or PUT consistently across all specs).
- [ ] design: P0: Acceptance test checklist mapping requirement not met. A grep across jira/design found no 'verification checklist' / IDs (LOGIN/BOARD/LIST/SUMMARY/ISSUE/COMMENT/SEARCH/THEME) or a section that maps directly to the provided verification checklist. spec.project.json has an acceptance_tests array (ids 1-16) but it is not explicitly tied to the verification checklist items/IDs as requested.
- [ ] design: P0: Theme persistence is not fully specified in spec.json. It mentions localStorage and toggle locations, but does not specify key names, server mirroring (/settings/me), error handling (revert toggle + toast), or refresh behavior beyond a generic statement.
- [ ] design: P0: View-mode URL persistence is not fully specified in spec.json. Routes include /projects/:projectKey/{board|list|summary}, but spec.json does not state that switching tabs updates the URL, persists per project, and restores last view when navigating back unless URL overrides (this is present in spec.project.json only).
- [ ] design: P0: jira/design/spec.json is incomplete vs requirements. It lists routes and some acceptance bullets, but does NOT fully specify: issue detail editing flows, comments CRUD flows, global search filters behavior, settings/profile fields, notification preferences, or explicit loading/error/empty/success toast states per feature. Many of these exist in spec.project.json, but acceptance criteria explicitly requires spec.json to cover required features and UX states.
- [ ] None currently identified in design docs (re-run verifier to confirm)

## Test Status
- Design verification: pending re-run

## Deployment Status
[Deployment state]
