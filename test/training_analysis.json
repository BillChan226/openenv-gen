{
  "summary": {
    "total_evaluations": 19,
    "steps": [
      0,
      5,
      10,
      15,
      20,
      25,
      30,
      35,
      40,
      45,
      50,
      55,
      60,
      65,
      70,
      75,
      80,
      85,
      90
    ],
    "success_rates": [
      0.01818181818181818,
      0.2962962962962963,
      0.2222222222222222,
      0.07547169811320754,
      0.14545454545454545,
      0.2,
      0.2777777777777778,
      0.14545454545454545,
      0.21428571428571427,
      0.25925925925925924,
      0.16666666666666666,
      0.20967741935483872,
      0.12727272727272726,
      0.14814814814814814,
      0.33962264150943394,
      0.16666666666666666,
      0.2727272727272727,
      0.3018867924528302,
      0.21739130434782608
    ],
    "avg_rewards": [
      0.01818181818181818,
      0.2962962962962963,
      0.2222222222222222,
      0.07547169811320754,
      0.14545454545454545,
      0.2,
      0.2777777777777778,
      0.14545454545454545,
      0.21428571428571427,
      0.25925925925925924,
      0.16666666666666666,
      0.20967741935483872,
      0.12727272727272726,
      0.14814814814814814,
      0.33962264150943394,
      0.16666666666666666,
      0.2727272727272727,
      0.3018867924528302,
      0.21739130434782608
    ],
    "unique_task_types": 76
  },
  "evaluation_stages": [
    {
      "step": 0,
      "num_tasks": 32,
      "success_rate": 0.01818181818181818,
      "avg_reward": 0.01818181818181818,
      "results": [
        {
          "task_id": "e7c40bca",
          "task_type": "enter-text-dynamic",
          "benchmark": "miniwob",
          "goal": "Enter \"dCLfx\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "type('14', 'dCLfx')",
            "click('15')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "380c2617",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b3f6eeed",
          "task_type": "email-inbox-noscroll",
          "benchmark": "miniwob",
          "goal": "Find the email by Gracia and click the trash icon to delete it.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9b1262de",
          "task_type": "enter-time",
          "benchmark": "miniwob",
          "goal": "Enter 3:37 AM as the time and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "03b21d93",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a small black item",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "12a2dde2",
          "task_type": "find-word",
          "benchmark": "miniwob",
          "goal": "Find the 1st word in the paragraph, type that into the textbox and press \"Submit\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "31fd1c17",
          "task_type": "enter-time",
          "benchmark": "miniwob",
          "goal": "Enter 1:56 AM as the time and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "65702c92",
          "task_type": "text-editor",
          "benchmark": "miniwob",
          "goal": "Using the text editor, give the text neque the style underlined and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "47ee91a5",
          "task_type": "click-pie-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"m\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f8c4ebf0",
          "task_type": "click-collapsible",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "34d598b8",
          "task_type": "resize-textarea",
          "benchmark": "miniwob",
          "goal": "Resize the textarea so that the height is larger than its initial size then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8d539546",
          "task_type": "copy-paste-2",
          "benchmark": "miniwob",
          "goal": "Copy the text from the 1st text area below and paste it into the text input, then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6de9d625",
          "task_type": "scroll-text",
          "benchmark": "miniwob",
          "goal": "Find the last word in the text area, enter it into the text field and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5a5689ed",
          "task_type": "use-autocomplete",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"Latv\" and ends with \"via\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b9f0f59c",
          "task_type": "read-table-2",
          "benchmark": "miniwob",
          "goal": "Enter the value that corresponds with each label into the form and submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "75d98883",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/12/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "fill('17', '12/12/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "3072ac34",
          "task_type": "email-inbox-delete",
          "benchmark": "miniwob",
          "goal": "Find the email by Daniela and click the trash icon to delete it.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "cb402b1b",
          "task_type": "email-inbox-noscroll",
          "benchmark": "miniwob",
          "goal": "Find the email by Elena and reply to them with the text \"Rhoncus eu sed.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('17')",
            "noop()",
            "noop()",
            "click('53')",
            "type('53', 'Elena')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "c39d8a0e",
          "task_type": "use-spinner",
          "benchmark": "miniwob",
          "goal": "Select -8 with the spinner and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 8,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "select('18', '-8')",
            "noop()",
            "click('25')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "dddf1a03",
          "task_type": "resize-textarea",
          "benchmark": "miniwob",
          "goal": "Resize the textarea so that the width is larger than its initial size then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "61dd5e53",
          "task_type": "enter-text",
          "benchmark": "miniwob",
          "goal": "Enter \"Cierra\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "type(14, \"Cierra\")",
            "click(15)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "fill(14, \"Cierra\")",
            "noop()",
            "click(15)"
          ],
          "has_action_error": true
        },
        {
          "task_id": "bb57925c",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/12/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "fill('17', '12/12/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "9514f6c5",
          "task_type": "highlight-text",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the paragraph below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "df4ed267",
          "task_type": "use-spinner",
          "benchmark": "miniwob",
          "goal": "Select 9 with the spinner and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "4870ba23",
          "task_type": "copy-paste",
          "benchmark": "miniwob",
          "goal": "Copy the text in the textarea below, paste it into the textbox and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7ce36241",
          "task_type": "scroll-text",
          "benchmark": "miniwob",
          "goal": "Find the last word in the text area, enter it into the text field and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "38b983d9",
          "task_type": "read-table-2",
          "benchmark": "miniwob",
          "goal": "Enter the value that corresponds with each label into the form and submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b3a5171d",
          "task_type": "copy-paste-2",
          "benchmark": "miniwob",
          "goal": "Copy the text from the 1st text area below and paste it into the text input, then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6128268e",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/21/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type(17, '12/21/2016')",
            "noop()",
            "click(19)",
            "click(19)",
            "noop()",
            "noop()",
            "type(17, '12/21/2016')",
            "noop()",
            "noop()",
            "click(19)"
          ],
          "has_action_error": true
        },
        {
          "task_id": "e0ad7c25",
          "task_type": "enter-text-dynamic",
          "benchmark": "miniwob",
          "goal": "Enter \"vKr\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "type('14', 'vKr')",
            "click('15')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "da64efa8",
          "task_type": "email-inbox-reply",
          "benchmark": "miniwob",
          "goal": "Find the email by Verna and reply to them with the text \"Consectetur.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "click('27')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n  [EVAL] Task 0 (enter-text-dynamic): FAILED, reward=0.0\n  [EVAL] Task 1 (click-test-transfer): SUCCESS in 1 steps\n  [EVAL] Task 2 (email-inbox-noscroll): FAILED, reward=0.0\n  [EVAL] Task 3 (enter-time): FAILED, reward=0.0\n  [EVAL] Task 4 (click-shape): FAILED, reward=0.0\n  [EVAL] Task 5 (find-word): FAILED, reward=0.0\n  [EVAL] Task 6 (enter-time): FAILED, reward=0.0\n  [EVAL] Task 7 (text-editor): FAILED, reward=0.0\n  [EVAL] Task 8 (click-pie-nodelay): FAILED, reward=0.0\n  [EVAL] Task 9 (click-collapsible): FAILED, reward=0.0\n  [EVAL] Task 10 (resize-textarea): FAILED, reward=0.0\n  [EVAL] Task 11 (copy-paste-2): FAILED, reward=0.0\n  [EVAL] Task 12 (scroll-text): FAILED, reward=0.0\n  [EVAL] Task 13 (use-autocomplete): FAILED, reward=0.0\n  [EVAL] Task 14 (read-table-2): FAILED, reward=0.0\n  [EVAL] Task 15 (choose-date-easy): FAILED, reward=0.0\n  [EVAL] Task 16 (email-inbox-delete): FAILED, reward=0.0\n  [EVAL] Task 17 (email-inbox-noscroll): FAILED, reward=0.0\n  [EVAL] Task 18 (use-spinner): FAILED, reward=0.0\n  [EVAL] Task 19 (resize-textarea): FAILED, reward=0.0\n  [EVAL] Task 20 (enter-text): FAILED, reward=0.0\n  [EVAL] Task 21 (choose-date-easy): FAILED, reward=0.0\n  [EVAL] Task 22 (highlight-text): FAILED, reward=0.0\n  [EVAL] Task 23 (use-spinner): FAILED, reward=0.0\n  [EVAL] Task 24 (copy-paste): FAILED, reward=0.0\n  [EVAL] Task 25 (scroll-text): FAILED, reward=0.0\n  [EVAL] Task 26 (read-table-2): FAILED, reward=0.0\n  [EVAL] Task 27 (copy-paste-2): FAILED, reward=0.0\n  [EVAL] Task 28 (choose-date-easy): FAILED, reward=0.0\n  [EVAL] Task 29 (use-colorwheel): FAILED, reward=0.0\n  [EVAL] Task 30 (enter-text-dynamic): FAILED, reward=0.0\n  [EVAL] Task 31 (email-inbox-reply): FAILED, reward=0.0\n\n============================================================\nEVAL RESULTS: 1/32 = 3.1% success rate\nAvg reward: 0.031, Avg steps: 8.7\n============================================================"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1d183d8c",
          "task_type": "email-inbox-important",
          "benchmark": "miniwob",
          "goal": "Find the email by Lacie and click the star icon to mark it as important.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "fe7d9ab3",
          "task_type": "email-inbox-important",
          "benchmark": "miniwob",
          "goal": "Find the email by Shirleen and click the star icon to mark it as important.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2bf6f884",
          "task_type": "email-inbox-important",
          "benchmark": "miniwob",
          "goal": "Find the email by Roxanne and click the star icon to mark it as important.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "73c5adea",
          "task_type": "email-inbox-important",
          "benchmark": "miniwob",
          "goal": "Find the email by Gayla and click the star icon to mark it as important.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b5b15147",
          "task_type": "email-inbox-important",
          "benchmark": "miniwob",
          "goal": "Find the email by Beckie and click the star icon to mark it as important.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9a0cda85",
          "task_type": "email-inbox-important",
          "benchmark": "miniwob",
          "goal": "Find the email by Mella and click the star icon to mark it as important.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9c87548b",
          "task_type": "email-inbox-important",
          "benchmark": "miniwob",
          "goal": "Find the email by Gloriane and click the star icon to mark it as important.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "91a47db0",
          "task_type": "email-inbox-important",
          "benchmark": "miniwob",
          "goal": "Find the email by Clerissa and click the star icon to mark it as important.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 80 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/80...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/80...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/80...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/80...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/80...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/80...[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.500, std=0.000\n[ADVANTAGES DEBUG] advantages: min=0.00, max=0.00\n/scratch/czr/env-gen/openenv/examples/grpo_web/utils/data.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  tensor = torch.tensor(request_tokens, dtype=torch.long)\n/scratch/czr/env-gen/openenv/examples/grpo_web/utils/data.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  tensor = torch.tensor(response_tokens, dtype=torch.long)\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:00:19 INFO\u001b[0m Pushing weights for policy version 1\n/scratch/czr/anaconda3/envs/openenv/lib/python3.12/site-packages/torch/distributed/checkpoint/filesystem.py:133: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  if tensor.storage().size() != tensor.numel():\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:00:22 INFO\u001b[0m Completed weights push in 2.76 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 19:00:22 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 19:00:28 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 19:00:28 INFO\u001b[0m Weight update completed (now v1)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 1 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 0.0\n  buffer/sample/avg_sampled_policy_age: 0.0\n  buffer/sample/demand_to_size_ratio: 0.2\n  buffer/sample/max_sampled_policy_age: 0.0\n  eval/avg_reward: 0.03125\n  eval/num_tasks: 32.0\n  eval/success_rate: 0.03125\n  eval/successes: 1.0\n  generator/generate/count_requests: 363.0\n  generator/generate/count_sequences_completed: 362.0\n  generator_perf/generate/total_duration_avg_s: 0.9355665836228854\n  generator_perf/generate/total_duration_max_s: 1.332966552734375\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.8720618616789579\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.521835327148438\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 1.7671304009854794\n  reference_perf/forward/total_duration_max_s: 1.7692081220448017\n  reward/avg_reward: -0.5\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.0\n  rl_trainer/learning_rate: 1e-05\n  rl_trainer/loss: 0.0009498316794633865\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 3.1490593887865543\n  rl_trainer_perf/step/forward_backward/duration_max_s: 3.1490593887865543\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 7.631597518920898\n  rl_trainer_perf/step/memory_peak_max_gb: 15.204451084136963\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.03500499203801155\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.03500499203801155\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.013015631586313248\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.013015631586313248\n  rl_trainer_perf/step/total_duration_avg_s: 3.1970878560096025\n  rl_trainer_perf/step/total_duration_max_s: 3.1970878560096025\n  task/avg_length: 8.975\n  task/avg_reward: 0.025\n  task/count_completed: 40.0\n  task/success_rate: 0.025\n  train/avg_reward: -0.5\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n==============================\n\n\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/80...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/80...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/80...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/80...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/80...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/80...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/80...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/80...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/80...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/80...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/80...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/80...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/80...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/80...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/80...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/80...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/80...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/80...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/80...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/80...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/80...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/80...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/80...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/80...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/80...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/80...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/80...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/80...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/80...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/80...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/80...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/80...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/80...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/80...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/80...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/80...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/80...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/80...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/80...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/80...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/80...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/80...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/80...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/80...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/80...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/80...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/80...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/80...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/80...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/80...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/80...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/80...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/80...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/80...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/80...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/80...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/80...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/80...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/80...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 66/80...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/80...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/80...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 69/80...\n[TRAINER DEBUG] Episode 69 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 70/80...\n[TRAINER DEBUG] Episode 70 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 71/80...\n[TRAINER DEBUG] Episode 71 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 72/80...\n[TRAINER DEBUG] Episode 72 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 73/80...\n[TRAINER DEBUG] Episode 73 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 74/80...\n[TRAINER DEBUG] Episode 74 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 75/80...\n[TRAINER DEBUG] Episode 75 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 76/80...\n[TRAINER DEBUG] Episode 76 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 77/80...\n[TRAINER DEBUG] Episode 77 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 78/80...\n[TRAINER DEBUG] Episode 78 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 79/80...\n[TRAINER DEBUG] Episode 79 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 80/80...\n[TRAINER DEBUG] Episode 80 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 80 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 1: 80 episodes, success rate: 0.0%, avg reward: -0.50"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0aa2dc1d",
          "task_type": "click-shades",
          "benchmark": "miniwob",
          "goal": "Select all the shades of blue and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7033fd2d",
          "task_type": "click-shades",
          "benchmark": "miniwob",
          "goal": "Select all the shades of green and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8041f4ad",
          "task_type": "click-shades",
          "benchmark": "miniwob",
          "goal": "Select all the shades of red and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9e63ee6b",
          "task_type": "click-shades",
          "benchmark": "miniwob",
          "goal": "Select all the shades of blue and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e708ff7a",
          "task_type": "click-shades",
          "benchmark": "miniwob",
          "goal": "Select all the shades of red and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3babf450",
          "task_type": "click-shades",
          "benchmark": "miniwob",
          "goal": "Select all the shades of red and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "4a1a4b62",
          "task_type": "click-shades",
          "benchmark": "miniwob",
          "goal": "Select all the shades of red and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "02c365c2",
          "task_type": "click-shades",
          "benchmark": "miniwob",
          "goal": "Select all the shades of blue and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 80 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/80...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/80...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/80...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/80...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/80...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/80...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/80...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/80...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/80...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/80...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/80...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/80...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/80...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/80...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/80...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/80...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/80...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/80...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/80...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/80...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/80...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/80...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/80...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/80...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/80...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/80...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/80...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/80...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/80...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/80...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/80...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/80...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/80...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/80...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/80...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/80...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/80...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/80...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/80...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/80...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/80...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/80...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/80...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/80...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/80...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/80...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/80...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/80...\n[TRAINER DEBUG] Episode 48 reward: -0.5[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.500, std=0.000\n[ADVANTAGES DEBUG] advantages: min=0.00, max=0.00\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:03:58 INFO\u001b[0m [GC] Performing periodic GC collection took 0.14 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:03:59 INFO\u001b[0m Pushing weights for policy version 3\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:04:02 INFO\u001b[0m Completed weights push in 2.86 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 19:04:03 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 19:04:07 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 19:04:07 INFO\u001b[0m Weight update completed (now v3)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 3 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 75.0\n  buffer/sample/avg_sampled_policy_age: 0.0\n  buffer/sample/demand_to_size_ratio: 0.12121212121212122\n  buffer/sample/max_sampled_policy_age: 0.0\n  generator/generate/count_requests: 77.0\n  generator/generate/count_sequences_completed: 77.0\n  generator_perf/generate/total_duration_avg_s: 0.9935990029867595\n  generator_perf/generate/total_duration_max_s: 4.1226171875\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.7176435831934214\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.490585327148438\n  reference_perf/forward/memory_peak_max_gb: 36.414405822753906\n  reference_perf/forward/total_duration_avg_s: 0.5687765898182988\n  reference_perf/forward/total_duration_max_s: 0.5722814798355103\n  reward/avg_reward: -0.5\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.0\n  rl_trainer/learning_rate: 9.97995991983968e-06\n  rl_trainer/loss: 830.1979370117188\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.6052352115511894\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.6052352115511894\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013553900644183159\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013553900644183159\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.150899825617671\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.150899825617671\n  rl_trainer_perf/step/total_duration_avg_s: 0.7696966081857681\n  rl_trainer_perf/step/total_duration_max_s: 0.7696966081857681\n  task/avg_length: 10.0\n  task/avg_reward: 0.0\n  task/count_completed: 8.0\n  task/success_rate: 0.0\n  train/avg_reward: -0.5\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n==============================\n\n\n[TRAINER DEBUG] Computing reward for episode 49/80...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/80...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/80...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/80...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/80...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/80...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/80...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/80...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/80...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/80...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/80...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/80...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/80...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/80...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/80...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/80...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/80...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 66/80...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/80...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/80...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 69/80...\n[TRAINER DEBUG] Episode 69 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 70/80...\n[TRAINER DEBUG] Episode 70 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 71/80...\n[TRAINER DEBUG] Episode 71 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 72/80...\n[TRAINER DEBUG] Episode 72 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 73/80...\n[TRAINER DEBUG] Episode 73 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 74/80...\n[TRAINER DEBUG] Episode 74 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 75/80...\n[TRAINER DEBUG] Episode 75 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 76/80...\n[TRAINER DEBUG] Episode 76 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 77/80...\n[TRAINER DEBUG] Episode 77 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 78/80...\n[TRAINER DEBUG] Episode 78 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 79/80...\n[TRAINER DEBUG] Episode 79 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 80/80...\n[TRAINER DEBUG] Episode 80 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 80 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 2: 80 episodes, success rate: 0.0%, avg reward: -0.50"
          ],
          "has_action_error": false
        },
        {
          "task_id": "45b2df92",
          "task_type": "enter-date",
          "benchmark": "miniwob",
          "goal": "Enter 05/26/2014 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8165053e",
          "task_type": "enter-date",
          "benchmark": "miniwob",
          "goal": "Enter 04/11/2017 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d850b348",
          "task_type": "enter-date",
          "benchmark": "miniwob",
          "goal": "Enter 08/15/2019 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0ce7d7cd",
          "task_type": "enter-date",
          "benchmark": "miniwob",
          "goal": "Enter 04/11/2017 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2ab31a02",
          "task_type": "enter-date",
          "benchmark": "miniwob",
          "goal": "Enter 05/04/2010 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "94b58ccf",
          "task_type": "enter-date",
          "benchmark": "miniwob",
          "goal": "Enter 03/21/2017 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5936db8b",
          "task_type": "enter-date",
          "benchmark": "miniwob",
          "goal": "Enter 12/04/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e63778c7",
          "task_type": "enter-date",
          "benchmark": "miniwob",
          "goal": "Enter 09/03/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 80 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/80...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/80...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/80...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/80...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/80...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/80...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/80...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/80...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/80...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/80...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/80...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/80...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/80...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/80...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/80...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/80...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/80...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/80...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/80...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/80...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/80...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/80...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/80...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/80...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/80...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/80...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/80...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/80...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/80...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/80...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/80...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/80...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/80...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/80...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/80...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/80...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/80...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/80...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/80...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/80...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/80...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/80...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/80...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/80...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/80...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/80...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/80...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/80...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/80...\u001b[34m[ReferenceModel-0/2] 2025-12-25 19:07:40 INFO\u001b[0m [GC] Performing periodic GC collection took 0.02 seconds\n\u001b[34m[ReferenceModel-1/2] 2025-12-25 19:07:40 INFO\u001b[0m [GC] Performing periodic GC collection took 0.02 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.500, std=0.000\n[ADVANTAGES DEBUG] advantages: min=0.00, max=0.00\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:07:41 INFO\u001b[0m [GC] Performing periodic GC collection took 0.13 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:07:42 INFO\u001b[0m Pushing weights for policy version 5\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:07:45 INFO\u001b[0m Completed weights push in 2.88 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 19:07:46 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 19:07:50 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 19:07:50 INFO\u001b[0m Weight update completed (now v5)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 5 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 69.0\n  buffer/sample/avg_sampled_policy_age: 0.25\n  buffer/sample/demand_to_size_ratio: 0.11428571428571428\n  buffer/sample/max_sampled_policy_age: 1.0\n  generator/generate/count_requests: 77.0\n  generator/generate/count_sequences_completed: 77.0\n  generator_perf/generate/total_duration_avg_s: 0.9975930350167416\n  generator_perf/generate/total_duration_max_s: 4.41279833984375\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.7451915759593248\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.490432739257812\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 0.5953435655683279\n  reference_perf/forward/total_duration_max_s: 0.6088410150259733\n  reward/avg_reward: -0.5\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.0\n  rl_trainer/learning_rate: 9.939879759519038e-06\n  rl_trainer/loss: 0.04819497466087341\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.5862653031945229\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.5862653031945229\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013863340020179749\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013863340020179749\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.15119162760674953\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.15119162760674953\n  rl_trainer_perf/step/total_duration_avg_s: 0.7513260990381241\n  rl_trainer_perf/step/total_duration_max_s: 0.7513260990381241\n  task/avg_length: 10.0\n  task/avg_reward: 0.0\n  task/count_completed: 8.0\n  task/success_rate: 0.0\n  train/avg_reward: -0.5\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n==============================\n\n\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/80...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/80...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/80...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/80...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/80...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/80...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/80...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/80...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/80...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/80...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/80...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/80...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/80...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/80...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/80...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/80...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 66/80...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/80...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/80...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 69/80...\n[TRAINER DEBUG] Episode 69 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 70/80...\n[TRAINER DEBUG] Episode 70 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 71/80...\n[TRAINER DEBUG] Episode 71 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 72/80...\n[TRAINER DEBUG] Episode 72 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 73/80...\n[TRAINER DEBUG] Episode 73 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 74/80...\n[TRAINER DEBUG] Episode 74 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 75/80...\n[TRAINER DEBUG] Episode 75 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 76/80...\n[TRAINER DEBUG] Episode 76 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 77/80...\n[TRAINER DEBUG] Episode 77 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 78/80...\n[TRAINER DEBUG] Episode 78 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 79/80...\n[TRAINER DEBUG] Episode 79 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 80/80...\n[TRAINER DEBUG] Episode 80 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 80 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 3: 80 episodes, success rate: 0.0%, avg reward: -0.50"
          ],
          "has_action_error": false
        }
      ]
    },
    {
      "step": 5,
      "num_tasks": 32,
      "success_rate": 0.2962962962962963,
      "avg_reward": 0.2962962962962963,
      "results": [
        {
          "task_id": "4d289e0b",
          "task_type": "click-tab-2-hard",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"Sagittis,\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "cd4d73eb",
          "task_type": "click-tab-2-hard",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"nec,\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e5448e4d",
          "task_type": "click-tab-2-hard",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"quis.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "click('20')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "436b8250",
          "task_type": "click-tab-2-hard",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"pharetra\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "click('25')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c0c456b0",
          "task_type": "click-tab-2-hard",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"quam\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "click('20')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f0b7bc11",
          "task_type": "click-tab-2-hard",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"tellus.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d08d4925",
          "task_type": "click-tab-2-hard",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"nisl.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "click('20')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5f0500d0",
          "task_type": "click-checkboxes-soft",
          "benchmark": "miniwob",
          "goal": "Select words similar to real, swine, camouflage and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c12271c6",
          "task_type": "copy-paste-2",
          "benchmark": "miniwob",
          "goal": "Copy the text from the 3rd text area below and paste it into the text input, then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b8243f5b",
          "task_type": "click-checkboxes-transfer",
          "benchmark": "miniwob",
          "goal": "Select 8VyZE, s1cjlff, 0WNHqPU and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "4613220a",
          "task_type": "resize-textarea",
          "benchmark": "miniwob",
          "goal": "Resize the textarea so that the height is larger than its initial size then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b722ca8d",
          "task_type": "click-checkboxes",
          "benchmark": "miniwob",
          "goal": "Select QjC, ls6lSn and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f70faa53",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00e5\u008f\u2013\u00e6\u00b6\u02c6\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('15')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9cdd609d",
          "task_type": "click-tab-2-easy",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"id\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3c9112a8",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Kylynn from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "select('19', 'Kylynn')",
            "click('22')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "70ac78c3",
          "task_type": "click-tab-2-easy",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"Turpis.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "click('21')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ce91e09f",
          "task_type": "use-autocomplete",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"Cos\" and ends with \"Rica\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "type(17, \"CosRica\")",
            "noop()",
            "noop()",
            "click(18)",
            "noop()",
            "check(20)",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "aa641d28",
          "task_type": "click-tab-2-hard",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"Praesent\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "click('20')",
            "noop()",
            "click('21')",
            "click('25')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "233275b3",
          "task_type": "scroll-text",
          "benchmark": "miniwob",
          "goal": "Find the last word in the text area, enter it into the text field and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "834a9035",
          "task_type": "enter-text-dynamic",
          "benchmark": "miniwob",
          "goal": "Enter \"mds\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type(14, \"mds\")",
            "noop()",
            "click(15)",
            "noop()",
            "noop()",
            "noop()",
            "fill(14, \"mds\")",
            "noop()",
            "click(15)",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "b3cfd4b3",
          "task_type": "simple-algebra",
          "benchmark": "miniwob",
          "goal": "Solve for x and type your answer into the textbox. Press Submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "fill(16, '44')",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "1f05b684",
          "task_type": "use-slider",
          "benchmark": "miniwob",
          "goal": "Select 20 with the slider and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ac04d402",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #f8a123 with the color picker and hit Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 7,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "fill('[15]', '#f8a123')",
            "noop()",
            "click('16')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "332a3789",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a5176a45",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/27/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 6,
          "actions": [
            "noop()",
            "noop()",
            "type('17', '12/27/2016')",
            "noop()",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "0c0db14b",
          "task_type": "use-autocomplete",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"Ando\" and ends with \"ra\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8756328a",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/26/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 8,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "type('17', '12/26/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "1b278a64",
          "task_type": "copy-paste",
          "benchmark": "miniwob",
          "goal": "Copy the text in the textarea below, paste it into the textbox and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0002cffe",
          "task_type": "click-link",
          "benchmark": "miniwob",
          "goal": "Click on the link \"eget\".",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "noop()",
            "noop()",
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "118cf91d",
          "task_type": "copy-paste-2",
          "benchmark": "miniwob",
          "goal": "Copy the text from the 1st text area below and paste it into the text input, then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "fa556b1d",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #764ccb with the color picker and hit Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 5,
          "actions": [
            "type('15', '#764ccb')",
            "noop()",
            "noop()",
            "noop()",
            "click('16')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "c4fb3700",
          "task_type": "read-table-2",
          "benchmark": "miniwob",
          "goal": "Enter the value that corresponds with each label into the form and submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8629a06c",
          "task_type": "click-checkboxes-large",
          "benchmark": "miniwob",
          "goal": "Select 2mSYw7d, Bgn9, 30w, Rx, q1, RO4DMN, 0ptv, qc7EU8W, py80F, FeE and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1413f3ce",
          "task_type": "scroll-text-2",
          "benchmark": "miniwob",
          "goal": "Scroll the textarea to the bottom of the text hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9751b644",
          "task_type": "enter-text-2",
          "benchmark": "miniwob",
          "goal": "Type \"THADDEUS\" in all lower case letters in the text input and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('14', 'thaddeus')",
            "noop()",
            "click('15')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "2e89c70f",
          "task_type": "copy-paste-2",
          "benchmark": "miniwob",
          "goal": "Copy the text from the 2nd text area below and paste it into the text input, then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "59d1b0ea",
          "task_type": "click-tab-2",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"Mi,\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "af8c03fe",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a 5",
          "success": false,
          "reward": 0.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "59d7f5fe",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00c3\u2013K\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('15')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8512c374",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00c7\u00a1\u00ae\u00e5\u00ae\u0161\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('21')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "35c6db22",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00e2\u2122\u00a5\u00e2\u2122\u00a5\u00e2\u2122\u00a5\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('16')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3d18b0f4",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00c7\u00a1\u00ae\u00e5\u00ae\u0161\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('18')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "25940696",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00e3\u0192\u02dc\u00e3\u0192\u00ab\u00e3\u0192\u2014\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('16')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "4bacf683",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"Canc\u00c3\u00a9l\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('15')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a3d8e9fc",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00e2\u2122\u00a5\u00e2\u2122\u00a5\u00e2\u2122\u00a5\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "193158ac",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"Canc\u00c3\u00a9l\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('17')\n[TRAINER DEBUG] Creating 11 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/11...\n[TRAINER DEBUG] Episode 1 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 2/11...\n[TRAINER DEBUG] Episode 2 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 3/11...\n[TRAINER DEBUG] Episode 3 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 4/11...\n[TRAINER DEBUG] Episode 4 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 5/11...\n[TRAINER DEBUG] Episode 5 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 6/11...\n[TRAINER DEBUG] Episode 6 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 7/11...\n[TRAINER DEBUG] Episode 7 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 8/11...\n[TRAINER DEBUG] Episode 8 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 9/11...\n[TRAINER DEBUG] Episode 9 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 10/11...\n[TRAINER DEBUG] Episode 10 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 11/11...\n[TRAINER DEBUG] Episode 11 reward: 2.4\n[TRAINER DEBUG] Getting ref model logprobs for 11 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 5: 11 episodes, success rate: 100.0%, avg reward: 2.42"
          ],
          "has_action_error": false
        },
        {
          "task_id": "917d5307",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1119d563",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('18')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "773a4135",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "75880429",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": true,
          "reward": 1.0,
          "task_length": 8,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('19')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5a0e99d7",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": true,
          "reward": 1.0,
          "task_length": 8,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('18')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3915728e",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": true,
          "reward": 1.0,
          "task_length": 8,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('18')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "099b0b07",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e151adb2",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 65 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/65...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/65...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/65...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/65...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/65...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/65...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/65...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/65...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/65...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/65...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/65...\n[TRAINER DEBUG] Episode 11 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 12/65...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/65...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/65...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/65...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/65...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/65...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/65...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/65...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/65...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/65...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/65...\n[TRAINER DEBUG] Episode 22 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 23/65...\n[TRAINER DEBUG] Episode 23 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 24/65...\n[TRAINER DEBUG] Episode 24 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 25/65...\n[TRAINER DEBUG] Episode 25 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 26/65...\n[TRAINER DEBUG] Episode 26 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 27/65...\n[TRAINER DEBUG] Episode 27 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 28/65...\n[TRAINER DEBUG] Episode 28 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 29/65...\n[TRAINER DEBUG] Episode 29 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 30/65...\n[TRAINER DEBUG] Episode 30 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 31/65...\n[TRAINER DEBUG] Episode 31 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 32/65...\n[TRAINER DEBUG] Episode 32 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 33/65...\n[TRAINER DEBUG] Episode 33 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 34/65...\n[TRAINER DEBUG] Episode 34 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 35/65...\n[TRAINER DEBUG] Episode 35 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 36/65...\n[TRAINER DEBUG] Episode 36 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 37/65...\n[TRAINER DEBUG] Episode 37 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 38/65...\n[TRAINER DEBUG] Episode 38 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 39/65...\n[TRAINER DEBUG] Episode 39 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 40/65...\n[TRAINER DEBUG] Episode 40 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 41/65...\n[TRAINER DEBUG] Episode 41 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 42/65...\n[TRAINER DEBUG] Episode 42 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 43/65...\n[TRAINER DEBUG] Episode 43 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 44/65...\n[TRAINER DEBUG] Episode 44 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 45/65...\n[TRAINER DEBUG] Episode 45 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 46/65...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/65...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/65...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n\u001b[34m[ReferenceModel-0/2] 2025-12-25 19:25:52 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-1/2] 2025-12-25 19:25:52 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, 2.45, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=0.505, std=1.282\n[ADVANTAGES DEBUG] advantages: min=-0.78, max=1.52\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:25:54 INFO\u001b[0m [GC] Performing periodic GC collection took 0.15 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:25:54 INFO\u001b[0m Pushing weights for policy version 9\n/scratch/czr/anaconda3/envs/openenv/lib/python3.12/site-packages/torch/distributed/checkpoint/filesystem.py:652: UserWarning: Detected an existing checkpoint in forge_dcp_tmp/policy_ver_0000000009.dcp_whole_state_dict, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.\n  warnings.warn(\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:25:57 INFO\u001b[0m Completed weights push in 2.72 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 19:25:58 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 19:26:03 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 19:26:03 INFO\u001b[0m Weight update completed (now v9)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 9 ===\n  buffer/add/count_episodes_added: 65.0\n  buffer/evict/sum_episodes_evicted: 8.0\n  buffer/sample/avg_sampled_policy_age: 0.0\n  buffer/sample/demand_to_size_ratio: 0.14545454545454545\n  buffer/sample/max_sampled_policy_age: 0.0\n  generator/generate/count_requests: 65.0\n  generator/generate/count_sequences_completed: 65.0\n  generator_perf/generate/total_duration_avg_s: 0.9979276489257811\n  generator_perf/generate/total_duration_max_s: 4.92270947265625\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.8776718806475401\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 11.773590087890625\n  reference_perf/forward/memory_peak_max_gb: 29.893729209899902\n  reference_perf/forward/total_duration_avg_s: 1.174286941997707\n  reference_perf/forward/total_duration_max_s: 1.1840631645172834\n  reward/avg_reward: 0.5053846153846157\n  reward/success_rate: 0.38461538461538464\n  reward/sum_reward: 32.850000000000016\n  rl_trainer/learning_rate: 9.859719438877757e-06\n  rl_trainer/loss: 0.1928742229938507\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.6049978453665972\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.6049978453665972\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013578154146671295\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013578154146671295\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.15138189308345318\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.15138189308345318\n  rl_trainer_perf/step/total_duration_avg_s: 0.7699678167700768\n  rl_trainer_perf/step/total_duration_max_s: 0.7699678167700768\n  task/avg_length: 8.125\n  task/avg_reward: 0.5\n  task/count_completed: 8.0\n  task/success_rate: 0.5\n  train/avg_reward: 0.5053846153846154\n  train/episodes_per_rollout: 65.0\n  train/success_rate: 0.38461538461538464\n==============================\n\n[TRAINER DEBUG] Computing reward for episode 49/65...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/65...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/65...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/65...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/65...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/65...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/65...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/65...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/65...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/65...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/65...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/65...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/65...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/65...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/65...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/65...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/65...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 65 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 6: 65 episodes, success rate: 38.5%, avg reward: 0.51"
          ],
          "has_action_error": false
        }
      ]
    },
    {
      "step": 10,
      "num_tasks": 32,
      "success_rate": 0.2222222222222222,
      "avg_reward": 0.2222222222222222,
      "results": [
        {
          "task_id": "ae56f2ba",
          "task_type": "click-pie",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"s\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('[13]')",
            "noop()",
            "noop()",
            "click('[13]')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "af64f025",
          "task_type": "click-pie",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"F\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "click('13')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "067efd2d",
          "task_type": "click-pie",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"2\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "80ff98c7",
          "task_type": "click-pie",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"o\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('13')",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "330dd27b",
          "task_type": "click-pie",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"h\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('13')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "fc777adb",
          "task_type": "click-pie",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"b\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9e09fc27",
          "task_type": "click-pie",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"z\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bfe368fc",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #314e16 with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0196c50a",
          "task_type": "copy-paste",
          "benchmark": "miniwob",
          "goal": "Copy the text in the textarea below, paste it into the textbox and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "60e5f578",
          "task_type": "drag-single-shape",
          "benchmark": "miniwob",
          "goal": "Drag the item down then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8820ca52",
          "task_type": "resize-textarea",
          "benchmark": "miniwob",
          "goal": "Resize the textarea so that the height is larger than its initial size then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "075807ce",
          "task_type": "click-tab-2-hard",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"eu\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "55be855b",
          "task_type": "click-test-2",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7e84959d",
          "task_type": "guess-number",
          "benchmark": "miniwob",
          "goal": "Guess the number between 0-9 and press Submit. Use the feedback below to find the right number.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8c59583f",
          "task_type": "highlight-text-2",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the 3rd paragraph and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2c223506",
          "task_type": "click-collapsible",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "398fd988",
          "task_type": "click-collapsible",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 3,
          "actions": [
            "noop()",
            "click('16')",
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5dfce03a",
          "task_type": "use-slider",
          "benchmark": "miniwob",
          "goal": "Select -97 with the slider and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b8e3dd7e",
          "task_type": "click-menu-2",
          "benchmark": "miniwob",
          "goal": "Click the \"Menu\" button, and then find and click on the item with the \"ui-icon-seek-start\" icon.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "click('22')",
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3f3affa8",
          "task_type": "use-slider-2",
          "benchmark": "miniwob",
          "goal": "Set the sliders to the combination [2,5,7] and submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "afab3de1",
          "task_type": "click-checkboxes",
          "benchmark": "miniwob",
          "goal": "Select WYmcWo and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "check('18')",
            "click('24')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "501012ad",
          "task_type": "click-checkboxes",
          "benchmark": "miniwob",
          "goal": "Select nothing and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "click('Submit')",
            "click('27')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "cc01aa3d",
          "task_type": "click-tab-2-easy",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"arcu\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b44553d9",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"pellentesque\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "700e7dfb",
          "task_type": "highlight-text",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the paragraph below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "4f53e710",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b2ed609c",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "661f19f3",
          "task_type": "click-collapsible-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d10985af",
          "task_type": "click-tab-2-easy",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"adipiscing\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "feb04852",
          "task_type": "guess-number",
          "benchmark": "miniwob",
          "goal": "Guess the number between 0-9 and press Submit. Use the feedback below to find the right number.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "4694769a",
          "task_type": "use-autocomplete",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"Ando\".",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('17', 'Ando')",
            "noop()",
            "click('18')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "fac79783",
          "task_type": "use-autocomplete",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"Mar\" and ends with \"ue\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:40:24 INFO\u001b[0m [GC] Performing periodic GC collection took 0.12 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:40:24 INFO\u001b[0m Pushing weights for policy version 11\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:40:27 INFO\u001b[0m Completed weights push in 2.73 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 19:40:27 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 19:40:33 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 19:40:33 INFO\u001b[0m Weight update completed (now v11)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 11 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 57.0\n  buffer/sample/avg_sampled_policy_age: 0.0\n  buffer/sample/demand_to_size_ratio: 0.10256410256410256\n  buffer/sample/max_sampled_policy_age: 0.0\n  eval/avg_reward: 0.1875\n  eval/num_tasks: 32.0\n  eval/success_rate: 0.1875\n  eval/successes: 6.0\n  generator/generate/count_requests: 325.0\n  generator/generate/count_sequences_completed: 325.0\n  generator_perf/generate/total_duration_avg_s: 0.9499775089205224\n  generator_perf/generate/total_duration_max_s: 4.6229599609375\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.43562583439052105\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.490523338317871\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 0.5727829728275537\n  reference_perf/forward/total_duration_max_s: 0.5736275166273117\n  reward/avg_reward: -0.5025000000000001\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.2\n  rl_trainer/learning_rate: 9.819639278557115e-06\n  rl_trainer/loss: -0.12886583805084229\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.3319473806768656\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.3319473806768656\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.006236983463168144\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.006236983463168144\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.07054218836128712\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.07054218836128712\n  rl_trainer_perf/step/total_duration_avg_s: 0.40873842872679234\n  rl_trainer_perf/step/total_duration_max_s: 0.40873842872679234\n  task/avg_length: 8.2\n  task/avg_reward: 0.15\n  task/count_completed: 40.0\n  task/success_rate: 0.15\n  train/avg_reward: -0.5025000000000001\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n=============================="
          ],
          "has_action_error": false
        },
        {
          "task_id": "85dd6bf1",
          "task_type": "click-button-sequence",
          "benchmark": "miniwob",
          "goal": "Click button ONE, then click button TWO.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('12')",
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "606fcd03",
          "task_type": "email-inbox-reply",
          "benchmark": "miniwob",
          "goal": "Find the email by Zia and reply to them with the text \"In leo.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "click('37')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "814ca693",
          "task_type": "click-tab",
          "benchmark": "miniwob",
          "goal": "Click on Tab #1.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('18')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "be0b2145",
          "task_type": "click-tab-2-medium",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"amet\".",
          "success": false,
          "reward": 0.0,
          "task_length": 1,
          "actions": [
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b20e975b",
          "task_type": "email-inbox-important",
          "benchmark": "miniwob",
          "goal": "Find the email by Ameline and click the star icon to mark it as important.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "36b54d2c",
          "task_type": "highlight-text-2",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the 3rd paragraph and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "130883ec",
          "task_type": "copy-paste",
          "benchmark": "miniwob",
          "goal": "Copy the text in the textarea below, paste it into the textbox and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1951041b",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"adipiscing\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7728c2ce",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"cum\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6a8059b8",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"Tellus\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b9111150",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"maecenas\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "158d0d42",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"risus\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('16')",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "585dbfb3",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"ut.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "cad7eb53",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"Commodo\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c0817148",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"volutpat\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 80 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/80...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/80...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/80...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/80...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/80...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/80...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/80...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/80...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/80...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/80...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/80...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/80...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/80...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/80...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/80...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/80...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/80...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/80...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/80...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/80...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/80...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/80...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/80...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/80...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/80...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/80...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/80...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/80...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/80...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/80...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/80...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/80...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/80...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/80...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/80...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/80...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/80...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/80...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/80...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/80...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/80...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/80...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/80...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/80...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/80...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/80...\u001b[34m[ReferenceModel-0/2] 2025-12-25 19:44:08 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-1/2] 2025-12-25 19:44:08 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.500, std=0.000\n[ADVANTAGES DEBUG] advantages: min=0.00, max=0.00\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:44:09 INFO\u001b[0m [GC] Performing periodic GC collection took 0.16 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:44:10 INFO\u001b[0m Pushing weights for policy version 13\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:44:13 INFO\u001b[0m Completed weights push in 2.59 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 19:44:13 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 19:44:19 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 19:44:19 INFO\u001b[0m Weight update completed (now v13)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 13 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 69.0\n  buffer/sample/avg_sampled_policy_age: 0.125\n  buffer/sample/demand_to_size_ratio: 0.15384615384615385\n  buffer/sample/max_sampled_policy_age: 1.0\n  generator/generate/count_requests: 77.0\n  generator/generate/count_sequences_completed: 77.0\n  generator_perf/generate/total_duration_avg_s: 1.0291647814465807\n  generator_perf/generate/total_duration_max_s: 5.0331484375\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.35219187662005424\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.490509033203125\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 0.5732287336140871\n  reference_perf/forward/total_duration_max_s: 0.5762492697685957\n  reward/avg_reward: -0.5\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.0\n  rl_trainer/learning_rate: 9.779559118236473e-06\n  rl_trainer/loss: 0.029313253238797188\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.4697193391621113\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.4697193391621113\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.009260889142751694\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.009260889142751694\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.10484535619616508\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.10484535619616508\n  rl_trainer_perf/step/total_duration_avg_s: 0.5838280040770769\n  rl_trainer_perf/step/total_duration_max_s: 0.5838280040770769\n  task/avg_length: 9.11111111111111\n  task/avg_reward: 0.1111111111111111\n  task/count_completed: 9.0\n  task/success_rate: 0.1111111111111111\n  train/avg_reward: -0.5\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n==============================\n\n\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/80...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/80...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/80...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/80...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/80...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/80...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/80...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/80...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/80...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/80...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/80...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/80...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/80...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/80...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/80...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/80...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/80...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/80...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/80...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 66/80...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/80...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/80...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 69/80...\n[TRAINER DEBUG] Episode 69 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 70/80...\n[TRAINER DEBUG] Episode 70 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 71/80...\n[TRAINER DEBUG] Episode 71 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 72/80...\n[TRAINER DEBUG] Episode 72 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 73/80...\n[TRAINER DEBUG] Episode 73 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 74/80...\n[TRAINER DEBUG] Episode 74 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 75/80...\n[TRAINER DEBUG] Episode 75 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 76/80...\n[TRAINER DEBUG] Episode 76 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 77/80...\n[TRAINER DEBUG] Episode 77 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 78/80...\n[TRAINER DEBUG] Episode 78 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 79/80...\n[TRAINER DEBUG] Episode 79 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 80/80...\n[TRAINER DEBUG] Episode 80 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 80 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 8: 80 episodes, success rate: 0.0%, avg reward: -0.50"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0a44a76e",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "69150bd1",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d108aec5",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6d38908f",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f295207d",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1ca4fb58",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ef0b8458",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3b84aa0b",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')\n[TRAINER DEBUG] Creating 9 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/9...\n[TRAINER DEBUG] Episode 1 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 2/9...\n[TRAINER DEBUG] Episode 2 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 3/9...\n[TRAINER DEBUG] Episode 3 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 4/9...\n[TRAINER DEBUG] Episode 4 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 5/9...\n[TRAINER DEBUG] Episode 5 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 6/9...\n[TRAINER DEBUG] Episode 6 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 7/9...\n[TRAINER DEBUG] Episode 7 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 8/9...\n[TRAINER DEBUG] Episode 8 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 9/9...\n[TRAINER DEBUG] Episode 9 reward: 2.45\n[TRAINER DEBUG] Getting ref model logprobs for 9 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 9: 9 episodes, success rate: 100.0%, avg reward: 2.44"
          ],
          "has_action_error": false
        },
        {
          "task_id": "00f88e7b",
          "task_type": "scroll-text",
          "benchmark": "miniwob",
          "goal": "Find the last word in the text area, enter it into the text field and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "36fcc9dd",
          "task_type": "scroll-text",
          "benchmark": "miniwob",
          "goal": "Find the last word in the text area, enter it into the text field and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f8683dc9",
          "task_type": "scroll-text",
          "benchmark": "miniwob",
          "goal": "Find the last word in the text area, enter it into the text field and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "57ed22c1",
          "task_type": "scroll-text",
          "benchmark": "miniwob",
          "goal": "Find the last word in the text area, enter it into the text field and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9153cd84",
          "task_type": "scroll-text",
          "benchmark": "miniwob",
          "goal": "Find the last word in the text area, enter it into the text field and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d22d29df",
          "task_type": "scroll-text",
          "benchmark": "miniwob",
          "goal": "Find the last word in the text area, enter it into the text field and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7b883063",
          "task_type": "scroll-text",
          "benchmark": "miniwob",
          "goal": "Find the last word in the text area, enter it into the text field and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d19290da",
          "task_type": "scroll-text",
          "benchmark": "miniwob",
          "goal": "Find the last word in the text area, enter it into the text field and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 80 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/80...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/80...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/80...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/80...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/80...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/80...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/80...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/80...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/80...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/80...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/80...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/80...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/80...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/80...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/80...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/80...\u001b[34m[ReferenceModel-0/2] 2025-12-25 19:48:18 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-1/2] 2025-12-25 19:48:18 INFO\u001b[0m [GC] Performing periodic GC collection took 0.02 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.500, std=0.000\n[ADVANTAGES DEBUG] advantages: min=0.00, max=0.00\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:48:19 INFO\u001b[0m [GC] Performing periodic GC collection took 0.10 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:48:20 INFO\u001b[0m Pushing weights for policy version 15\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 19:48:23 INFO\u001b[0m Completed weights push in 2.57 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 19:48:23 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 19:48:28 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 19:48:28 INFO\u001b[0m Weight update completed (now v15)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 15 ===\n  buffer/add/count_episodes_added: 89.0\n  buffer/evict/sum_episodes_evicted: 70.0\n  buffer/sample/avg_sampled_policy_age: 0.375\n  buffer/sample/demand_to_size_ratio: 1.1426876800542185\n  buffer/sample/max_sampled_policy_age: 1.0\n  generator/generate/count_requests: 86.0\n  generator/generate/count_sequences_completed: 86.0\n  generator_perf/generate/total_duration_avg_s: 1.0297328640250272\n  generator_perf/generate/total_duration_max_s: 4.02228466796875\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.07566646859049797\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 4.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 8.060345649719238\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 0.8957393812015653\n  reference_perf/forward/total_duration_max_s: 1.1779090408235788\n  reward/avg_reward: -0.202808988764045\n  reward/success_rate: 0.10112359550561797\n  reward/sum_reward: -18.050000000000004\n  rl_trainer/learning_rate: 9.739478957915832e-06\n  rl_trainer/loss: -0.4080095887184143\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.552719859406352\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.552719859406352\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013547752052545547\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013547752052545547\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.1510010752826929\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.1510010752826929\n  rl_trainer_perf/step/total_duration_avg_s: 0.7172764707356691\n  rl_trainer_perf/step/total_duration_max_s: 0.7172764707356691\n  task/avg_length: 7.0\n  task/avg_reward: 0.3333333333333333\n  task/count_completed: 12.0\n  task/success_rate: 0.3333333333333333\n  train/avg_reward: 0.9694444444444446\n  train/episodes_per_rollout: 44.5\n  train/success_rate: 0.5\n==============================\n\n\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/80...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/80...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/80...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/80...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/80...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/80...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/80...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/80...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/80...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/80...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/80...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/80...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/80...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/80...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/80...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/80...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/80...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/80...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/80...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/80...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/80...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/80...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/80...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/80...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/80...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/80...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/80...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/80...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/80...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/80...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/80...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/80...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/80...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/80...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/80...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/80...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/80...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/80...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/80...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/80...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/80...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/80...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/80...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/80...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/80...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/80...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/80...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/80...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/80...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 66/80...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/80...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/80...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 69/80...\n[TRAINER DEBUG] Episode 69 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 70/80...\n[TRAINER DEBUG] Episode 70 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 71/80...\n[TRAINER DEBUG] Episode 71 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 72/80...\n[TRAINER DEBUG] Episode 72 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 73/80...\n[TRAINER DEBUG] Episode 73 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 74/80...\n[TRAINER DEBUG] Episode 74 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 75/80...\n[TRAINER DEBUG] Episode 75 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 76/80...\n[TRAINER DEBUG] Episode 76 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 77/80...\n[TRAINER DEBUG] Episode 77 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 78/80...\n[TRAINER DEBUG] Episode 78 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 79/80...\n[TRAINER DEBUG] Episode 79 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 80/80...\n[TRAINER DEBUG] Episode 80 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 80 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 10: 80 episodes, success rate: 0.0%, avg reward: -0.50"
          ],
          "has_action_error": false
        }
      ]
    },
    {
      "step": 15,
      "num_tasks": 32,
      "success_rate": 0.07547169811320754,
      "avg_reward": 0.07547169811320754,
      "results": [
        {
          "task_id": "ddf8cbba",
          "task_type": "email-inbox-noscroll",
          "benchmark": "miniwob",
          "goal": "Find the email by Joeann and reply to them with the text \"Congue. Pharetra.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ab4ccd87",
          "task_type": "email-inbox-noscroll",
          "benchmark": "miniwob",
          "goal": "Find the email by Nissie and forward that email to Darla.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "click('36')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "58e45876",
          "task_type": "email-inbox-noscroll",
          "benchmark": "miniwob",
          "goal": "Find the email by Kiele and forward that email to Clo.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('27')",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c3e6c284",
          "task_type": "email-inbox-noscroll",
          "benchmark": "miniwob",
          "goal": "Find the email by Tiffani and reply to them with the text \"Arcu venenatis. Mattis.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('27')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "cd3554f0",
          "task_type": "email-inbox-noscroll",
          "benchmark": "miniwob",
          "goal": "Find the email by Karon and reply to them with the text \"Nunc,. Sit.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "73896ea2",
          "task_type": "email-inbox-noscroll",
          "benchmark": "miniwob",
          "goal": "Find the email by Michaella and click the star icon to mark it as important.",
          "success": false,
          "reward": 0.0,
          "task_length": 5,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('37')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "110a20ad",
          "task_type": "email-inbox-noscroll",
          "benchmark": "miniwob",
          "goal": "Find the email by Blinny and reply to them with the text \"Quis nunc, rhoncus.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "click('37')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2228be6e",
          "task_type": "click-menu",
          "benchmark": "miniwob",
          "goal": "Select Gabbi>Eryn>Reggi",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('21')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "11b3c593",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "dd337a75",
          "task_type": "use-autocomplete",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"And\" and ends with \"ra\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a83faf62",
          "task_type": "email-inbox-star-reply",
          "benchmark": "miniwob",
          "goal": "Find the email by Janice and reply to them with the text \"Porta vestibulum sit nisl.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "316ba53c",
          "task_type": "read-table",
          "benchmark": "miniwob",
          "goal": "Enter the value of Country into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "dbbaf30c",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"proin\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e312c8a5",
          "task_type": "click-menu",
          "benchmark": "miniwob",
          "goal": "Select Bambie>Shani>Kelsy",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e987040c",
          "task_type": "click-menu-2",
          "benchmark": "miniwob",
          "goal": "Click the \"Menu\" button, and then find and click on the item labeled \"Prev\".",
          "success": true,
          "reward": 1.0,
          "task_length": 3,
          "actions": [
            "click('16')",
            "click('22')",
            "click('26')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d766db76",
          "task_type": "use-autocomplete",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"Ir\" and ends with \"Iran\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9d4846c5",
          "task_type": "enter-time",
          "benchmark": "miniwob",
          "goal": "Enter 8:15 AM as the time and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "410c8b79",
          "task_type": "click-tab-2-hard",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"tortor\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0892cab7",
          "task_type": "highlight-text",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the paragraph below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b834cdb9",
          "task_type": "click-tab-2-medium",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"enim,\".",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0ab09b53",
          "task_type": "use-spinner",
          "benchmark": "miniwob",
          "goal": "Select -2 with the spinner and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1d79110d",
          "task_type": "enter-text",
          "benchmark": "miniwob",
          "goal": "Enter \"Jess\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('14', 'Jess')",
            "noop()",
            "click('15')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "9ba6b31a",
          "task_type": "click-pie-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"E\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d3a4b99f",
          "task_type": "choose-list",
          "benchmark": "miniwob",
          "goal": "Select Ranique from the list and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "65393b9c",
          "task_type": "click-button",
          "benchmark": "miniwob",
          "goal": "Click on the \"next\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('17')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1e378fd4",
          "task_type": "use-slider",
          "benchmark": "miniwob",
          "goal": "Select -18 with the slider and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2d02bcb6",
          "task_type": "click-link",
          "benchmark": "miniwob",
          "goal": "Click on the link \"tempus\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('tempus')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('tempus')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "91b9c672",
          "task_type": "enter-time",
          "benchmark": "miniwob",
          "goal": "Enter 8:20 PM as the time and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e02c3cae",
          "task_type": "drag-circle",
          "benchmark": "miniwob",
          "goal": "Drag the circle right then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f0d47e96",
          "task_type": "guess-number",
          "benchmark": "miniwob",
          "goal": "Guess the number between 0-9 and press Submit. Use the feedback below to find the right number.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6ce1c286",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #5d53f7 with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c16b9408",
          "task_type": "click-checkboxes-soft",
          "benchmark": "miniwob",
          "goal": "Select words similar to aged, camouflage, house, reply, pork and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "53029f0f",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a digit",
          "success": false,
          "reward": 0.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e086f5db",
          "task_type": "highlight-text-2",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the 2nd paragraph and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b05f113d",
          "task_type": "click-pie",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"N\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c03262e6",
          "task_type": "simple-algebra",
          "benchmark": "miniwob",
          "goal": "Solve for x and type your answer into the textbox. Press Submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "fill([16], '98')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "dff0d1ad",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"Maecenas\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click(16)",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "ebd7771e",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Jacquelynn from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "noop()",
            "select('18', 'Jacquelynn')",
            "click('23')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "f0b6a585",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a small blue digit",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "noop()",
            "noop()",
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "28132e21",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a green Y",
          "success": false,
          "reward": 0.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "62b3568a",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a magenta item",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b3bddb93",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a small green item",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b892cf84",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a small black circle",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "4fbe13cb",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a large l",
          "success": false,
          "reward": 0.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d1d5333e",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a small item",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "4be27208",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a small green item",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 55 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/55...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/55...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/55...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/55...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/55...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/55...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/55...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/55...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/55...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/55...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/55...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/55...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/55...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/55...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/55...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/55...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/55...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/55...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/55...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/55...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/55...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/55...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/55...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/55...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/55...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/55...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/55...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/55...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/55...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/55...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/55...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/55...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/55...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/55...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/55...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/55...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/55...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/55...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/55...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/55...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/55...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/55...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/55...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/55...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/55...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/55...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/55...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/55...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/55...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/55...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/55...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/55...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/55...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/55...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/55...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 55 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 12: 55 episodes, success rate: 0.0%, avg reward: -0.50"
          ],
          "has_action_error": false
        },
        {
          "task_id": "4cb07f19",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"amet\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a2f4c139",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"sem\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "25327062",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"proin\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "cd97136d",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"hac\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click(16)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "f56b85e9",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"cursus\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b0cdc59a",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"lectus.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('16')",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bd2cc490",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"in\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 80 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/80...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/80...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/80...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/80...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/80...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/80...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/80...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/80...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/80...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/80...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/80...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/80...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/80...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/80...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/80...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/80...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/80...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/80...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/80...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/80...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/80...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/80...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/80...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/80...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/80...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/80...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/80...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/80...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/80...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/80...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/80...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/80...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/80...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/80...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/80...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/80...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/80...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/80...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/80...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/80...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/80...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/80...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/80...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/80...\n[TRAINER DEBUG] Episode 44 reward: -0.5\u001b[34m[ReferenceModel-0/2] 2025-12-25 20:08:44 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-1/2] 2025-12-25 20:08:44 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.501, std=0.011\n[ADVANTAGES DEBUG] advantages: min=-8.75, max=0.11\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 20:08:45 INFO\u001b[0m [GC] Performing periodic GC collection took 0.10 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 20:08:45 INFO\u001b[0m Pushing weights for policy version 20\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 20:08:48 INFO\u001b[0m Completed weights push in 2.45 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 20:08:49 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 20:08:54 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 20:08:54 INFO\u001b[0m Weight update completed (now v20)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 20 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 46.0\n  buffer/sample/avg_sampled_policy_age: 0.0\n  buffer/sample/demand_to_size_ratio: 0.11764705882352941\n  buffer/sample/max_sampled_policy_age: 0.0\n  generator/generate/count_requests: 78.0\n  generator/generate/count_sequences_completed: 78.0\n  generator_perf/generate/total_duration_avg_s: 1.0240813794258317\n  generator_perf/generate/total_duration_max_s: 5.0791923828125\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.9540446680039167\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.490532875061035\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 0.5972369657829404\n  reference_perf/forward/total_duration_max_s: 0.6011211071163416\n  reward/avg_reward: -0.50125\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.1\n  rl_trainer/learning_rate: 9.63927855711423e-06\n  rl_trainer/loss: -0.07478500157594681\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.4536920040845871\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.4536920040845871\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.01368732750415802\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.01368732750415802\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.15050929598510265\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.15050929598510265\n  rl_trainer_perf/step/total_duration_avg_s: 0.6178973764181137\n  rl_trainer_perf/step/total_duration_max_s: 0.6178973764181137\n  task/avg_length: 10.0\n  task/avg_reward: 0.0\n  task/count_completed: 8.0\n  task/success_rate: 0.0\n  train/avg_reward: -0.50125\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n==============================\n\n\n[TRAINER DEBUG] Computing reward for episode 45/80...\n[TRAINER DEBUG] Episode 45 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 46/80...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/80...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/80...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/80...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/80...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/80...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/80...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/80...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/80...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/80...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/80...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/80...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/80...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/80...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/80...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/80...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/80...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/80...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/80...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/80...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 66/80...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/80...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/80...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 69/80...\n[TRAINER DEBUG] Episode 69 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 70/80...\n[TRAINER DEBUG] Episode 70 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 71/80...\n[TRAINER DEBUG] Episode 71 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 72/80...\n[TRAINER DEBUG] Episode 72 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 73/80...\n[TRAINER DEBUG] Episode 73 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 74/80...\n[TRAINER DEBUG] Episode 74 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 75/80...\n[TRAINER DEBUG] Episode 75 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 76/80...\n[TRAINER DEBUG] Episode 76 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 77/80...\n[TRAINER DEBUG] Episode 77 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 78/80...\n[TRAINER DEBUG] Episode 78 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 79/80...\n[TRAINER DEBUG] Episode 79 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 80/80...\n[TRAINER DEBUG] Episode 80 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 80 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 13: 80 episodes, success rate: 0.0%, avg reward: -0.50"
          ],
          "has_action_error": false
        }
      ]
    },
    {
      "step": 20,
      "num_tasks": 32,
      "success_rate": 0.14545454545454545,
      "avg_reward": 0.14545454545454545,
      "results": [
        {
          "task_id": "ff74bcca",
          "task_type": "click-color",
          "benchmark": "miniwob",
          "goal": "Click on the cyan colored box.",
          "success": true,
          "reward": 1.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "click('12')",
            "noop()",
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "edf2bd46",
          "task_type": "click-color",
          "benchmark": "miniwob",
          "goal": "Click on the purple colored box.",
          "success": false,
          "reward": 0.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5b43f3a0",
          "task_type": "click-color",
          "benchmark": "miniwob",
          "goal": "Click on the pink colored box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7eb39741",
          "task_type": "click-color",
          "benchmark": "miniwob",
          "goal": "Click on the white colored box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "db1112ef",
          "task_type": "click-color",
          "benchmark": "miniwob",
          "goal": "Click on the lime colored box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bd3af552",
          "task_type": "click-color",
          "benchmark": "miniwob",
          "goal": "Click on the blue colored box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d0159ff1",
          "task_type": "click-color",
          "benchmark": "miniwob",
          "goal": "Click on the orange colored box.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "noop()",
            "noop()",
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "52052232",
          "task_type": "drag-box",
          "benchmark": "miniwob",
          "goal": "Drag the smaller box so that it is completely inside the larger box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "82797c0f",
          "task_type": "click-menu",
          "benchmark": "miniwob",
          "goal": "Select Blaire>Gray>Corissa",
          "success": false,
          "reward": 0.0,
          "task_length": 1,
          "actions": [
            "click('25')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "632f3f7a",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Harri from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "noop()",
            "select('15')",
            "click('25')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "3e46812e",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select 1n3m and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "select('24', '1n3m')",
            "click('27')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "b09d3f2c",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"risus.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b0d2a076",
          "task_type": "copy-paste",
          "benchmark": "miniwob",
          "goal": "Copy the text in the textarea below, paste it into the textbox and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "847d6684",
          "task_type": "enter-text-2",
          "benchmark": "miniwob",
          "goal": "Type \"RENDA\" in all lower case letters in the text input and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 4,
          "actions": [
            "type('14', 'RENDA')",
            "noop()",
            "noop()",
            "click('15')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "64ee8c7e",
          "task_type": "highlight-text-2",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the 1st paragraph and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0bdb49ef",
          "task_type": "highlight-text",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the paragraph below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d744aea7",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ad29822e",
          "task_type": "click-widget",
          "benchmark": "miniwob",
          "goal": "Click on a \"checkbox\" widget.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('19')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "53a1121a",
          "task_type": "highlight-text-2",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the 3rd paragraph and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b8c88362",
          "task_type": "enter-date",
          "benchmark": "miniwob",
          "goal": "Enter 05/22/2011 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0da187bb",
          "task_type": "enter-password",
          "benchmark": "miniwob",
          "goal": "Enter the password \"QMj\" into both text fields and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "fa8d27af",
          "task_type": "click-tab-2-hard",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"facilisi\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a7797374",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"iaculis\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b165a3fb",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f7ad0306",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"Ut\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5ca6bbae",
          "task_type": "click-shades",
          "benchmark": "miniwob",
          "goal": "Select all the shades of green and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d44e66ad",
          "task_type": "click-tab-2-medium",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"interdum\".",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e853b839",
          "task_type": "use-autocomplete",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"Cy\" and ends with \"rus\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2d4aa9df",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "720c05a0",
          "task_type": "click-tab-2-medium",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"proin\".",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "noop()",
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "35e49107",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0cbe76b8",
          "task_type": "find-word",
          "benchmark": "miniwob",
          "goal": "Find the 5th word in the paragraph, type that into the textbox and press \"Submit\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bd412651",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"Eget\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b9338fff",
          "task_type": "simple-algebra",
          "benchmark": "miniwob",
          "goal": "Solve for x and type your answer into the textbox. Press Submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2eabb17b",
          "task_type": "use-colorwheel",
          "benchmark": "miniwob",
          "goal": "Select maroon with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "22a0020c",
          "task_type": "click-shades",
          "benchmark": "miniwob",
          "goal": "Select all the shades of red and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6bdb54ee",
          "task_type": "email-inbox-delete",
          "benchmark": "miniwob",
          "goal": "Find the email by Cosette and click the trash icon to delete it.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\u001b[34m[TitanTrainer-0/1] 2025-12-25 20:22:39 INFO\u001b[0m [GC] Performing periodic GC collection took 0.11 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 20:22:39 INFO\u001b[0m Pushing weights for policy version 21\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 20:22:42 INFO\u001b[0m Completed weights push in 2.54 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 20:22:44 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 20:22:49 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 20:22:49 INFO\u001b[0m Weight update completed (now v21)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 21 ===\n  buffer/add/count_episodes_added: 58.0\n  buffer/evict/sum_episodes_evicted: 10.0\n  buffer/sample/avg_sampled_policy_age: 0.375\n  buffer/sample/demand_to_size_ratio: 0.06349206349206349\n  buffer/sample/max_sampled_policy_age: 1.0\n  eval/avg_reward: 0.09375\n  eval/num_tasks: 32.0\n  eval/success_rate: 0.09375\n  eval/successes: 3.0\n  generator/generate/count_requests: 307.0\n  generator/generate/count_sequences_completed: 307.0\n  generator_perf/generate/total_duration_avg_s: 0.9675898479250435\n  generator_perf/generate/total_duration_max_s: 4.807734375\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.9457806628197432\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 10.505598068237305\n  reference_perf/forward/memory_peak_max_gb: 26.85078239440918\n  reference_perf/forward/total_duration_avg_s: 1.1811464196071029\n  reference_perf/forward/total_duration_max_s: 1.193317299708724\n  reward/avg_reward: -0.30689655172413793\n  reward/success_rate: 0.06896551724137931\n  reward/sum_reward: -17.8\n  rl_trainer/learning_rate: 9.619238476953909e-06\n  rl_trainer/loss: 0.16624772548675537\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.5571135245263577\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.5571135245263577\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013699941337108612\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013699941337108612\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.1510378122329712\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.1510378122329712\n  rl_trainer_perf/step/total_duration_avg_s: 0.7218585275113583\n  rl_trainer_perf/step/total_duration_max_s: 0.7218585275113583\n  task/avg_length: 7.512195121951219\n  task/avg_reward: 0.12195121951219512\n  task/count_completed: 41.0\n  task/success_rate: 0.12195121951219512\n  train/avg_reward: -0.30689655172413793\n  train/episodes_per_rollout: 58.0\n  train/success_rate: 0.06896551724137931\n=============================="
          ],
          "has_action_error": false
        },
        {
          "task_id": "e01890fb",
          "task_type": "choose-list",
          "benchmark": "miniwob",
          "goal": "Select Bonaire from the list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "select('15')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('15')",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "2b8f9343",
          "task_type": "click-button",
          "benchmark": "miniwob",
          "goal": "Click on the \"Previous\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('15')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b2b1f899",
          "task_type": "click-collapsible",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('16')",
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0e577479",
          "task_type": "click-collapsible",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "click('16')",
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2c6dd1eb",
          "task_type": "click-collapsible",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2bd411a6",
          "task_type": "click-collapsible",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 3,
          "actions": [
            "click('16')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2971422f",
          "task_type": "click-collapsible",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "be6573ce",
          "task_type": "click-collapsible",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click(16)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "c5b014aa",
          "task_type": "click-collapsible",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 3,
          "actions": [
            "click('16')",
            "noop()",
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "65cb5812",
          "task_type": "click-collapsible",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 52 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/52...\n[TRAINER DEBUG] Episode 1 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 2/52...\n[TRAINER DEBUG] Episode 2 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 3/52...\n[TRAINER DEBUG] Episode 3 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 4/52...\n[TRAINER DEBUG] Episode 4 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 5/52...\n[TRAINER DEBUG] Episode 5 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 6/52...\n[TRAINER DEBUG] Episode 6 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 7/52...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/52...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/52...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/52...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/52...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/52...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/52...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/52...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/52...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/52...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/52...\n[TRAINER DEBUG] Episode 17 reward: 2.35\n[TRAINER DEBUG] Computing reward for episode 18/52...\n[TRAINER DEBUG] Episode 18 reward: 2.35\n[TRAINER DEBUG] Computing reward for episode 19/52...\n[TRAINER DEBUG] Episode 19 reward: 2.35\n[TRAINER DEBUG] Computing reward for episode 20/52...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/52...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/52...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/52...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/52...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/52...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/52...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/52...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/52...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/52...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/52...\n[TRAINER DEBUG] Episode 30 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 31/52...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/52...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/52...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/52...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/52...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/52...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/52...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/52...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/52...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/52...\n[TRAINER DEBUG] Episode 40 reward: 2.35\n[TRAINER DEBUG] Computing reward for episode 41/52...\n[TRAINER DEBUG] Episode 41 reward: 2.35\n[TRAINER DEBUG] Computing reward for episode 42/52...\n[TRAINER DEBUG] Episode 42 reward: 2.35\n[TRAINER DEBUG] Computing reward for episode 43/52...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/52...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/52...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/52...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/52...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/52...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/52...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/52...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/52...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/52...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 52 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\u001b[34m[TitanTrainer-0/1] 2025-12-25 20:24:56 INFO\u001b[0m [GC] Performing periodic GC collection took 0.13 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 20:24:57 INFO\u001b[0m Pushing weights for policy version 23\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 20:24:59 INFO\u001b[0m Completed weights push in 2.43 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 20:25:00 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 20:25:05 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 20:25:05 INFO\u001b[0m Weight update completed (now v23)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 23 ===\n  buffer/add/count_episodes_added: 52.0\n  buffer/evict/sum_episodes_evicted: 54.0\n  buffer/sample/avg_sampled_policy_age: 0.25\n  buffer/sample/demand_to_size_ratio: 0.4\n  buffer/sample/max_sampled_policy_age: 1.0\n  generator/generate/count_requests: 49.0\n  generator/generate/count_sequences_completed: 49.0\n  generator_perf/generate/total_duration_avg_s: 0.9994643118722101\n  generator_perf/generate/total_duration_max_s: 5.2636396484375\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.9379905331879854\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 9.418825149536133\n  reference_perf/forward/memory_peak_max_gb: 24.242542266845703\n  reference_perf/forward/total_duration_avg_s: 1.21837087161839\n  reference_perf/forward/total_duration_max_s: 1.2575359474867582\n  reward/avg_reward: 0.15384615384615385\n  reward/success_rate: 0.23076923076923078\n  reward/sum_reward: 8.0\n  rl_trainer/learning_rate: 9.579158316633267e-06\n  rl_trainer/loss: -0.010384023189544678\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.4460740927606821\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.4460740927606821\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013733990490436554\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013733990490436554\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.15086545795202255\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.15086545795202255\n  rl_trainer_perf/step/total_duration_avg_s: 0.61067577265203\n  rl_trainer_perf/step/total_duration_max_s: 0.61067577265203\n  task/avg_length: 7.142857142857143\n  task/avg_reward: 0.42857142857142855\n  task/count_completed: 7.0\n  task/success_rate: 0.42857142857142855\n  train/avg_reward: 0.15384615384615385\n  train/episodes_per_rollout: 52.0\n  train/success_rate: 0.23076923076923078\n==============================\n\n\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 15: 52 episodes, success rate: 23.1%, avg reward: 0.15"
          ],
          "has_action_error": false
        },
        {
          "task_id": "09630b23",
          "task_type": "choose-date-medium",
          "benchmark": "miniwob",
          "goal": "Select 11/20/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 5,
          "actions": [
            "noop()",
            "noop()",
            "type('17', '11/20/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "5f1e8bff",
          "task_type": "choose-date-medium",
          "benchmark": "miniwob",
          "goal": "Select 12/23/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 5,
          "actions": [
            "noop()",
            "noop()",
            "fill('17', '12/23/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "32a2b8ed",
          "task_type": "choose-date-medium",
          "benchmark": "miniwob",
          "goal": "Select 11/14/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type(17, '11/14/2016')",
            "noop()",
            "noop()",
            "click(19)",
            "noop()",
            "noop()",
            "noop()",
            "fill(17, '11/14/2016')",
            "noop()",
            "click(19)"
          ],
          "has_action_error": true
        },
        {
          "task_id": "c0fd927c",
          "task_type": "choose-date-medium",
          "benchmark": "miniwob",
          "goal": "Select 12/06/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 7,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "type('17', '12/06/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "a35edbaa",
          "task_type": "choose-date-medium",
          "benchmark": "miniwob",
          "goal": "Select 11/28/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 5,
          "actions": [
            "noop()",
            "noop()",
            "type('17', '11/28/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "879e5093",
          "task_type": "choose-date-medium",
          "benchmark": "miniwob",
          "goal": "Select 11/01/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "fill('[17]', '11/01/2016')",
            "noop()",
            "click('[19]')",
            "noop()",
            "noop()",
            "noop()",
            "type('[17]', '11/01/2016')",
            "noop()",
            "click('[19]')",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "3aa9c1ef",
          "task_type": "choose-date-medium",
          "benchmark": "miniwob",
          "goal": "Select 12/17/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 5,
          "actions": [
            "noop()",
            "noop()",
            "fill('17', '12/17/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "17f058eb",
          "task_type": "choose-date-medium",
          "benchmark": "miniwob",
          "goal": "Select 11/16/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "type([17], '11/16/2016')",
            "noop()",
            "click([19])",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 57 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/57...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/57...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/57...\n[TRAINER DEBUG] Episode 3 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 4/57...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/57...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/57...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/57...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/57...\n[TRAINER DEBUG] Episode 8 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 9/57...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/57...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/57...\n[TRAINER DEBUG] Episode 11 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 12/57...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/57...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/57...\n[TRAINER DEBUG] Episode 14 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 15/57...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/57...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/57...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/57...\n[TRAINER DEBUG] Episode 18 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 19/57...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/57...\n[TRAINER DEBUG] Episode 20 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 21/57...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/57...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/57...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/57...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/57...\n[TRAINER DEBUG] Episode 25 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 26/57...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/57...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/57...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/57...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/57...\n[TRAINER DEBUG] Episode 30 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 31/57...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/57...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/57...\n[TRAINER DEBUG] Episode 33 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 34/57...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/57...\n[TRAINER DEBUG] Episode 35 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 36/57...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/57...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/57...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/57...\n[TRAINER DEBUG] Episode 39 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 40/57...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/57...\n[TRAINER DEBUG] Episode 41 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 42/57...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/57...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/57...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/57...\n[TRAINER DEBUG] Episode 45 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 46/57...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/57...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/57...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/57...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/57...\u001b[34m[ReferenceModel-0/2] 2025-12-25 20:27:14 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-1/2] 2025-12-25 20:27:14 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.6, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.6, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.6, -0.5, -0.6, -0.5, -0.5, -0.5, -0.6, -0.5, -0.6, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.526, std=0.044\n[ADVANTAGES DEBUG] advantages: min=-1.65, max=0.59\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 20:27:17 INFO\u001b[0m [GC] Performing periodic GC collection took 0.12 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 20:27:17 INFO\u001b[0m Pushing weights for policy version 25\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 20:27:20 INFO\u001b[0m Completed weights push in 2.41 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 20:27:20 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 20:27:25 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 20:27:25 INFO\u001b[0m Weight update completed (now v25)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 25 ===\n  buffer/add/count_episodes_added: 57.0\n  buffer/evict/sum_episodes_evicted: 43.0\n  buffer/sample/avg_sampled_policy_age: 0.0\n  buffer/sample/demand_to_size_ratio: 0.14545454545454545\n  buffer/sample/max_sampled_policy_age: 0.0\n  generator/generate/count_requests: 54.0\n  generator/generate/count_sequences_completed: 54.0\n  generator_perf/generate/total_duration_avg_s: 0.9207816569010419\n  generator_perf/generate/total_duration_max_s: 4.2673818359375\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.4115843642503023\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 10.324492454528809\n  reference_perf/forward/memory_peak_max_gb: 26.416075706481934\n  reference_perf/forward/total_duration_avg_s: 1.1666358336806297\n  reference_perf/forward/total_duration_max_s: 1.1818159203976393\n  reward/avg_reward: -0.5263157894736843\n  reward/success_rate: 0.0\n  reward/sum_reward: -30.000000000000007\n  rl_trainer/learning_rate: 9.539078156312625e-06\n  rl_trainer/loss: -0.28137946128845215\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.5564207043498755\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.5564207043498755\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.01376444660127163\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.01376444660127163\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.15092754364013672\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.15092754364013672\n  rl_trainer_perf/step/total_duration_avg_s: 0.721123119816184\n  rl_trainer_perf/step/total_duration_max_s: 0.721123119816184\n  task/avg_length: 7.428571428571429\n  task/avg_reward: 0.0\n  task/count_completed: 7.0\n  task/success_rate: 0.0\n  train/avg_reward: -0.5263157894736842\n  train/episodes_per_rollout: 57.0\n  train/success_rate: 0.0\n==============================\n\n\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/57...\n[TRAINER DEBUG] Episode 51 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 52/57...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/57...\n[TRAINER DEBUG] Episode 53 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 54/57...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/57...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/57...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/57...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 57 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 16: 57 episodes, success rate: 0.0%, avg reward: -0.53"
          ],
          "has_action_error": true
        }
      ]
    },
    {
      "step": 25,
      "num_tasks": 32,
      "success_rate": 0.2,
      "avg_reward": 0.2,
      "results": [
        {
          "task_id": "a4def431",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #987529 with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "12443014",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #2c9143 with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "822ce913",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #39dead with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5e530388",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #b14dbb with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0d6d31fe",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #1e2719 with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ed26de47",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #8d3214 with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "72da3ad7",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #08ec9e with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6d06a5d0",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"et\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "12406253",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"Tempus\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('18')",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2f2f065a",
          "task_type": "click-collapsible",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 7,
          "actions": [
            "noop()",
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "54aeeb22",
          "task_type": "enter-text-dynamic",
          "benchmark": "miniwob",
          "goal": "Enter \"z\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('14', 'z')",
            "noop()",
            "click('15')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "96d96647",
          "task_type": "click-menu-2",
          "benchmark": "miniwob",
          "goal": "Click the \"Menu\" button, and then find and click on the item with the \"ui-icon-zoomin\" icon.",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "click('16')",
            "click('44')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f4dcd622",
          "task_type": "click-tab-2",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"Sit\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "80b0bf01",
          "task_type": "simple-algebra",
          "benchmark": "miniwob",
          "goal": "Solve for x and type your answer into the textbox. Press Submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type(16, '63')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "4bb91d5d",
          "task_type": "click-tab-2-medium",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"Integer.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 1,
          "actions": [
            "click('21')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0356bfc5",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d25278fd",
          "task_type": "drag-single-shape",
          "benchmark": "miniwob",
          "goal": "Drag the item left then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f415671c",
          "task_type": "simple-algebra",
          "benchmark": "miniwob",
          "goal": "Solve for x and type your answer into the textbox. Press Submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "type('[16]', '-12')",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "3c29e1df",
          "task_type": "click-pie-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"A\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "click('+')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "636b11b8",
          "task_type": "read-table",
          "benchmark": "miniwob",
          "goal": "Enter the value of Color into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "fill('16', 'Color')",
            "noop()",
            "click('32')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "6a8a1f76",
          "task_type": "email-inbox-reply",
          "benchmark": "miniwob",
          "goal": "Find the email by Nana and reply to them with the text \"Turpis. Neque blandit sodales.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d88eab7c",
          "task_type": "click-checkboxes",
          "benchmark": "miniwob",
          "goal": "Select TvIn, svBMq and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e89f7ab3",
          "task_type": "click-checkboxes-soft",
          "benchmark": "miniwob",
          "goal": "Select words similar to small and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "check('24')",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "3cde06a7",
          "task_type": "click-checkboxes-transfer",
          "benchmark": "miniwob",
          "goal": "Select nothing and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('Submit')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('Submit')",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "e16c6dc9",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00c5\u008f\u2013\u00e6\u00b6\u02c6\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('19')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "973542fa",
          "task_type": "find-word",
          "benchmark": "miniwob",
          "goal": "Find the 3rd word in the paragraph, type that into the textbox and press \"Submit\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2f61448c",
          "task_type": "click-tab-2-medium",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"quam.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "click(20)",
            "noop()",
            "noop()",
            "click(18)",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "c4638d98",
          "task_type": "click-pie-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"l\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "734a7d65",
          "task_type": "scroll-text-2",
          "benchmark": "miniwob",
          "goal": "Scroll the textarea to the bottom of the text hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "893dfbbb",
          "task_type": "drag-single-shape",
          "benchmark": "miniwob",
          "goal": "Drag the item right then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "37401540",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a507f658",
          "task_type": "count-sides",
          "benchmark": "miniwob",
          "goal": "Press the button that correctly denotes how many sides the shape has.\u001b[34m[TitanTrainer-0/1] 2025-12-25 20:41:53 INFO\u001b[0m [GC] Performing periodic GC collection took 0.13 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 20:41:54 INFO\u001b[0m Pushing weights for policy version 26\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 20:41:56 INFO\u001b[0m Completed weights push in 2.46 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 20:41:57 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 20:42:02 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 20:42:02 INFO\u001b[0m Weight update completed (now v26)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 26 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 11.0\n  buffer/sample/avg_sampled_policy_age: 0.5\n  buffer/sample/demand_to_size_ratio: 0.06451612903225806\n  buffer/sample/max_sampled_policy_age: 1.0\n  eval/avg_reward: 0.125\n  eval/num_tasks: 32.0\n  eval/success_rate: 0.125\n  eval/successes: 4.0\n  generator/generate/count_requests: 333.0\n  generator/generate/count_sequences_completed: 333.0\n  generator_perf/generate/total_duration_avg_s: 0.9425501073888836\n  generator_perf/generate/total_duration_max_s: 4.2105302734375\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.5096084009855986\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.490530967712402\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 0.6300624758005142\n  reference_perf/forward/total_duration_max_s: 0.6782851479947567\n  reward/avg_reward: -0.5\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.0\n  rl_trainer/learning_rate: 9.519038076152304e-06\n  rl_trainer/loss: 0.3033145070075989\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.5361342485994101\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.5361342485994101\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.009136481210589409\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.009136481210589409\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.13653387688100338\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.13653387688100338\n  rl_trainer_perf/step/total_duration_avg_s: 0.6818064544349909\n  rl_trainer_perf/step/total_duration_max_s: 0.6818064544349909\n  task/avg_length: 8.325\n  task/avg_reward: 0.1\n  task/count_completed: 40.0\n  task/success_rate: 0.1\n  train/avg_reward: -0.5\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n==============================",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "fcd3aa6b",
          "task_type": "count-shape",
          "benchmark": "miniwob",
          "goal": "How many black shapes are there?",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bd8581df",
          "task_type": "click-dialog-2",
          "benchmark": "miniwob",
          "goal": "Click the button in the dialog box labeled \"x\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d54116ce",
          "task_type": "scroll-text",
          "benchmark": "miniwob",
          "goal": "Find the last word in the text area, enter it into the text field and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3b8f317b",
          "task_type": "copy-paste-2",
          "benchmark": "miniwob",
          "goal": "Copy the text from the 2nd text area below and paste it into the text input, then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d9c333d1",
          "task_type": "drag-circle",
          "benchmark": "miniwob",
          "goal": "Drag the circle up then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "985e0418",
          "task_type": "text-editor",
          "benchmark": "miniwob",
          "goal": "Using the text editor, give the text Justo,. the style underlined and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "4e808bf1",
          "task_type": "choose-date-medium",
          "benchmark": "miniwob",
          "goal": "Select 11/12/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "type('17', '11/12/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "11680b26",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "fill([15], '2')",
            "click([16])",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "1b3a60b6",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 4,
          "actions": [
            "type('15', '7')",
            "noop()",
            "noop()",
            "click('16')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "472c34ff",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "fill('[15]', '56')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "0aa1e441",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 5,
          "actions": [
            "click('15')",
            "noop()",
            "type('15', '10')",
            "noop()",
            "click('16')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "704e9cb3",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "fill([15], '28')",
            "click([16])",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "fbe03722",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "fill([15], \"-3\")",
            "noop()",
            "click([16])",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "type([15], \"-3\")",
            "click([16])",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "5dc524db",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "fill('[15]', '56')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('[16]')",
            "type('[15]', '56')",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "6878f5e8",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "type('15', '2')",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 69 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/69...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/69...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/69...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/69...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/69...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/69...\n[TRAINER DEBUG] Episode 6 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 7/69...\n[TRAINER DEBUG] Episode 7 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 8/69...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/69...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/69...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/69...\n[TRAINER DEBUG] Episode 11 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 12/69...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/69...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/69...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/69...\n[TRAINER DEBUG] Episode 15 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 16/69...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/69...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/69...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/69...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/69...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/69...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/69...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/69...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/69...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/69...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/69...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/69...\n[TRAINER DEBUG] Episode 27 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 28/69...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/69...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/69...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/69...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/69...\n[TRAINER DEBUG] Episode 32 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 33/69...\n[TRAINER DEBUG] Episode 33 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 34/69...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/69...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/69...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/69...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/69...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/69...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/69...\n[TRAINER DEBUG] Episode 40 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 41/69...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/69...\u001b[34m[ReferenceModel-0/2] 2025-12-25 20:44:46 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-1/2] 2025-12-25 20:44:46 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.6, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.6, -0.6, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.6, -0.6, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.6, -0.6, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.522, std=0.042\n[ADVANTAGES DEBUG] advantages: min=-1.88, max=0.52\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 20:44:47 INFO\u001b[0m [GC] Performing periodic GC collection took 0.11 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 20:44:48 INFO\u001b[0m Pushing weights for policy version 28\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 20:44:50 INFO\u001b[0m Completed weights push in 2.31 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 20:44:51 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 20:44:56 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 20:44:56 INFO\u001b[0m Weight update completed (now v28)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 28 ===\n  buffer/add/count_episodes_added: 69.0\n  buffer/evict/sum_episodes_evicted: 76.0\n  buffer/sample/avg_sampled_policy_age: 0.0\n  buffer/sample/demand_to_size_ratio: 0.12121212121212122\n  buffer/sample/max_sampled_policy_age: 0.0\n  generator/generate/count_requests: 66.0\n  generator/generate/count_sequences_completed: 66.0\n  generator_perf/generate/total_duration_avg_s: 0.9444836675470525\n  generator_perf/generate/total_duration_max_s: 4.019154541015625\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.3137756045907736\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 12.498053550720215\n  reference_perf/forward/memory_peak_max_gb: 31.632555961608887\n  reference_perf/forward/total_duration_avg_s: 1.2130840793251991\n  reference_perf/forward/total_duration_max_s: 1.2654869556427002\n  reward/avg_reward: -0.5217391304347827\n  reward/success_rate: 0.0\n  reward/sum_reward: -36.00000000000001\n  rl_trainer/learning_rate: 9.478957915831663e-06\n  rl_trainer/loss: 0.19034507870674133\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.5586533471941948\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.5586533471941948\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013837523758411407\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013837523758411407\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.1491907611489296\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.1491907611489296\n  rl_trainer_perf/step/total_duration_avg_s: 0.7216845843940973\n  rl_trainer_perf/step/total_duration_max_s: 0.7216845843940973\n  task/avg_length: 7.1\n  task/avg_reward: 0.2\n  task/count_completed: 10.0\n  task/success_rate: 0.2\n  train/avg_reward: -0.5217391304347826\n  train/episodes_per_rollout: 69.0\n  train/success_rate: 0.0\n==============================\n\n\n[TRAINER DEBUG] Episode 42 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 43/69...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/69...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/69...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/69...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/69...\n[TRAINER DEBUG] Episode 47 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 48/69...\n[TRAINER DEBUG] Episode 48 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 49/69...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/69...\n[TRAINER DEBUG] Episode 50 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 51/69...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/69...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/69...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/69...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/69...\n[TRAINER DEBUG] Episode 55 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 56/69...\n[TRAINER DEBUG] Episode 56 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 57/69...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/69...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/69...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/69...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/69...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/69...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/69...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/69...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/69...\n[TRAINER DEBUG] Episode 65 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 66/69...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/69...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/69...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 69/69...\n[TRAINER DEBUG] Episode 69 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 69 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 18: 69 episodes, success rate: 0.0%, avg reward: -0.52"
          ],
          "has_action_error": true
        },
        {
          "task_id": "efb5320c",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f9de1d80",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "047a2c6f",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1c08bba7",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b459339f",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "4544da73",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "19e1fc45",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a4672ab6",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('13')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('13')",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 17 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/17...\n[TRAINER DEBUG] Episode 1 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 2/17...\n[TRAINER DEBUG] Episode 2 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 3/17...\n[TRAINER DEBUG] Episode 3 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 4/17...\n[TRAINER DEBUG] Episode 4 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 5/17...\n[TRAINER DEBUG] Episode 5 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 6/17...\n[TRAINER DEBUG] Episode 6 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 7/17...\n[TRAINER DEBUG] Episode 7 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 8/17...\n[TRAINER DEBUG] Episode 8 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 9/17...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/17...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/17...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/17...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/17...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/17...\n[TRAINER DEBUG] Episode 14 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 15/17...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/17...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/17...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 17 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 19: 17 episodes, success rate: 41.2%, avg reward: 0.70"
          ],
          "has_action_error": true
        }
      ]
    },
    {
      "step": 30,
      "num_tasks": 32,
      "success_rate": 0.2777777777777778,
      "avg_reward": 0.2777777777777778,
      "results": [
        {
          "task_id": "2b63c793",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/29/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('17')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3b51448e",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/06/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click(17)",
            "type(17, '12/06/2016')",
            "noop()",
            "click(19)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "fill(17, '12/06/2016')",
            "click(19)"
          ],
          "has_action_error": true
        },
        {
          "task_id": "6fa6031a",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/24/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 8,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "type('17', '12/24/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "238caea8",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/18/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('17', '12/18/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "b8add2ec",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/06/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "type('17', '12/06/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "55182cc6",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/27/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 6,
          "actions": [
            "noop()",
            "noop()",
            "type('17', '12/27/2016')",
            "noop()",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "3fa2bf6d",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/05/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 5,
          "actions": [
            "noop()",
            "noop()",
            "fill('17', '12/05/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "14ffc035",
          "task_type": "click-link",
          "benchmark": "miniwob",
          "goal": "Click on the link \"vel\".",
          "success": false,
          "reward": 0.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6f5885ca",
          "task_type": "read-table-2",
          "benchmark": "miniwob",
          "goal": "Enter the value that corresponds with each label into the form and submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "63d2affb",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a digit",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "fde27fb2",
          "task_type": "click-checkboxes-large",
          "benchmark": "miniwob",
          "goal": "Select 2JfV, JNz, ClER, avWBi, RD8Yu, dTcWAqb, vW7Z, VHICU, bi and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2dcdeccc",
          "task_type": "enter-password",
          "benchmark": "miniwob",
          "goal": "Enter the password \"89\" into both text fields and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "dd456a7a",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Czech Republic, Chile from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "28285c7c",
          "task_type": "text-editor",
          "benchmark": "miniwob",
          "goal": "Using the text editor, give everything the style italics and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5193e37b",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"Enim\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d08e8ff5",
          "task_type": "choose-date-medium",
          "benchmark": "miniwob",
          "goal": "Select 11/30/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 6,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "type('17', '11/30/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "839c2795",
          "task_type": "enter-text-2",
          "benchmark": "miniwob",
          "goal": "Type \"ignacio\" in all upper case letters in the text input and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "fill(14, \"IGNACIO\")",
            "noop()",
            "noop()",
            "click(15)",
            "noop()",
            "noop()",
            "fill(14, \"IGNACIO\")",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "8a350633",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": true,
          "reward": 1.0,
          "task_length": 5,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('18')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9b0d3131",
          "task_type": "guess-number",
          "benchmark": "miniwob",
          "goal": "Guess the number between 0-9 and press Submit. Use the feedback below to find the right number.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c9829587",
          "task_type": "click-collapsible-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('18')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "fe3252ae",
          "task_type": "click-shades",
          "benchmark": "miniwob",
          "goal": "Select all the shades of blue and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d5c51f2c",
          "task_type": "click-collapsible-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 9,
          "actions": [
            "noop()",
            "click(16)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('16')",
            "click('22')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "23735dd6",
          "task_type": "click-menu",
          "benchmark": "miniwob",
          "goal": "Select Nell",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('85')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2a5f9ada",
          "task_type": "click-tab-2-medium",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"sapien\".",
          "success": true,
          "reward": 1.0,
          "task_length": 3,
          "actions": [
            "noop()",
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "97f4b22e",
          "task_type": "count-sides",
          "benchmark": "miniwob",
          "goal": "Press the button that correctly denotes how many sides the shape has.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('14')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "aeb1ad30",
          "task_type": "click-collapsible",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 3,
          "actions": [
            "click('16')",
            "noop()",
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6e294076",
          "task_type": "choose-list",
          "benchmark": "miniwob",
          "goal": "Select Maisie from the list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "select('16')",
            "noop()",
            "click('17')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "77b5dc01",
          "task_type": "click-tab-2-medium",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"Pellentesque\".",
          "success": true,
          "reward": 1.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9fe549d0",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select pGTJgOB and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 8,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "check('24')",
            "noop()",
            "noop()",
            "click('27')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "4e1905ce",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/06/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "type('17', '12/06/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "1334f922",
          "task_type": "click-tab-2-hard",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"tempus.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "16920ea9",
          "task_type": "click-pie-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"c\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a33435a5",
          "task_type": "click-widget",
          "benchmark": "miniwob",
          "goal": "Click on a \"textarea\" widget.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ac638ccd",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00c5\u008f\u2013\u00e6\u00b6\u02c6\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('19')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c6b070d7",
          "task_type": "click-shades",
          "benchmark": "miniwob",
          "goal": "Select all the shades of green and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "fe637739",
          "task_type": "highlight-text",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the paragraph below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1cc0bdaf",
          "task_type": "click-tab-2",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"dui.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2e21477c",
          "task_type": "use-autocomplete",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"Lib\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type(17, \"Lib\")",
            "noop()",
            "click(20)",
            "noop()",
            "noop()",
            "noop()",
            "type(17, \"Lib\")",
            "noop()",
            "click(18)",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "fd470e34",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c2af03cd",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e6bbf09e",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9ba68495",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b599bdbd",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b169b8af",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7a409308",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3054bc73",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('12')\n[TRAINER DEBUG] Creating 10 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/10...\n[TRAINER DEBUG] Episode 1 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 2/10...\n[TRAINER DEBUG] Episode 2 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 3/10...\n[TRAINER DEBUG] Episode 3 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 4/10...\n[TRAINER DEBUG] Episode 4 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 5/10...\n[TRAINER DEBUG] Episode 5 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 6/10...\n[TRAINER DEBUG] Episode 6 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 7/10...\n[TRAINER DEBUG] Episode 7 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 8/10...\n[TRAINER DEBUG] Episode 8 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 9/10...\n[TRAINER DEBUG] Episode 9 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 10/10...\n[TRAINER DEBUG] Episode 10 reward: 2.4\n[TRAINER DEBUG] Getting ref model logprobs for 10 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 21: 10 episodes, success rate: 100.0%, avg reward: 2.43"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b5d8f2a5",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "209c37bf",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0a554e5a",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b32586ce",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ace21083",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "type('[19]', '8j3L')",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "255fd39e",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "eded0007",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b20103cf",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 80 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/80...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/80...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/80...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/80...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/80...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/80...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/80...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/80...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/80...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/80...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/80...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/80...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/80...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/80...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/80...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/80...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/80...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/80...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/80...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/80...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/80...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/80...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/80...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/80...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/80...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/80...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/80...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/80...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/80...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/80...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/80...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/80...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/80...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/80...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/80...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/80...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/80...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/80...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/80...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/80...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/80...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/80...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/80...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/80...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/80...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/80...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/80...\u001b[34m[ReferenceModel-0/2] 2025-12-25 21:02:23 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-1/2] 2025-12-25 21:02:23 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.501, std=0.011\n[ADVANTAGES DEBUG] advantages: min=-8.75, max=0.11\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 21:02:24 INFO\u001b[0m [GC] Performing periodic GC collection took 0.10 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 21:02:25 INFO\u001b[0m Pushing weights for policy version 34\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 21:02:27 INFO\u001b[0m Completed weights push in 2.41 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 21:02:28 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 21:02:33 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 21:02:33 INFO\u001b[0m Weight update completed (now v34)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 34 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 8.0\n  buffer/sample/avg_sampled_policy_age: 0.25\n  buffer/sample/demand_to_size_ratio: 1.0\n  buffer/sample/max_sampled_policy_age: 1.0\n  generator/generate/count_requests: 80.0\n  generator/generate/count_sequences_completed: 80.0\n  generator_perf/generate/total_duration_avg_s: 0.975793372344971\n  generator_perf/generate/total_duration_max_s: 3.7817333984375\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.9428693670779467\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.490575790405273\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 0.5747383199632168\n  reference_perf/forward/total_duration_max_s: 0.577125383540988\n  reward/avg_reward: -0.50125\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.1\n  rl_trainer/learning_rate: 9.358717434869741e-06\n  rl_trainer/loss: 0.07782852649688721\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.5516238436102867\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.5516238436102867\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013758109882473946\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013758109882473946\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.14866514317691326\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.14866514317691326\n  rl_trainer_perf/step/total_duration_avg_s: 0.7140496093779802\n  rl_trainer_perf/step/total_duration_max_s: 0.7140496093779802\n  task/avg_length: 10.0\n  task/avg_reward: 0.0\n  task/count_completed: 8.0\n  task/success_rate: 0.0\n  train/avg_reward: -0.50125\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n==============================\n\n\n[TRAINER DEBUG] Episode 47 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 48/80...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/80...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/80...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/80...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/80...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/80...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/80...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/80...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/80...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/80...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/80...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/80...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/80...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/80...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/80...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/80...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/80...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/80...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 66/80...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/80...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/80...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 69/80...\n[TRAINER DEBUG] Episode 69 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 70/80...\n[TRAINER DEBUG] Episode 70 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 71/80...\n[TRAINER DEBUG] Episode 71 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 72/80...\n[TRAINER DEBUG] Episode 72 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 73/80...\n[TRAINER DEBUG] Episode 73 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 74/80...\n[TRAINER DEBUG] Episode 74 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 75/80...\n[TRAINER DEBUG] Episode 75 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 76/80...\n[TRAINER DEBUG] Episode 76 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 77/80...\n[TRAINER DEBUG] Episode 77 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 78/80...\n[TRAINER DEBUG] Episode 78 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 79/80...\n[TRAINER DEBUG] Episode 79 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 80/80...\n[TRAINER DEBUG] Episode 80 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 80 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 22: 80 episodes, success rate: 0.0%, avg reward: -0.50"
          ],
          "has_action_error": false
        }
      ]
    },
    {
      "step": 35,
      "num_tasks": 32,
      "success_rate": 0.14545454545454545,
      "avg_reward": 0.14545454545454545,
      "results": [
        {
          "task_id": "91496af4",
          "task_type": "email-inbox-delete",
          "benchmark": "miniwob",
          "goal": "Find the email by Hollie and click the trash icon to delete it.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "da4ca6b7",
          "task_type": "email-inbox-delete",
          "benchmark": "miniwob",
          "goal": "Find the email by Jessamine and click the trash icon to delete it.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "887b898c",
          "task_type": "email-inbox-delete",
          "benchmark": "miniwob",
          "goal": "Find the email by Berenice and click the trash icon to delete it.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "4a0c5362",
          "task_type": "email-inbox-delete",
          "benchmark": "miniwob",
          "goal": "Find the email by Stephie and click the trash icon to delete it.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b39fe48d",
          "task_type": "email-inbox-delete",
          "benchmark": "miniwob",
          "goal": "Find the email by Dodi and click the trash icon to delete it.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a0ebde34",
          "task_type": "email-inbox-delete",
          "benchmark": "miniwob",
          "goal": "Find the email by Vivi and click the trash icon to delete it.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9a1d8e0e",
          "task_type": "email-inbox-delete",
          "benchmark": "miniwob",
          "goal": "Find the email by Evelina and click the trash icon to delete it.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ae448485",
          "task_type": "highlight-text-2",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the 3rd paragraph and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "10c257b6",
          "task_type": "use-slider-2",
          "benchmark": "miniwob",
          "goal": "Set the sliders to the combination [4,16,12] and submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7aef5e18",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00c3\u2013K\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('17')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "34e7dba3",
          "task_type": "use-colorwheel",
          "benchmark": "miniwob",
          "goal": "Select yellow with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "fee535da",
          "task_type": "click-checkboxes-transfer",
          "benchmark": "miniwob",
          "goal": "Select nothing and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('27')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2190d291",
          "task_type": "click-tab-2-medium",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"et\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ca7b20e4",
          "task_type": "click-checkboxes",
          "benchmark": "miniwob",
          "goal": "Select 4JfmSzZ and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('4JfmSzZ')",
            "noop()",
            "noop()",
            "click('Submit')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "7742b799",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "fill('[19]', 'KsVz')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "28e27f1f",
          "task_type": "click-tab-2-easy",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"Egestas\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "70382713",
          "task_type": "click-collapsible-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 5,
          "actions": [
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f6b50783",
          "task_type": "email-inbox-noscroll",
          "benchmark": "miniwob",
          "goal": "Find the email by Charmine and click the star icon to mark it as important.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "84dc76dd",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #5fb9c8 with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ad754eda",
          "task_type": "use-slider",
          "benchmark": "miniwob",
          "goal": "Select -95 with the slider and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "10ae3c22",
          "task_type": "click-tab-2",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"Nec\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('20')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e213fd51",
          "task_type": "click-tab-2-medium",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"semper.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 1,
          "actions": [
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f1aacf0e",
          "task_type": "use-autocomplete",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"Be\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "type('17', 'Be')",
            "noop()",
            "click('20')",
            "noop()",
            "noop()",
            "noop()",
            "fill('17', 'Be')",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "f33b0f82",
          "task_type": "choose-list",
          "benchmark": "miniwob",
          "goal": "Select South Africa from the list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "select('19')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "select('14')",
            "noop()",
            "select('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "8a1466d4",
          "task_type": "focus-text-2",
          "benchmark": "miniwob",
          "goal": "Focus into the 1st input textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d6b880fc",
          "task_type": "click-color",
          "benchmark": "miniwob",
          "goal": "Click on the lime colored box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "befbdd45",
          "task_type": "copy-paste",
          "benchmark": "miniwob",
          "goal": "Copy the text in the textarea below, paste it into the textbox and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e954373d",
          "task_type": "click-pie-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"L\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "236dc069",
          "task_type": "use-autocomplete",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"And\" and ends with \"orra\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2ef22f5d",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Dot from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "select('19', 'Dot')",
            "click('22')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "6be8a117",
          "task_type": "click-color",
          "benchmark": "miniwob",
          "goal": "Click on the white colored box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "54f91c0f",
          "task_type": "drag-circle",
          "benchmark": "miniwob",
          "goal": "Drag the circle left then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\u001b[34m[TitanTrainer-0/1] 2025-12-25 21:17:36 INFO\u001b[0m [GC] Performing periodic GC collection took 0.10 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 21:17:37 INFO\u001b[0m Pushing weights for policy version 36\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 21:17:39 INFO\u001b[0m Completed weights push in 2.32 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 21:17:40 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 21:17:44 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 21:17:44 INFO\u001b[0m Weight update completed (now v36)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 36 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 74.0\n  buffer/sample/avg_sampled_policy_age: 0.0\n  buffer/sample/demand_to_size_ratio: 0.10256410256410256\n  buffer/sample/max_sampled_policy_age: 0.0\n  eval/avg_reward: 0.15625\n  eval/num_tasks: 32.0\n  eval/success_rate: 0.15625\n  eval/successes: 5.0\n  generator/generate/count_requests: 342.0\n  generator/generate/count_sequences_completed: 342.0\n  generator_perf/generate/total_duration_avg_s: 0.9367844067846828\n  generator_perf/generate/total_duration_max_s: 3.722663330078125\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.6881453637033701\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.490509033203125\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 0.5675890436396003\n  reference_perf/forward/total_duration_max_s: 0.5686174798756838\n  reward/avg_reward: -0.5\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.0\n  rl_trainer/learning_rate: 9.3186372745491e-06\n  rl_trainer/loss: 0.05867546796798706\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.45610128343105316\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.45610128343105316\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.009459728375077248\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.009459728375077248\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.10110006667673588\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.10110006667673588\n  rl_trainer_perf/step/total_duration_avg_s: 0.5666650850325823\n  rl_trainer_perf/step/total_duration_max_s: 0.5666650850325823\n  task/avg_length: 8.6\n  task/avg_reward: 0.125\n  task/count_completed: 40.0\n  task/success_rate: 0.125\n  train/avg_reward: -0.5\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n=============================="
          ],
          "has_action_error": false
        },
        {
          "task_id": "f0b413e1",
          "task_type": "click-button",
          "benchmark": "miniwob",
          "goal": "Click on the \"submit\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "307b179f",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #19fa38 with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8352591b",
          "task_type": "click-pie",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"z\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "191a526e",
          "task_type": "find-word",
          "benchmark": "miniwob",
          "goal": "Find the 3rd word in the paragraph, type that into the textbox and press \"Submit\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "389fb627",
          "task_type": "email-inbox-important",
          "benchmark": "miniwob",
          "goal": "Find the email by Zorana and click the star icon to mark it as important.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7868e1f1",
          "task_type": "use-slider",
          "benchmark": "miniwob",
          "goal": "Select -14 with the slider and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a37efde3",
          "task_type": "use-slider-2",
          "benchmark": "miniwob",
          "goal": "Set the sliders to the combination [7,20,4] and submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bb53ddfe",
          "task_type": "click-tab-2-easy",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"placerat\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "cc06333b",
          "task_type": "click-tab-2-easy",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"turpis\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1058c4fc",
          "task_type": "click-tab-2-easy",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"Proin.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "92bacf88",
          "task_type": "click-tab-2-easy",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"consectetur\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f1c42f17",
          "task_type": "click-tab-2-easy",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"magna\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "96fd38c9",
          "task_type": "click-tab-2-easy",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"felis,\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "635f1df8",
          "task_type": "click-tab-2-easy",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"ut.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "396b1b3a",
          "task_type": "click-tab-2-easy",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"duis\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "click('21')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('21')\n[TRAINER DEBUG] Creating 80 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/80...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/80...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/80...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/80...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/80...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/80...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/80...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/80...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/80...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/80...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/80...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/80...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/80...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/80...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/80...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/80...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/80...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/80...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/80...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/80...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/80...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/80...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/80...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/80...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/80...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/80...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/80...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/80...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/80...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/80...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/80...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/80...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/80...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/80...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/80...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/80...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/80...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/80...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/80...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/80...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/80...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/80...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/80...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/80...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/80...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/80...\n[TRAINER DEBUG] Episode 46 reward: -0.5\u001b[34m[ReferenceModel-1/2] 2025-12-25 21:21:14 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-0/2] 2025-12-25 21:21:14 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.500, std=0.000\n[ADVANTAGES DEBUG] advantages: min=0.00, max=0.00\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 21:21:15 INFO\u001b[0m [GC] Performing periodic GC collection took 0.11 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 21:21:16 INFO\u001b[0m Pushing weights for policy version 38\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 21:21:18 INFO\u001b[0m Completed weights push in 2.46 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 21:21:19 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 21:21:24 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 21:21:24 INFO\u001b[0m Weight update completed (now v38)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 38 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 70.0\n  buffer/sample/avg_sampled_policy_age: 0.25\n  buffer/sample/demand_to_size_ratio: 1.0\n  buffer/sample/max_sampled_policy_age: 1.0\n  generator/generate/count_requests: 78.0\n  generator/generate/count_sequences_completed: 78.0\n  generator_perf/generate/total_duration_avg_s: 0.9799713095640524\n  generator_perf/generate/total_duration_max_s: 4.015459716796875\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.9430158883333206\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.490509033203125\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 0.5728101367130876\n  reference_perf/forward/total_duration_max_s: 0.5757403764873743\n  reward/avg_reward: -0.5\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.0\n  rl_trainer/learning_rate: 9.278557114228458e-06\n  rl_trainer/loss: 0.05358085408806801\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.508191155269742\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.508191155269742\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.007439056411385536\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.007439056411385536\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.14776859618723392\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.14776859618723392\n  rl_trainer_perf/step/total_duration_avg_s: 0.6634006705135107\n  rl_trainer_perf/step/total_duration_max_s: 0.6634006705135107\n  task/avg_length: 10.0\n  task/avg_reward: 0.0\n  task/count_completed: 8.0\n  task/success_rate: 0.0\n  train/avg_reward: -0.5\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n==============================\n\n\n[TRAINER DEBUG] Computing reward for episode 47/80...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/80...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/80...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/80...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/80...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/80...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/80...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/80...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/80...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/80...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/80...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/80...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/80...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/80...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/80...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/80...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/80...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/80...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/80...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 66/80...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/80...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/80...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 69/80...\n[TRAINER DEBUG] Episode 69 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 70/80...\n[TRAINER DEBUG] Episode 70 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 71/80...\n[TRAINER DEBUG] Episode 71 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 72/80...\n[TRAINER DEBUG] Episode 72 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 73/80...\n[TRAINER DEBUG] Episode 73 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 74/80...\n[TRAINER DEBUG] Episode 74 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 75/80...\n[TRAINER DEBUG] Episode 75 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 76/80...\n[TRAINER DEBUG] Episode 76 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 77/80...\n[TRAINER DEBUG] Episode 77 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 78/80...\n[TRAINER DEBUG] Episode 78 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 79/80...\n[TRAINER DEBUG] Episode 79 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 80/80...\n[TRAINER DEBUG] Episode 80 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 80 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 24: 80 episodes, success rate: 0.0%, avg reward: -0.50"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0898fb46",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select sx and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "select('24', 'sx')",
            "click('27')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "b6198469",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select dbs and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "check('24')",
            "click('27')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "f94ff5f0",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select A0Fhxwx and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "select('30', 'A0Fhxwx')",
            "noop()",
            "click('33')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "15b756be",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select qolHjvG and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "select('27', 'qolHjvG')",
            "click('36')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "4388b860",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select RWQpxnk and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('27')",
            "click('36')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "11972dd3",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select 4jXHB and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "select('27', '4jXHB')",
            "click('30')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "1d046e6a",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select Y59 and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('21')",
            "click('24')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "dafec0e3",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select K6 and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 9,
          "actions": [
            "select('21', 'K6')",
            "noop()",
            "click('Submit')",
            "noop()",
            "noop()",
            "noop()",
            "click('21')",
            "noop()",
            "click('24')\n[TRAINER DEBUG] Creating 29 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/29...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/29...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/29...\n[TRAINER DEBUG] Episode 3 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 4/29...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/29...\n[TRAINER DEBUG] Episode 5 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 6/29...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/29...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/29...\n[TRAINER DEBUG] Episode 8 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 9/29...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/29...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/29...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/29...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/29...\n[TRAINER DEBUG] Episode 13 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 14/29...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/29...\n[TRAINER DEBUG] Episode 15 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 16/29...\n[TRAINER DEBUG] Episode 16 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 17/29...\n[TRAINER DEBUG] Episode 17 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 18/29...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/29...\n[TRAINER DEBUG] Episode 19 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 20/29...\n[TRAINER DEBUG] Episode 20 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 21/29...\n[TRAINER DEBUG] Episode 21 reward: 1.9499999999999997\n[TRAINER DEBUG] Computing reward for episode 22/29...\n[TRAINER DEBUG] Episode 22 reward: 2.05\n[TRAINER DEBUG] Computing reward for episode 23/29...\n[TRAINER DEBUG] Episode 23 reward: 1.9499999999999997\n[TRAINER DEBUG] Computing reward for episode 24/29...\n[TRAINER DEBUG] Episode 24 reward: 2.05\n[TRAINER DEBUG] Computing reward for episode 25/29...\n[TRAINER DEBUG] Episode 25 reward: 2.05\n[TRAINER DEBUG] Computing reward for episode 26/29...\n[TRAINER DEBUG] Episode 26 reward: 2.05\n[TRAINER DEBUG] Computing reward for episode 27/29...\n[TRAINER DEBUG] Episode 27 reward: 2.05\n[TRAINER DEBUG] Computing reward for episode 28/29...\n[TRAINER DEBUG] Episode 28 reward: 2.05\n[TRAINER DEBUG] Computing reward for episode 29/29...\n[TRAINER DEBUG] Episode 29 reward: 2.05\n[TRAINER DEBUG] Getting ref model logprobs for 29 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 25: 29 episodes, success rate: 44.8%, avg reward: 0.67"
          ],
          "has_action_error": true
        }
      ]
    },
    {
      "step": 40,
      "num_tasks": 32,
      "success_rate": 0.21428571428571427,
      "avg_reward": 0.21428571428571427,
      "results": [
        {
          "task_id": "0d5e3927",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"Canc\u00c3\u00a9l\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('21')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b578c8ba",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00e7\u00a1\u00ae\u00e5\u00ae\u0161\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e0479cdd",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00e2\u2122\u00a5\u00e2\u2122\u00a5\u00e2\u2122\u00a5\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('18')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2b42ef20",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00e3\u0192\u02dc\u00e3\u0192\u00ab\u00e3\u0192\u2014\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('15')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "39bbd76e",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00e5\u008f\u2013\u00e6\u00b6\u02c6\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('17')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e423dcf3",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00c3\u2013K\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('15')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9f2ea7af",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00c3\u2013K\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('15')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "fb92e8a9",
          "task_type": "choose-date-medium",
          "benchmark": "miniwob",
          "goal": "Select 12/13/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type(17, '12/13/2016')",
            "noop()",
            "click(19)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "8de9c9ae",
          "task_type": "click-tab-2-medium",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"pulvinar.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e15e12da",
          "task_type": "email-inbox-star-reply",
          "benchmark": "miniwob",
          "goal": "Find the email by Stephi and reply to them with the text \"Libero non.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "click('37')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "86df312c",
          "task_type": "find-word",
          "benchmark": "miniwob",
          "goal": "Find the 1st word in the paragraph, type that into the textbox and press \"Submit\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "type(14, \"Auctor\")",
            "click(15)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "ae5a1cb1",
          "task_type": "drag-box",
          "benchmark": "miniwob",
          "goal": "Drag the smaller box so that it is completely inside the larger box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "cd5b469e",
          "task_type": "email-inbox-important",
          "benchmark": "miniwob",
          "goal": "Find the email by Alex and click the star icon to mark it as important.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b58aa047",
          "task_type": "use-spinner",
          "benchmark": "miniwob",
          "goal": "Select -2 with the spinner and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c1dc89e0",
          "task_type": "click-checkboxes-soft",
          "benchmark": "miniwob",
          "goal": "Select words similar to sad, small, flames, odd and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "22c3bd27",
          "task_type": "use-colorwheel",
          "benchmark": "miniwob",
          "goal": "Select black with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9da21429",
          "task_type": "enter-time",
          "benchmark": "miniwob",
          "goal": "Enter 10:05 AM as the time and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d5b743f6",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "017cf5e5",
          "task_type": "copy-paste-2",
          "benchmark": "miniwob",
          "goal": "Copy the text from the 2nd text area below and paste it into the text input, then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e073e67a",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": true,
          "reward": 1.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b0150a72",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9225ede9",
          "task_type": "click-tab",
          "benchmark": "miniwob",
          "goal": "Click on Tab #3.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6b94c647",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2ec989cd",
          "task_type": "drag-circle",
          "benchmark": "miniwob",
          "goal": "Drag the circle up then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('13')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2e080a03",
          "task_type": "use-slider-2",
          "benchmark": "miniwob",
          "goal": "Set the sliders to the combination [4,8,6] and submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ddda4dc1",
          "task_type": "choose-date-medium",
          "benchmark": "miniwob",
          "goal": "Select 12/15/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type(17, '12/15/2016')",
            "noop()",
            "click(19)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('17')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "4e5653f6",
          "task_type": "use-spinner",
          "benchmark": "miniwob",
          "goal": "Select -7 with the spinner and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "39065bf3",
          "task_type": "use-autocomplete-nodelay",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"Sa\" and ends with \"ucia\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "type(17, \"Saucia\")",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "4cdef757",
          "task_type": "enter-text-2",
          "benchmark": "miniwob",
          "goal": "Type \"RILEY\" in all lower case letters in the text input and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type(14, \"riley\")",
            "noop()",
            "click(15)",
            "noop()",
            "noop()",
            "noop()",
            "fill(14, \"riley\")",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "cbecbe84",
          "task_type": "focus-text-2",
          "benchmark": "miniwob",
          "goal": "Focus into the 2nd input textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "da941bf9",
          "task_type": "drag-single-shape",
          "benchmark": "miniwob",
          "goal": "Drag the item up then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ec8be2bb",
          "task_type": "highlight-text-2",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the 2nd paragraph and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "227abc16",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "21f99479",
          "task_type": "drag-box",
          "benchmark": "miniwob",
          "goal": "Drag the smaller box so that it is completely inside the larger box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c893b5fe",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d671fc1d",
          "task_type": "click-pie",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"F\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('18')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e5885e1a",
          "task_type": "count-shape",
          "benchmark": "miniwob",
          "goal": "How many large shapes are there?",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f4d26a18",
          "task_type": "click-tab-2-medium",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"Nunc\".",
          "success": false,
          "reward": 0.0,
          "task_length": 7,
          "actions": [
            "click(20)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('20')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "e09b7366",
          "task_type": "drag-circle",
          "benchmark": "miniwob",
          "goal": "Drag the circle right then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "42e195e5",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/25/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 8,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "type('17', '12/25/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "f944c7c5",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/22/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 9,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "type('17', '12/22/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "e6e1ac02",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/28/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('17')",
            "fill('17', '12/28/2016')",
            "noop()",
            "click('19')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "03a938a7",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/09/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 5,
          "actions": [
            "noop()",
            "noop()",
            "fill('17', '12/09/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "3f4587f0",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/22/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 8,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "type('17', '12/22/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "24a66ebd",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/16/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('17', '12/16/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "a6e5e171",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/22/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 8,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "type('17', '12/22/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "7cc69e5e",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/04/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "type('17', '12/04/2016')",
            "noop()",
            "click('19')\n[TRAINER DEBUG] Creating 55 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/55...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/55...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/55...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/55...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/55...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/55...\n[TRAINER DEBUG] Episode 6 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 7/55...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/55...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/55...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/55...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/55...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/55...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/55...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/55...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/55...\n[TRAINER DEBUG] Episode 15 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 16/55...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/55...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/55...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/55...\n[TRAINER DEBUG] Episode 19 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 20/55...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/55...\n[TRAINER DEBUG] Episode 21 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 22/55...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/55...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/55...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/55...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/55...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/55...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/55...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/55...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/55...\n[TRAINER DEBUG] Episode 30 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 31/55...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/55...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/55...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/55...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/55...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/55...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/55...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/55...\n[TRAINER DEBUG] Episode 38 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 39/55...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/55...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/55...\n[TRAINER DEBUG] Episode 41 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 42/55...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/55...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/55...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/55...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/55...\u001b[34m[ReferenceModel-0/2] 2025-12-25 21:37:01 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-1/2] 2025-12-25 21:37:01 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.6, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.516, std=0.037\n[ADVANTAGES DEBUG] advantages: min=-2.23, max=0.44\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 21:37:02 INFO\u001b[0m [GC] Performing periodic GC collection took 0.09 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 21:37:03 INFO\u001b[0m Pushing weights for policy version 42\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 21:37:05 INFO\u001b[0m Completed weights push in 2.34 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 21:37:06 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 21:37:11 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 21:37:11 INFO\u001b[0m Weight update completed (now v42)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 42 ===\n  buffer/add/count_episodes_added: 55.0\n  buffer/evict/sum_episodes_evicted: 21.0\n  buffer/sample/avg_sampled_policy_age: 0.0\n  buffer/sample/demand_to_size_ratio: 1.5970006816632722\n  buffer/sample/max_sampled_policy_age: 0.0\n  generator/generate/count_requests: 55.0\n  generator/generate/count_sequences_completed: 55.0\n  generator_perf/generate/total_duration_avg_s: 0.925496006081321\n  generator_perf/generate/total_duration_max_s: 4.28226220703125\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.5730272363871336\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 9.962267875671387\n  reference_perf/forward/memory_peak_max_gb: 25.54666233062744\n  reference_perf/forward/total_duration_avg_s: 0.5774618592113256\n  reference_perf/forward/total_duration_max_s: 0.5813923459500074\n  reward/avg_reward: -0.5163636363636365\n  reward/success_rate: 0.0\n  reward/sum_reward: -28.400000000000006\n  rl_trainer/learning_rate: 9.198396793587175e-06\n  rl_trainer/loss: 0.2642238140106201\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.45689060911536217\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.45689060911536217\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013849543407559395\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013849543407559395\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.1515756119042635\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.1515756119042635\n  rl_trainer_perf/step/total_duration_avg_s: 0.6223178580403328\n  rl_trainer_perf/step/total_duration_max_s: 0.6223178580403328\n  task/avg_length: 6.875\n  task/avg_reward: 0.0\n  task/count_completed: 8.0\n  task/success_rate: 0.0\n  train/avg_reward: -0.5163636363636364\n  train/episodes_per_rollout: 55.0\n  train/success_rate: 0.0\n==============================\n\n\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/55...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/55...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/55...\n[TRAINER DEBUG] Episode 49 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 50/55...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/55...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/55...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/55...\n[TRAINER DEBUG] Episode 53 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 54/55...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/55...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 55 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 27: 55 episodes, success rate: 0.0%, avg reward: -0.52"
          ],
          "has_action_error": true
        },
        {
          "task_id": "18bdbf66",
          "task_type": "use-spinner",
          "benchmark": "miniwob",
          "goal": "Select -10 with the spinner and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "dd15df7b",
          "task_type": "use-spinner",
          "benchmark": "miniwob",
          "goal": "Select 1 with the spinner and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e917e929",
          "task_type": "use-spinner",
          "benchmark": "miniwob",
          "goal": "Select 5 with the spinner and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0150927f",
          "task_type": "use-spinner",
          "benchmark": "miniwob",
          "goal": "Select 9 with the spinner and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "click(18)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "285c9fce",
          "task_type": "use-spinner",
          "benchmark": "miniwob",
          "goal": "Select 1 with the spinner and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ca8c84bd",
          "task_type": "use-spinner",
          "benchmark": "miniwob",
          "goal": "Select -3 with the spinner and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "717b5edf",
          "task_type": "use-spinner",
          "benchmark": "miniwob",
          "goal": "Select 1 with the spinner and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "31cc9b72",
          "task_type": "use-spinner",
          "benchmark": "miniwob",
          "goal": "Select -5 with the spinner and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 80 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/80...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/80...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/80...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/80...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/80...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/80...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/80...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/80...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/80...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/80...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/80...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/80...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/80...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/80...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/80...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/80...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/80...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/80...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/80...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/80...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/80...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/80...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/80...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/80...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/80...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/80...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/80...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/80...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/80...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/80...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/80...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/80...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/80...\n[TRAINER DEBUG] Episode 33 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 34/80...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/80...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/80...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/80...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/80...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/80...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/80...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/80...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/80...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/80...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/80...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/80...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/80...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/80...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/80...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/80...\u001b[34m[ReferenceModel-0/2] 2025-12-25 21:40:41 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-1/2] 2025-12-25 21:40:41 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.501, std=0.011\n[ADVANTAGES DEBUG] advantages: min=-8.75, max=0.11\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 21:40:42 INFO\u001b[0m [GC] Performing periodic GC collection took 0.10 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 21:40:43 INFO\u001b[0m Pushing weights for policy version 44\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 21:40:45 INFO\u001b[0m Completed weights push in 2.41 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 21:40:46 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 21:40:51 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 21:40:51 INFO\u001b[0m Weight update completed (now v44)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 44 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 47.0\n  buffer/sample/avg_sampled_policy_age: 0.25\n  buffer/sample/demand_to_size_ratio: 0.5714285714285714\n  buffer/sample/max_sampled_policy_age: 1.0\n  generator/generate/count_requests: 77.0\n  generator/generate/count_sequences_completed: 77.0\n  generator_perf/generate/total_duration_avg_s: 0.9782544539810775\n  generator_perf/generate/total_duration_max_s: 3.897286376953125\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.9023269638419151\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.490532875061035\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 0.5647943764925003\n  reference_perf/forward/total_duration_max_s: 0.5654286742210388\n  reward/avg_reward: -0.50125\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.1\n  rl_trainer/learning_rate: 9.158316633266535e-06\n  rl_trainer/loss: -0.08987437188625336\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.5544055011123419\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.5544055011123419\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013737915083765984\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013737915083765984\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.15135253593325615\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.15135253593325615\n  rl_trainer_perf/step/total_duration_avg_s: 0.7195021975785494\n  rl_trainer_perf/step/total_duration_max_s: 0.7195021975785494\n  task/avg_length: 10.0\n  task/avg_reward: 0.0\n  task/count_completed: 8.0\n  task/success_rate: 0.0\n  train/avg_reward: -0.50125\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n==============================\n\n\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/80...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/80...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/80...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/80...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/80...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/80...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/80...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/80...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/80...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/80...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/80...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/80...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/80...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/80...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/80...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/80...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 66/80...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/80...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/80...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 69/80...\n[TRAINER DEBUG] Episode 69 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 70/80...\n[TRAINER DEBUG] Episode 70 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 71/80...\n[TRAINER DEBUG] Episode 71 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 72/80...\n[TRAINER DEBUG] Episode 72 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 73/80...\n[TRAINER DEBUG] Episode 73 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 74/80...\n[TRAINER DEBUG] Episode 74 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 75/80...\n[TRAINER DEBUG] Episode 75 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 76/80...\n[TRAINER DEBUG] Episode 76 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 77/80...\n[TRAINER DEBUG] Episode 77 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 78/80...\n[TRAINER DEBUG] Episode 78 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 79/80...\n[TRAINER DEBUG] Episode 79 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 80/80...\n[TRAINER DEBUG] Episode 80 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 80 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 28: 80 episodes, success rate: 0.0%, avg reward: -0.50"
          ],
          "has_action_error": false
        },
        {
          "task_id": "246676fa",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "fill('[15]', '6')",
            "noop()",
            "click('16')"
          ],
          "has_action_error": true
        }
      ]
    },
    {
      "step": 45,
      "num_tasks": 32,
      "success_rate": 0.25925925925925924,
      "avg_reward": 0.25925925925925924,
      "results": [
        {
          "task_id": "098d7b7a",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type([15], '7')",
            "noop()",
            "noop()",
            "noop()",
            "click([16])",
            "noop()",
            "noop()",
            "noop()",
            "fill([15], \"7\")",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "df5006fa",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 6,
          "actions": [
            "noop()",
            "type('15', '1')",
            "noop()",
            "noop()",
            "noop()",
            "click('16')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "f5b27c34",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "fill(15, '6')",
            "noop()",
            "noop()",
            "click(16)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "b4edc83a",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "fill([15], '4')",
            "noop()",
            "click([16])",
            "noop()",
            "noop()",
            "noop()",
            "type([15], '4')",
            "noop()",
            "noop()",
            "click([16])"
          ],
          "has_action_error": true
        },
        {
          "task_id": "942dc4cb",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 8,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "type('[15]', '11')",
            "noop()",
            "click('16')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "598e5082",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "fill('[15]', '54')",
            "noop()",
            "click('16')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "29f8b2a4",
          "task_type": "click-menu",
          "benchmark": "miniwob",
          "goal": "Select Latashia>Vivien>Blair",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7dd36c36",
          "task_type": "use-colorwheel",
          "benchmark": "miniwob",
          "goal": "Select teal with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "12bb64a7",
          "task_type": "click-test",
          "benchmark": "miniwob",
          "goal": "Click the button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "98686ab0",
          "task_type": "drag-circle",
          "benchmark": "miniwob",
          "goal": "Drag the circle left then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bc3c0a8d",
          "task_type": "drag-single-shape",
          "benchmark": "miniwob",
          "goal": "Drag the item down then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6195d62a",
          "task_type": "drag-box",
          "benchmark": "miniwob",
          "goal": "Drag the smaller box so that it is completely inside the larger box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "efddbcf8",
          "task_type": "resize-textarea",
          "benchmark": "miniwob",
          "goal": "Resize the textarea so that the width is larger than its initial size then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "508a13c6",
          "task_type": "click-color",
          "benchmark": "miniwob",
          "goal": "Click on the cyan colored box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "aa5c1356",
          "task_type": "click-dialog-2",
          "benchmark": "miniwob",
          "goal": "Click the button in the dialog box labeled \"Cancel\".",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('27')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2e9478b0",
          "task_type": "use-slider-2",
          "benchmark": "miniwob",
          "goal": "Set the sliders to the combination [14,14,17] and submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "11f088fb",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"elit.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('16')",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b737a813",
          "task_type": "highlight-text",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the paragraph below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "95431113",
          "task_type": "scroll-text",
          "benchmark": "miniwob",
          "goal": "Find the last word in the text area, enter it into the text field and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "70bddfd9",
          "task_type": "enter-text-dynamic",
          "benchmark": "miniwob",
          "goal": "Enter \"FQ8\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "type(14, \"FQ8\")",
            "click(15)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "fill('14', 'FQ8')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "9aa19f1d",
          "task_type": "focus-text-2",
          "benchmark": "miniwob",
          "goal": "Focus into the 2nd input textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7f4c32c7",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": true,
          "reward": 1.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "87535b93",
          "task_type": "email-inbox-delete",
          "benchmark": "miniwob",
          "goal": "Find the email by Nelia and click the trash icon to delete it.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d0801ecc",
          "task_type": "click-checkboxes",
          "benchmark": "miniwob",
          "goal": "Select nothing and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('24')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "75c34b7a",
          "task_type": "find-word",
          "benchmark": "miniwob",
          "goal": "Find the 11th word in the paragraph, type that into the textbox and press \"Submit\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "94979ee5",
          "task_type": "use-autocomplete-nodelay",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"Yeme\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type(17, 'Yeme')",
            "click(18)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "type(17, \"Yeme\")",
            "noop()",
            "click(18)",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "9f1f08a6",
          "task_type": "highlight-text-2",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the 3rd paragraph and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f0c9f39a",
          "task_type": "click-menu-2",
          "benchmark": "miniwob",
          "goal": "Click the \"Menu\" button, and then find and click on the item with the \"ui-icon-seek-start\" icon.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('22')",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f7e1b26a",
          "task_type": "drag-circle",
          "benchmark": "miniwob",
          "goal": "Drag the circle left then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "76c62426",
          "task_type": "enter-password",
          "benchmark": "miniwob",
          "goal": "Enter the password \"Ct5\" into both text fields and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "06f43f74",
          "task_type": "click-tab",
          "benchmark": "miniwob",
          "goal": "Click on Tab #3.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "07e78336",
          "task_type": "drag-single-shape",
          "benchmark": "miniwob",
          "goal": "Drag the item right then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0012889e",
          "task_type": "click-shades",
          "benchmark": "miniwob",
          "goal": "Select all the shades of green and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0b023b26",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "type([15], '0')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "f7adc9ea",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #98bd6e with the color picker and hit Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 3,
          "actions": [
            "fill('[15]', '#98bd6e')",
            "noop()",
            "click('16')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "c799fffc",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select gZxeA and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "check('27')",
            "noop()",
            "click('30')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "ad827f7b",
          "task_type": "text-editor",
          "benchmark": "miniwob",
          "goal": "Using the text editor, make everything the color orange and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "eeadadba",
          "task_type": "use-slider",
          "benchmark": "miniwob",
          "goal": "Select -96 with the slider and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "064c0fc6",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Montserrat, Cameroon from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "028efb59",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Seychelles from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 6,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "select('15', 'Seychelles')",
            "noop()",
            "click('25')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "cb542581",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Russian Federation, Northern Mariana Islands from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e22442b8",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Dre, Anya from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c5c5570d",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Romania, Malaysia from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5c4b35ac",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Caren from the scroll list and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('18')",
            "click('23')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "11b1497a",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Elsa, Janel from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b12849d2",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Sao Tome and Principe, Zambia from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 68 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/68...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/68...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/68...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/68...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/68...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/68...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/68...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/68...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/68...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/68...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/68...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/68...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/68...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/68...\n[TRAINER DEBUG] Episode 14 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 15/68...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/68...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/68...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/68...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/68...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/68...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/68...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/68...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/68...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/68...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/68...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/68...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/68...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/68...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/68...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/68...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/68...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/68...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/68...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/68...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/68...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/68...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/68...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/68...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/68...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/68...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/68...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/68...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/68...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/68...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/68...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/68...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/68...\u001b[34m[ReferenceModel-1/2] 2025-12-25 21:57:25 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-0/2] 2025-12-25 21:57:25 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, 2.4, 2.4, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.416, std=0.494\n[ADVANTAGES DEBUG] advantages: min=-0.37, max=5.70\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 21:57:27 INFO\u001b[0m [GC] Performing periodic GC collection took 0.09 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 21:57:27 INFO\u001b[0m Pushing weights for policy version 48\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 21:57:30 INFO\u001b[0m Completed weights push in 2.32 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 21:57:31 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 21:57:36 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 21:57:36 INFO\u001b[0m Weight update completed (now v48)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 48 ===\n  buffer/add/count_episodes_added: 68.0\n  buffer/evict/sum_episodes_evicted: 51.0\n  buffer/sample/avg_sampled_policy_age: 0.25\n  buffer/sample/demand_to_size_ratio: 0.1951219512195122\n  buffer/sample/max_sampled_policy_age: 1.0\n  generator/generate/count_requests: 65.0\n  generator/generate/count_sequences_completed: 65.0\n  generator_perf/generate/total_duration_avg_s: 0.9677377178485578\n  generator_perf/generate/total_duration_max_s: 3.38716455078125\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.9580470658838749\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 12.316938400268555\n  reference_perf/forward/memory_peak_max_gb: 31.19784927368164\n  reference_perf/forward/total_duration_avg_s: 1.1696344036608934\n  reference_perf/forward/total_duration_max_s: 1.1727583650499582\n  reward/avg_reward: -0.41617647058823537\n  reward/success_rate: 0.029411764705882353\n  reward/sum_reward: -28.300000000000004\n  rl_trainer/learning_rate: 9.078156312625251e-06\n  rl_trainer/loss: 1.133241891860962\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.5382212083786726\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.5382212083786726\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013619119301438332\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013619119301438332\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.15124217979609966\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.15124217979609966\n  rl_trainer_perf/step/total_duration_avg_s: 0.7030850406736135\n  rl_trainer_perf/step/total_duration_max_s: 0.7030850406736135\n  task/avg_length: 8.5\n  task/avg_reward: 0.125\n  task/count_completed: 8.0\n  task/success_rate: 0.125\n  train/avg_reward: -0.4161764705882353\n  train/episodes_per_rollout: 68.0\n  train/success_rate: 0.029411764705882353\n==============================\n\n\n[TRAINER DEBUG] Episode 47 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 48/68...\n[TRAINER DEBUG] Episode 48 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 49/68...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/68...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/68...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/68...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/68...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/68...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/68...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/68...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/68...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/68...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/68...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/68...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/68...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/68...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/68...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/68...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/68...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 66/68...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/68...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/68...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 68 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 30: 68 episodes, success rate: 2.9%, avg reward: -0.42"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b5557c96",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": true,
          "reward": 1.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0548f350",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e97b6ca7",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": true,
          "reward": 1.0,
          "task_length": 7,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a73b37fe",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": true,
          "reward": 1.0,
          "task_length": 7,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "84957060",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "73f60b67",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": true,
          "reward": 1.0,
          "task_length": 5,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2ba0cc6d",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": true,
          "reward": 1.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6feb003c",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 49 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/49...\n[TRAINER DEBUG] Episode 1 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 2/49...\n[TRAINER DEBUG] Episode 2 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 3/49...\n[TRAINER DEBUG] Episode 3 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 4/49...\n[TRAINER DEBUG] Episode 4 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 5/49...\n[TRAINER DEBUG] Episode 5 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 6/49...\n[TRAINER DEBUG] Episode 6 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 7/49...\n[TRAINER DEBUG] Episode 7 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 8/49...\n[TRAINER DEBUG] Episode 8 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 9/49...\n[TRAINER DEBUG] Episode 9 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 10/49...\n[TRAINER DEBUG] Episode 10 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 11/49...\n[TRAINER DEBUG] Episode 11 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 12/49...\n[TRAINER DEBUG] Episode 12 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 13/49...\n[TRAINER DEBUG] Episode 13 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 14/49...\n[TRAINER DEBUG] Episode 14 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 15/49...\n[TRAINER DEBUG] Episode 15 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 16/49...\n[TRAINER DEBUG] Episode 16 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 17/49...\n[TRAINER DEBUG] Episode 17 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 18/49...\n[TRAINER DEBUG] Episode 18 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 19/49...\n[TRAINER DEBUG] Episode 19 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 20/49...\n[TRAINER DEBUG] Episode 20 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 21/49...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/49...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/49...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/49...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/49...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/49...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/49...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/49...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/49...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/49...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/49...\n[TRAINER DEBUG] Episode 31 reward: 2.25\n[TRAINER DEBUG] Computing reward for episode 32/49...\n[TRAINER DEBUG] Episode 32 reward: 2.25\n[TRAINER DEBUG] Computing reward for episode 33/49...\n[TRAINER DEBUG] Episode 33 reward: 2.25\n[TRAINER DEBUG] Computing reward for episode 34/49...\n[TRAINER DEBUG] Episode 34 reward: 2.25\n[TRAINER DEBUG] Computing reward for episode 35/49...\n[TRAINER DEBUG] Episode 35 reward: 2.25\n[TRAINER DEBUG] Computing reward for episode 36/49...\n[TRAINER DEBUG] Episode 36 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 37/49...\n[TRAINER DEBUG] Episode 37 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 38/49...\n[TRAINER DEBUG] Episode 38 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 39/49...\n[TRAINER DEBUG] Episode 39 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 40/49...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/49...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/49...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/49...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/49...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/49...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/49...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/49...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/49...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/49...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 49 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 31: 49 episodes, success rate: 59.2%, avg reward: 1.11"
          ],
          "has_action_error": false
        }
      ]
    },
    {
      "step": 50,
      "num_tasks": 32,
      "success_rate": 0.16666666666666666,
      "avg_reward": 0.16666666666666666,
      "results": [
        {
          "task_id": "acc23442",
          "task_type": "email-inbox-star-reply",
          "benchmark": "miniwob",
          "goal": "Find the email by Molli and reply to them with the text \"Dolor a iaculis ullamcorper.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "cfe2af6e",
          "task_type": "email-inbox-star-reply",
          "benchmark": "miniwob",
          "goal": "Find the email by Brittne and reply to them with the text \"Id ultricies.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f9d89c8b",
          "task_type": "email-inbox-star-reply",
          "benchmark": "miniwob",
          "goal": "Find the email by Dorrie and reply to them with the text \"Metus, cras tellus.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('37')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('37')",
            "noop()",
            "click('37')",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7578581e",
          "task_type": "email-inbox-star-reply",
          "benchmark": "miniwob",
          "goal": "Find the email by Ira and reply to them with the text \"Dolor urna.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c2ccd1af",
          "task_type": "email-inbox-star-reply",
          "benchmark": "miniwob",
          "goal": "Find the email by La Verne and reply to them with the text \"Morbi pulvinar accumsan. Mattis.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('37')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('37')",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "40ddb1c0",
          "task_type": "email-inbox-star-reply",
          "benchmark": "miniwob",
          "goal": "Find the email by Teresina and reply to them with the text \"Sed.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7810099b",
          "task_type": "email-inbox-star-reply",
          "benchmark": "miniwob",
          "goal": "Find the email by Myrilla and reply to them with the text \"Amet feugiat.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "click('17')",
            "noop()",
            "fill('53', 'Myrilla')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1ae5aea4",
          "task_type": "click-widget",
          "benchmark": "miniwob",
          "goal": "Click on a \"checkbox\" widget.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('17')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "cfc88fff",
          "task_type": "use-slider-2",
          "benchmark": "miniwob",
          "goal": "Set the sliders to the combination [6,1,4] and submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bffa90a1",
          "task_type": "enter-date",
          "benchmark": "miniwob",
          "goal": "Enter 09/03/2013 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2614d70e",
          "task_type": "highlight-text",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the paragraph below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d9311da9",
          "task_type": "choose-date-medium",
          "benchmark": "miniwob",
          "goal": "Select 12/09/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 8,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "type('17', '12/09/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "1410c57f",
          "task_type": "text-editor",
          "benchmark": "miniwob",
          "goal": "Using the text editor, give the text Ultricies the style underlined and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "523bf834",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"montes,.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a6bdb708",
          "task_type": "click-collapsible-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5007b653",
          "task_type": "use-colorwheel",
          "benchmark": "miniwob",
          "goal": "Select yellow with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d9f85105",
          "task_type": "count-sides",
          "benchmark": "miniwob",
          "goal": "Press the button that correctly denotes how many sides the shape has.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1a164701",
          "task_type": "click-pie-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"5\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "dc4a8308",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00c7\u00a1\u00ae\u00e5\u00ae\u0161\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('14')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "88f4840b",
          "task_type": "drag-single-shape",
          "benchmark": "miniwob",
          "goal": "Drag the item right then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "676442fa",
          "task_type": "click-test-2",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f9c7ddd4",
          "task_type": "read-table-2",
          "benchmark": "miniwob",
          "goal": "Enter the value that corresponds with each label into the form and submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bcfbec71",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1cab8753",
          "task_type": "click-widget",
          "benchmark": "miniwob",
          "goal": "Click on a \"checkbox\" widget.",
          "success": true,
          "reward": 1.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "click('21')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a72e75ef",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d7491379",
          "task_type": "enter-text-2",
          "benchmark": "miniwob",
          "goal": "Type \"KELI\" in all lower case letters in the text input and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "type(14, \"KELI\")",
            "noop()",
            "click(15)",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "ba7d52e4",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ff6e9a8e",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('13')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('13')",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "4653da97",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "fill([15], '15')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "49416fc7",
          "task_type": "click-checkboxes-transfer",
          "benchmark": "miniwob",
          "goal": "Select nothing and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('Submit')",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "333d41c8",
          "task_type": "email-inbox-reply",
          "benchmark": "miniwob",
          "goal": "Find the email by Ebony and reply to them with the text \"Facilisi massa vestibulum. Orci.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ddf91c6e",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/15/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "fill('17', '12/15/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "41c5b823",
          "task_type": "click-tab-2-easy",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"Vitae\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a09dd03e",
          "task_type": "click-pie-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"8\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('13')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ac90717e",
          "task_type": "simple-algebra",
          "benchmark": "miniwob",
          "goal": "Solve for x and type your answer into the textbox. Press Submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "008b6775",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a yellow item",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "785bcc19",
          "task_type": "guess-number",
          "benchmark": "miniwob",
          "goal": "Guess the number between 0-9 and press Submit. Use the feedback below to find the right number.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3d2129be",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00c5\u008f\u2013\u00e6\u00b6\u02c6\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3257ccd8",
          "task_type": "click-checkboxes",
          "benchmark": "miniwob",
          "goal": "Select 2p4iZ, J3u and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f1a45b8a",
          "task_type": "click-checkboxes",
          "benchmark": "miniwob",
          "goal": "Select gSXFOd, WuNDW, no2vnx, hmdCeF and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "37c1d8fb",
          "task_type": "click-checkboxes",
          "benchmark": "miniwob",
          "goal": "Select wif and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click(18)",
            "noop()",
            "noop()",
            "noop()",
            "check(18)",
            "click(24)",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "12a2f2ad",
          "task_type": "click-checkboxes",
          "benchmark": "miniwob",
          "goal": "Select y805D4L and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "check('y805D4L')",
            "click('30')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "74f60d3b",
          "task_type": "click-checkboxes",
          "benchmark": "miniwob",
          "goal": "Select iG, 6bL1Jt7, uQ, 3HiInYt and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('18')",
            "click('21')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a4ee78d3",
          "task_type": "click-checkboxes",
          "benchmark": "miniwob",
          "goal": "Select 2h1 and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('21')",
            "click('27')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c649d1f5",
          "task_type": "click-checkboxes",
          "benchmark": "miniwob",
          "goal": "Select nothing and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('24')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ec3d3f0d",
          "task_type": "click-checkboxes",
          "benchmark": "miniwob",
          "goal": "Select 4H7 and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "check('24')",
            "noop()",
            "click('Submit')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "check('4H7')",
            "click('Submit')",
            "noop()\n[TRAINER DEBUG] Creating 57 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/57...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/57...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/57...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/57...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/57...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/57...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/57...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/57...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/57...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/57...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/57...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/57...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/57...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/57...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/57...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/57...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/57...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/57...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/57...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/57...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/57...\n[TRAINER DEBUG] Episode 21 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 22/57...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/57...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/57...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/57...\n[TRAINER DEBUG] Episode 25 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 26/57...\n[TRAINER DEBUG] Episode 26 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 27/57...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/57...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/57...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/57...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/57...\n[TRAINER DEBUG] Episode 31 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 32/57...\n[TRAINER DEBUG] Episode 32 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 33/57...\n[TRAINER DEBUG] Episode 33 reward: 2.1999999999999997\n[TRAINER DEBUG] Computing reward for episode 34/57...\n[TRAINER DEBUG] Episode 34 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 35/57...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/57...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/57...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/57...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/57...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/57...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/57...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/57...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/57...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/57...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/57...\n[TRAINER DEBUG] Episode 45 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 46/57...\n[TRAINER DEBUG] Episode 46 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 47/57...\n[TRAINER DEBUG] Episode 47 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 48/57...\n[TRAINER DEBUG] Episode 48 reward: -0.6\u001b[34m[ReferenceModel-0/2] 2025-12-25 22:16:42 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-1/2] 2025-12-25 22:16:42 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.6, -0.6, -0.5, -0.5, -0.5, -0.5, 2.3, 2.3, 2.2, 2.3, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, 2.4, 2.4, 2.45, -0.6, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.6, -0.6, -0.5]\n[ADVANTAGES DEBUG] mean=-0.164, std=0.945\n[ADVANTAGES DEBUG] advantages: min=-0.46, max=2.77\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 22:16:44 INFO\u001b[0m [GC] Performing periodic GC collection took 0.11 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 22:16:44 INFO\u001b[0m Pushing weights for policy version 53\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 22:16:47 INFO\u001b[0m Completed weights push in 2.48 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 22:16:47 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 22:16:52 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 22:16:52 INFO\u001b[0m Weight update completed (now v53)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 53 ===\n  buffer/add/count_episodes_added: 57.0\n  buffer/evict/sum_episodes_evicted: 75.0\n  buffer/sample/avg_sampled_policy_age: 0.0\n  buffer/sample/demand_to_size_ratio: 0.14545454545454545\n  buffer/sample/max_sampled_policy_age: 0.0\n  generator/generate/count_requests: 54.0\n  generator/generate/count_sequences_completed: 54.0\n  generator_perf/generate/total_duration_avg_s: 0.9415534978795934\n  generator_perf/generate/total_duration_max_s: 3.74193896484375\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.6783987190574408\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 10.32446575164795\n  reference_perf/forward/memory_peak_max_gb: 26.416075706481934\n  reference_perf/forward/total_duration_avg_s: 0.571944952942431\n  reference_perf/forward/total_duration_max_s: 0.5719888005405664\n  reward/avg_reward: -0.1640350877192982\n  reward/success_rate: 0.12280701754385964\n  reward/sum_reward: -9.349999999999998\n  rl_trainer/learning_rate: 8.977955911823647e-06\n  rl_trainer/loss: 0.41395166516304016\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.3119390979409218\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.3119390979409218\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.006408274173736572\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.006408274173736572\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.06992067396640778\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.06992067396640778\n  rl_trainer_perf/step/total_duration_avg_s: 0.388269754126668\n  rl_trainer_perf/step/total_duration_max_s: 0.388269754126668\n  task/avg_length: 7.125\n  task/avg_reward: 0.375\n  task/count_completed: 8.0\n  task/success_rate: 0.375\n  train/avg_reward: -0.16403508771929828\n  train/episodes_per_rollout: 57.0\n  train/success_rate: 0.12280701754385964\n==============================\n\n\n[TRAINER DEBUG] Computing reward for episode 49/57...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/57...\n[TRAINER DEBUG] Episode 50 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 51/57...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/57...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/57...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/57...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/57...\n[TRAINER DEBUG] Episode 55 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 56/57...\n[TRAINER DEBUG] Episode 56 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 57/57...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 57 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 33: 57 episodes, success rate: 12.3%, avg reward: -0.16"
          ],
          "has_action_error": true
        },
        {
          "task_id": "a191147c",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"vel\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "002ccfb4",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"Ut.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9676d89e",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"pharetra\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f8bd615e",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"at.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('16')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "46055703",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"turpis\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5d25ff2d",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"Egestas\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d123f8c9",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"Quis.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "cb04bb23",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"Augue\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 80 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/80...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/80...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/80...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/80...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/80...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/80...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/80...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/80...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/80...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/80...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/80...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/80...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/80...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/80...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/80...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/80...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/80...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/80...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/80...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/80...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/80...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/80...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/80...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/80...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/80...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/80...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/80...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/80...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/80...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/80...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/80...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/80...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/80...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/80...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/80...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/80...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/80...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/80...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/80...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/80...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/80...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/80...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/80...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/80...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/80...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/80...\u001b[34m[ReferenceModel-0/2] 2025-12-25 22:20:21 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-1/2] 2025-12-25 22:20:21 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.500, std=0.000\n[ADVANTAGES DEBUG] advantages: min=0.00, max=0.00\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 22:20:22 INFO\u001b[0m [GC] Performing periodic GC collection took 0.11 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 22:20:22 INFO\u001b[0m Pushing weights for policy version 55\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 22:20:25 INFO\u001b[0m Completed weights push in 2.59 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 22:20:26 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 22:20:31 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 22:20:31 INFO\u001b[0m Weight update completed (now v55)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 55 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 46.0\n  buffer/sample/avg_sampled_policy_age: 0.25\n  buffer/sample/demand_to_size_ratio: 1.0\n  buffer/sample/max_sampled_policy_age: 1.0\n  generator/generate/count_requests: 78.0\n  generator/generate/count_sequences_completed: 78.0\n  generator_perf/generate/total_duration_avg_s: 0.9822926142765925\n  generator_perf/generate/total_duration_max_s: 3.889902099609375\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.9378565885126591\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.490530967712402\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 0.5685593653470278\n  reference_perf/forward/total_duration_max_s: 0.5706680398434401\n  reward/avg_reward: -0.5\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.0\n  rl_trainer/learning_rate: 8.937875751503007e-06\n  rl_trainer/loss: 0.03092612698674202\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.5640021115541458\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.5640021115541458\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013697555288672447\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013697555288672447\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.15135080181062222\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.15135080181062222\n  rl_trainer_perf/step/total_duration_avg_s: 0.7290535252541304\n  rl_trainer_perf/step/total_duration_max_s: 0.7290535252541304\n  task/avg_length: 10.0\n  task/avg_reward: 0.0\n  task/count_completed: 8.0\n  task/success_rate: 0.0\n  train/avg_reward: -0.5\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n==============================\n\n\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/80...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/80...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/80...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/80...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/80...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/80...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/80...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/80...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/80...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/80...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/80...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/80...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/80...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/80...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/80...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/80...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/80...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/80...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/80...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 66/80...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/80...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/80...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 69/80...\n[TRAINER DEBUG] Episode 69 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 70/80...\n[TRAINER DEBUG] Episode 70 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 71/80...\n[TRAINER DEBUG] Episode 71 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 72/80...\n[TRAINER DEBUG] Episode 72 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 73/80...\n[TRAINER DEBUG] Episode 73 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 74/80...\n[TRAINER DEBUG] Episode 74 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 75/80...\n[TRAINER DEBUG] Episode 75 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 76/80...\n[TRAINER DEBUG] Episode 76 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 77/80...\n[TRAINER DEBUG] Episode 77 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 78/80...\n[TRAINER DEBUG] Episode 78 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 79/80...\n[TRAINER DEBUG] Episode 79 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 80/80...\n[TRAINER DEBUG] Episode 80 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 80 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 34: 80 episodes, success rate: 0.0%, avg reward: -0.50"
          ],
          "has_action_error": false
        }
      ]
    },
    {
      "step": 55,
      "num_tasks": 32,
      "success_rate": 0.20967741935483872,
      "avg_reward": 0.20967741935483872,
      "results": [
        {
          "task_id": "44776f32",
          "task_type": "email-inbox-noscroll",
          "benchmark": "miniwob",
          "goal": "Find the email by Roda and click the star icon to mark it as important.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3f22ec92",
          "task_type": "email-inbox-noscroll",
          "benchmark": "miniwob",
          "goal": "Find the email by Kimberli and click the trash icon to delete it.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "271488d1",
          "task_type": "email-inbox-noscroll",
          "benchmark": "miniwob",
          "goal": "Find the email by Dix and reply to them with the text \"Pretium.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "30698a92",
          "task_type": "email-inbox-noscroll",
          "benchmark": "miniwob",
          "goal": "Find the email by Eleanore and forward that email to Fallon.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9a01fd6c",
          "task_type": "email-inbox-noscroll",
          "benchmark": "miniwob",
          "goal": "Find the email by Chery and forward that email to Devina.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f908607f",
          "task_type": "email-inbox-noscroll",
          "benchmark": "miniwob",
          "goal": "Find the email by Danielle and click the trash icon to delete it.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "11d93169",
          "task_type": "email-inbox-noscroll",
          "benchmark": "miniwob",
          "goal": "Find the email by Joye and click the star icon to mark it as important.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "108cf2c3",
          "task_type": "resize-textarea",
          "benchmark": "miniwob",
          "goal": "Resize the textarea so that the width is larger than its initial size then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "eb56cba5",
          "task_type": "click-pie",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"f\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "click('13')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bb93daa1",
          "task_type": "click-collapsible",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 5,
          "actions": [
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "click('19')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "934760b5",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6822bac0",
          "task_type": "drag-box",
          "benchmark": "miniwob",
          "goal": "Drag the smaller box so that it is completely inside the larger box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b81c9109",
          "task_type": "click-collapsible-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 5,
          "actions": [
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8b743567",
          "task_type": "click-pie-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"L\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "58f43c61",
          "task_type": "use-colorwheel",
          "benchmark": "miniwob",
          "goal": "Select yellow with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "df87ecf8",
          "task_type": "use-spinner",
          "benchmark": "miniwob",
          "goal": "Select 9 with the spinner and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "click(18)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "cb69a907",
          "task_type": "click-button-sequence",
          "benchmark": "miniwob",
          "goal": "Click button ONE, then click button TWO.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('12')",
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6cbdb584",
          "task_type": "find-word",
          "benchmark": "miniwob",
          "goal": "Find the 11th word in the paragraph, type that into the textbox and press \"Submit\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5a95ac95",
          "task_type": "email-inbox-delete",
          "benchmark": "miniwob",
          "goal": "Find the email by Terza and click the trash icon to delete it.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3956ba05",
          "task_type": "use-spinner",
          "benchmark": "miniwob",
          "goal": "Select -9 with the spinner and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "08295977",
          "task_type": "drag-box",
          "benchmark": "miniwob",
          "goal": "Drag the smaller box so that it is completely inside the larger box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "22992ca6",
          "task_type": "click-tab-2-hard",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"aliquet\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "194e177a",
          "task_type": "click-checkboxes-soft",
          "benchmark": "miniwob",
          "goal": "Select words similar to preserve, fire, corrupt, mild, like and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "fdb852e9",
          "task_type": "click-pie",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"J\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('13')",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7fdd9918",
          "task_type": "scroll-text",
          "benchmark": "miniwob",
          "goal": "Find the last word in the text area, enter it into the text field and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0b414120",
          "task_type": "count-shape",
          "benchmark": "miniwob",
          "goal": "How many red items are there?",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "06450447",
          "task_type": "choose-list",
          "benchmark": "miniwob",
          "goal": "Select Sianna from the list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "select('15')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "f9a07663",
          "task_type": "click-checkboxes-soft",
          "benchmark": "miniwob",
          "goal": "Select words similar to camouflage and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3024e60e",
          "task_type": "read-table-2",
          "benchmark": "miniwob",
          "goal": "Enter the value that corresponds with each label into the form and submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "eeaf67e7",
          "task_type": "use-slider-2",
          "benchmark": "miniwob",
          "goal": "Set the sliders to the combination [6,16,0] and submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "70b2f166",
          "task_type": "read-table-2",
          "benchmark": "miniwob",
          "goal": "Enter the value that corresponds with each label into the form and submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "00737019",
          "task_type": "email-inbox-star-reply",
          "benchmark": "miniwob",
          "goal": "Find the email by Gray and reply to them with the text \"Senectus venenatis.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "click('27')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6310a0ea",
          "task_type": "use-slider",
          "benchmark": "miniwob",
          "goal": "Select 108 with the slider and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7185e37b",
          "task_type": "use-autocomplete-nodelay",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"To\" and ends with \"lau\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "641fee69",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a large red item",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f70b777a",
          "task_type": "click-shades",
          "benchmark": "miniwob",
          "goal": "Select all the shades of green and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7ba62d24",
          "task_type": "simple-algebra",
          "benchmark": "miniwob",
          "goal": "Solve for x and type your answer into the textbox. Press Submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "fill(16, -94)",
            "noop()",
            "click(17)"
          ],
          "has_action_error": true
        },
        {
          "task_id": "1079cfab",
          "task_type": "use-slider-2",
          "benchmark": "miniwob",
          "goal": "Set the sliders to the combination [1,18,15] and submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "eb88eee8",
          "task_type": "drag-box",
          "benchmark": "miniwob",
          "goal": "Drag the smaller box so that it is completely inside the larger box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "06a19e67",
          "task_type": "drag-box",
          "benchmark": "miniwob",
          "goal": "Drag the smaller box so that it is completely inside the larger box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b35e5dd7",
          "task_type": "drag-box",
          "benchmark": "miniwob",
          "goal": "Drag the smaller box so that it is completely inside the larger box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e270692a",
          "task_type": "drag-box",
          "benchmark": "miniwob",
          "goal": "Drag the smaller box so that it is completely inside the larger box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2df639ce",
          "task_type": "drag-box",
          "benchmark": "miniwob",
          "goal": "Drag the smaller box so that it is completely inside the larger box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "33a17dc5",
          "task_type": "drag-box",
          "benchmark": "miniwob",
          "goal": "Drag the smaller box so that it is completely inside the larger box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "67de0455",
          "task_type": "drag-box",
          "benchmark": "miniwob",
          "goal": "Drag the smaller box so that it is completely inside the larger box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e9660a75",
          "task_type": "drag-box",
          "benchmark": "miniwob",
          "goal": "Drag the smaller box so that it is completely inside the larger box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 80 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/80...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/80...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/80...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/80...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/80...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/80...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/80...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/80...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/80...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/80...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/80...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/80...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/80...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/80...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/80...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/80...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/80...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/80...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/80...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/80...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/80...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/80...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/80...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/80...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/80...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/80...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/80...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/80...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/80...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/80...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/80...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/80...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/80...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/80...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/80...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/80...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/80...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/80...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/80...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/80...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/80...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/80...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/80...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/80...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/80...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/80...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/80...\n[TRAINER DEBUG] Episode 47 reward: -0.5\u001b[34m[ReferenceModel-1/2] 2025-12-25 22:40:25 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-0/2] 2025-12-25 22:40:25 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.500, std=0.000\n[ADVANTAGES DEBUG] advantages: min=0.00, max=0.00\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 22:40:26 INFO\u001b[0m [GC] Performing periodic GC collection took 0.11 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 22:40:27 INFO\u001b[0m Pushing weights for policy version 58\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 22:40:29 INFO\u001b[0m Completed weights push in 2.28 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 22:40:30 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 22:40:34 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 22:40:34 INFO\u001b[0m Weight update completed (now v58)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 58 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 76.0\n  buffer/sample/avg_sampled_policy_age: 0.375\n  buffer/sample/demand_to_size_ratio: 0.8888888888888888\n  buffer/sample/max_sampled_policy_age: 1.0\n  generator/generate/count_requests: 77.0\n  generator/generate/count_sequences_completed: 77.0\n  generator_perf/generate/total_duration_avg_s: 0.9897154398336037\n  generator_perf/generate/total_duration_max_s: 4.40409375\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.3871591668576002\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.490509033203125\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 0.5700485398992896\n  reference_perf/forward/total_duration_max_s: 0.5709348693490028\n  reward/avg_reward: -0.5\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.0\n  rl_trainer/learning_rate: 8.877755511022045e-06\n  rl_trainer/loss: 0.07560370862483978\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.5587862990796566\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.5587862990796566\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013504989445209503\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013504989445209503\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.1487424671649933\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.1487424671649933\n  rl_trainer_perf/step/total_duration_avg_s: 0.7210363950580359\n  rl_trainer_perf/step/total_duration_max_s: 0.7210363950580359\n  task/avg_length: 8.2\n  task/avg_reward: 0.2\n  task/count_completed: 10.0\n  task/success_rate: 0.2\n  train/avg_reward: -0.5\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n==============================\n\n\n[TRAINER DEBUG] Computing reward for episode 48/80...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/80...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/80...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/80...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/80...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/80...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/80...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/80...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/80...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/80...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/80...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/80...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/80...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/80...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/80...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/80...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/80...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/80...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 66/80...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/80...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/80...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 69/80...\n[TRAINER DEBUG] Episode 69 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 70/80...\n[TRAINER DEBUG] Episode 70 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 71/80...\n[TRAINER DEBUG] Episode 71 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 72/80...\n[TRAINER DEBUG] Episode 72 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 73/80...\n[TRAINER DEBUG] Episode 73 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 74/80...\n[TRAINER DEBUG] Episode 74 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 75/80...\n[TRAINER DEBUG] Episode 75 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 76/80...\n[TRAINER DEBUG] Episode 76 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 77/80...\n[TRAINER DEBUG] Episode 77 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 78/80...\n[TRAINER DEBUG] Episode 78 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 79/80...\n[TRAINER DEBUG] Episode 79 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 80/80...\n[TRAINER DEBUG] Episode 80 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 80 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 36: 80 episodes, success rate: 0.0%, avg reward: -0.50"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d5d4130f",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ea6816a8",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "fd9ba145",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3a24b887",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8e9150a3",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "76829571",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "599be464",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2c6553c6",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')\n[TRAINER DEBUG] Creating 8 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/8...\n[TRAINER DEBUG] Episode 1 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 2/8...\n[TRAINER DEBUG] Episode 2 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 3/8...\n[TRAINER DEBUG] Episode 3 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 4/8...\n[TRAINER DEBUG] Episode 4 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 5/8...\n[TRAINER DEBUG] Episode 5 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 6/8...\n[TRAINER DEBUG] Episode 6 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 7/8...\n[TRAINER DEBUG] Episode 7 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 8/8...\n[TRAINER DEBUG] Episode 8 reward: 2.45\n[TRAINER DEBUG] Getting ref model logprobs for 8 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 37: 8 episodes, success rate: 100.0%, avg reward: 2.45"
          ],
          "has_action_error": false
        },
        {
          "task_id": "82c3338e",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "type([15], '3')",
            "click([16])",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "type([15], '3')",
            "click([16])"
          ],
          "has_action_error": true
        },
        {
          "task_id": "9a1e33be",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "4698ba6b",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "fill([15], '8')",
            "noop()",
            "click([16])",
            "noop()",
            "noop()",
            "noop()",
            "type([15], '8')",
            "noop()",
            "noop()",
            "click([16])"
          ],
          "has_action_error": true
        },
        {
          "task_id": "c07c17d8",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c6e1f141",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "fill(15, '11')",
            "noop()",
            "click(16)",
            "noop()",
            "noop()",
            "noop()",
            "type(15, '11')",
            "noop()",
            "click(16)",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "a26ee8d8",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "fill([15], \"-2\")",
            "click([16])",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "47d1135b",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "fill([15], '4')",
            "noop()",
            "click([16])",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "type([15], '4')",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "483ab76b",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": true,
          "reward": 1.0,
          "task_length": 6,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "fill('15', '-2')",
            "click('16')\n[TRAINER DEBUG] Creating 76 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/76...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/76...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/76...\n[TRAINER DEBUG] Episode 3 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 4/76...\n[TRAINER DEBUG] Episode 4 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 5/76...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/76...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/76...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/76...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/76...\n[TRAINER DEBUG] Episode 9 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 10/76...\n[TRAINER DEBUG] Episode 10 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 11/76...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/76...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/76...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/76...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/76...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/76...\u001b[34m[ReferenceModel-0/2] 2025-12-25 22:43:54 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-1/2] 2025-12-25 22:43:54 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.6, -0.6, -0.5, -0.5, -0.5, -0.5, -0.6, -0.6, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.6, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.6, -0.5, -0.5, -0.5, -0.6, -0.5, -0.6, -0.5, -0.6, -0.6, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2]\n[ADVANTAGES DEBUG] mean=-0.309, std=0.741\n[ADVANTAGES DEBUG] advantages: min=-0.39, max=3.39\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 22:43:56 INFO\u001b[0m [GC] Performing periodic GC collection took 0.09 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 22:43:56 INFO\u001b[0m Pushing weights for policy version 60\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 22:43:59 INFO\u001b[0m Completed weights push in 2.42 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 22:44:00 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 22:44:05 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 22:44:05 INFO\u001b[0m Weight update completed (now v60)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 60 ===\n  buffer/add/count_episodes_added: 84.0\n  buffer/evict/sum_episodes_evicted: 72.0\n  buffer/sample/avg_sampled_policy_age: 0.125\n  buffer/sample/demand_to_size_ratio: 1.3317993557294208\n  buffer/sample/max_sampled_policy_age: 1.0\n  generator/generate/count_requests: 81.0\n  generator/generate/count_sequences_completed: 81.0\n  generator_perf/generate/total_duration_avg_s: 0.935164498034819\n  generator_perf/generate/total_duration_max_s: 4.04116064453125\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.9526980128139257\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 4.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 7.607515335083008\n  reference_perf/forward/memory_peak_max_gb: 34.67550277709961\n  reference_perf/forward/total_duration_avg_s: 1.1675231284461915\n  reference_perf/forward/total_duration_max_s: 1.1909851748496294\n  reward/avg_reward: -0.04642857142857147\n  reward/success_rate: 0.16666666666666666\n  reward/sum_reward: -3.900000000000004\n  rl_trainer/learning_rate: 8.837675350701403e-06\n  rl_trainer/loss: 0.3287060260772705\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.4766032211482525\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.4766032211482525\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.011632915586233139\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.011632915586233139\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.15081487968564034\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.15081487968564034\n  rl_trainer_perf/step/total_duration_avg_s: 0.6390533093363047\n  rl_trainer_perf/step/total_duration_max_s: 0.6390533093363047\n  task/avg_length: 7.181818181818182\n  task/avg_reward: 0.36363636363636365\n  task/count_completed: 11.0\n  task/success_rate: 0.36363636363636365\n  train/avg_reward: 1.0703947368421054\n  train/episodes_per_rollout: 42.0\n  train/success_rate: 0.5394736842105263\n==============================\n\n\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/76...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/76...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/76...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/76...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/76...\n[TRAINER DEBUG] Episode 21 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 22/76...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/76...\n[TRAINER DEBUG] Episode 23 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 24/76...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/76...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/76...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/76...\n[TRAINER DEBUG] Episode 27 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 28/76...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/76...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/76...\n[TRAINER DEBUG] Episode 30 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 31/76...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/76...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/76...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/76...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/76...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/76...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/76...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/76...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/76...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/76...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/76...\n[TRAINER DEBUG] Episode 41 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 42/76...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/76...\n[TRAINER DEBUG] Episode 43 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 44/76...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/76...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/76...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/76...\n[TRAINER DEBUG] Episode 47 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 48/76...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/76...\n[TRAINER DEBUG] Episode 49 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 50/76...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/76...\n[TRAINER DEBUG] Episode 51 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 52/76...\n[TRAINER DEBUG] Episode 52 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 53/76...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/76...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/76...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/76...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/76...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/76...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/76...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/76...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/76...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/76...\n[TRAINER DEBUG] Episode 62 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 63/76...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/76...\n[TRAINER DEBUG] Episode 64 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 65/76...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 66/76...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/76...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/76...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 69/76...\n[TRAINER DEBUG] Episode 69 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 70/76...\n[TRAINER DEBUG] Episode 70 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 71/76...\n[TRAINER DEBUG] Episode 71 reward: 2.2\n[TRAINER DEBUG] Computing reward for episode 72/76...\n[TRAINER DEBUG] Episode 72 reward: 2.2\n[TRAINER DEBUG] Computing reward for episode 73/76...\n[TRAINER DEBUG] Episode 73 reward: 2.2\n[TRAINER DEBUG] Computing reward for episode 74/76...\n[TRAINER DEBUG] Episode 74 reward: 2.2\n[TRAINER DEBUG] Computing reward for episode 75/76...\n[TRAINER DEBUG] Episode 75 reward: 2.2\n[TRAINER DEBUG] Computing reward for episode 76/76...\n[TRAINER DEBUG] Episode 76 reward: 2.2\n[TRAINER DEBUG] Getting ref model logprobs for 76 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 38: 76 episodes, success rate: 7.9%, avg reward: -0.31"
          ],
          "has_action_error": false
        }
      ]
    },
    {
      "step": 60,
      "num_tasks": 32,
      "success_rate": 0.12727272727272726,
      "avg_reward": 0.12727272727272726,
      "results": [
        {
          "task_id": "21998241",
          "task_type": "enter-password",
          "benchmark": "miniwob",
          "goal": "Enter the password \"6tE4P\" into both text fields and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e64cde00",
          "task_type": "enter-password",
          "benchmark": "miniwob",
          "goal": "Enter the password \"Nk\" into both text fields and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "618b12e8",
          "task_type": "enter-password",
          "benchmark": "miniwob",
          "goal": "Enter the password \"Y5Hl\" into both text fields and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "fbd3dc4f",
          "task_type": "enter-password",
          "benchmark": "miniwob",
          "goal": "Enter the password \"1mW\" into both text fields and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3b8e2836",
          "task_type": "enter-password",
          "benchmark": "miniwob",
          "goal": "Enter the password \"uOg\" into both text fields and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f449e96e",
          "task_type": "enter-password",
          "benchmark": "miniwob",
          "goal": "Enter the password \"QMT9X\" into both text fields and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5f01bacb",
          "task_type": "enter-password",
          "benchmark": "miniwob",
          "goal": "Enter the password \"ADR\" into both text fields and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e9fa64c0",
          "task_type": "drag-box",
          "benchmark": "miniwob",
          "goal": "Drag the smaller box so that it is completely inside the larger box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "064a87a2",
          "task_type": "count-shape",
          "benchmark": "miniwob",
          "goal": "How many large letters are there?",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ae7adb15",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #e66530 with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "31669e6a",
          "task_type": "click-menu",
          "benchmark": "miniwob",
          "goal": "Select Emiline",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('21')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "30a76038",
          "task_type": "simple-algebra",
          "benchmark": "miniwob",
          "goal": "Solve for x and type your answer into the textbox. Press Submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "fill('[16]', '92')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "fill(16, '92')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "e1a64dba",
          "task_type": "count-shape",
          "benchmark": "miniwob",
          "goal": "How many small aqua items are there?",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "22b2bee9",
          "task_type": "count-sides",
          "benchmark": "miniwob",
          "goal": "Press the button that correctly denotes how many sides the shape has.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "16b677f3",
          "task_type": "read-table",
          "benchmark": "miniwob",
          "goal": "Enter the value of Color into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7358cf3a",
          "task_type": "click-color",
          "benchmark": "miniwob",
          "goal": "Click on the magenta colored box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "75ffdcc7",
          "task_type": "copy-paste",
          "benchmark": "miniwob",
          "goal": "Copy the text in the textarea below, paste it into the textbox and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7d705edb",
          "task_type": "click-checkboxes-transfer",
          "benchmark": "miniwob",
          "goal": "Select nothing and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 3,
          "actions": [
            "noop()",
            "noop()",
            "click('30')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "71b8f971",
          "task_type": "enter-text",
          "benchmark": "miniwob",
          "goal": "Enter \"Briana\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('14', 'Briana')",
            "noop()",
            "click('15')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "85ce46a0",
          "task_type": "click-widget",
          "benchmark": "miniwob",
          "goal": "Click on a \"button\" widget.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9e29843f",
          "task_type": "use-autocomplete",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"Dji\" and ends with \"outi\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "4409fa22",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "47497b68",
          "task_type": "click-menu-2",
          "benchmark": "miniwob",
          "goal": "Click the \"Menu\" button, and then find and click on the item with the \"ui-icon-play\" icon.",
          "success": true,
          "reward": 1.0,
          "task_length": 6,
          "actions": [
            "click('16')",
            "click('22')",
            "noop()",
            "noop()",
            "noop()",
            "click('32')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "78f4f203",
          "task_type": "click-tab-2",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"arcu.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "fafbf141",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a large black item",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "916e6e22",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #c45f61 with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5c2f7e60",
          "task_type": "focus-text-2",
          "benchmark": "miniwob",
          "goal": "Focus into the 3rd input textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('14')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "66da9065",
          "task_type": "drag-circle",
          "benchmark": "miniwob",
          "goal": "Drag the circle up then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2f5e940e",
          "task_type": "use-spinner",
          "benchmark": "miniwob",
          "goal": "Select -2 with the spinner and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c4420224",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a58d2e85",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "fill(15, \"-3\")",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "fb04b94f",
          "task_type": "drag-circle",
          "benchmark": "miniwob",
          "goal": "Drag the circle right then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\u001b[34m[TitanTrainer-0/1] 2025-12-25 22:58:57 INFO\u001b[0m [GC] Performing periodic GC collection took 0.10 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 22:58:58 INFO\u001b[0m Pushing weights for policy version 61\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 22:59:00 INFO\u001b[0m Completed weights push in 2.46 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 22:59:01 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 22:59:06 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 22:59:06 INFO\u001b[0m Weight update completed (now v61)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 61 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 10.0\n  buffer/sample/avg_sampled_policy_age: 0.25\n  buffer/sample/demand_to_size_ratio: 0.05263157894736842\n  buffer/sample/max_sampled_policy_age: 1.0\n  eval/avg_reward: 0.1875\n  eval/num_tasks: 32.0\n  eval/success_rate: 0.1875\n  eval/successes: 6.0\n  generator/generate/count_requests: 340.0\n  generator/generate/count_sequences_completed: 340.0\n  generator_perf/generate/total_duration_avg_s: 0.943691621488684\n  generator_perf/generate/total_duration_max_s: 4.18693701171875\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.4831451326608658\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.49051284790039\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 0.5706572569906712\n  reference_perf/forward/total_duration_max_s: 0.5711506716907024\n  reward/avg_reward: -0.5\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.0\n  rl_trainer/learning_rate: 8.817635270541082e-06\n  rl_trainer/loss: 0.11969870328903198\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.5334818754345179\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.5334818754345179\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.007282765582203865\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.007282765582203865\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.07151607796549797\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.07151607796549797\n  rl_trainer_perf/step/total_duration_avg_s: 0.6122832372784615\n  rl_trainer_perf/step/total_duration_max_s: 0.6122832372784615\n  task/avg_length: 8.5\n  task/avg_reward: 0.15\n  task/count_completed: 40.0\n  task/success_rate: 0.15\n  train/avg_reward: -0.5\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n=============================="
          ],
          "has_action_error": false
        },
        {
          "task_id": "b9d7021a",
          "task_type": "email-inbox-noscroll",
          "benchmark": "miniwob",
          "goal": "Find the email by Orelee and click the star icon to mark it as important.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "fa4c1ba5",
          "task_type": "read-table",
          "benchmark": "miniwob",
          "goal": "Enter the value of First name into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "381c21ec",
          "task_type": "click-tab",
          "benchmark": "miniwob",
          "goal": "Click on Tab #1.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('18')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5c811466",
          "task_type": "click-tab-2",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"ultrices\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "450b5547",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/10/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "type('17', '12/10/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "b2d9d014",
          "task_type": "click-test-2",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c41e6354",
          "task_type": "choose-list",
          "benchmark": "miniwob",
          "goal": "Select Latashia from the list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "select(17, 'Latashia')",
            "noop()",
            "click(21)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "select('17', 'Latashia')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "da94fe4a",
          "task_type": "highlight-text-2",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the 3rd paragraph and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bdd0f922",
          "task_type": "highlight-text-2",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the 2nd paragraph and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0ea3907d",
          "task_type": "highlight-text-2",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the 3rd paragraph and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e965411b",
          "task_type": "highlight-text-2",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the 1st paragraph and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3150750a",
          "task_type": "highlight-text-2",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the 1st paragraph and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8713b65f",
          "task_type": "highlight-text-2",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the 3rd paragraph and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "fb7c9901",
          "task_type": "highlight-text-2",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the 1st paragraph and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ee0ed33e",
          "task_type": "highlight-text-2",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the 3rd paragraph and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 80 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/80...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/80...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/80...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/80...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/80...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/80...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/80...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/80...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/80...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/80...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/80...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/80...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/80...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/80...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/80...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/80...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/80...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/80...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/80...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/80...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/80...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/80...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/80...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/80...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/80...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/80...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/80...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/80...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/80...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/80...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/80...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/80...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/80...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/80...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/80...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/80...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/80...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/80...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/80...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/80...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/80...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/80...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/80...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/80...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/80...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/80...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/80...\u001b[34m[ReferenceModel-1/2] 2025-12-25 23:02:39 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-0/2] 2025-12-25 23:02:39 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.500, std=0.000\n[ADVANTAGES DEBUG] advantages: min=0.00, max=0.00\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 23:02:40 INFO\u001b[0m [GC] Performing periodic GC collection took 0.09 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 23:02:41 INFO\u001b[0m Pushing weights for policy version 63\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 23:02:43 INFO\u001b[0m Completed weights push in 2.23 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 23:02:44 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 23:02:49 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 23:02:49 INFO\u001b[0m Weight update completed (now v63)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 63 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 74.0\n  buffer/sample/avg_sampled_policy_age: 0.125\n  buffer/sample/demand_to_size_ratio: 0.12698412698412698\n  buffer/sample/max_sampled_policy_age: 1.0\n  generator/generate/count_requests: 77.0\n  generator/generate/count_sequences_completed: 77.0\n  generator_perf/generate/total_duration_avg_s: 1.0043498186383923\n  generator_perf/generate/total_duration_max_s: 4.15106787109375\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.9440099056810141\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.490509033203125\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 0.5840565208345652\n  reference_perf/forward/total_duration_max_s: 0.5905777309089899\n  reward/avg_reward: -0.5\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.0\n  rl_trainer/learning_rate: 8.77755511022044e-06\n  rl_trainer/loss: 0.03690043464303017\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.5390426758676767\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.5390426758676767\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013627896085381508\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013627896085381508\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.15141946822404861\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.15141946822404861\n  rl_trainer_perf/step/total_duration_avg_s: 0.7040925286710262\n  rl_trainer_perf/step/total_duration_max_s: 0.7040925286710262\n  task/avg_length: 10.0\n  task/avg_reward: 0.0\n  task/count_completed: 8.0\n  task/success_rate: 0.0\n  train/avg_reward: -0.5\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n==============================\n\n\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/80...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/80...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/80...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/80...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/80...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/80...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/80...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/80...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/80...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/80...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/80...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/80...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/80...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/80...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/80...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/80...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/80...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/80...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 66/80...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/80...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/80...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 69/80...\n[TRAINER DEBUG] Episode 69 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 70/80...\n[TRAINER DEBUG] Episode 70 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 71/80...\n[TRAINER DEBUG] Episode 71 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 72/80...\n[TRAINER DEBUG] Episode 72 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 73/80...\n[TRAINER DEBUG] Episode 73 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 74/80...\n[TRAINER DEBUG] Episode 74 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 75/80...\n[TRAINER DEBUG] Episode 75 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 76/80...\n[TRAINER DEBUG] Episode 76 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 77/80...\n[TRAINER DEBUG] Episode 77 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 78/80...\n[TRAINER DEBUG] Episode 78 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 79/80...\n[TRAINER DEBUG] Episode 79 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 80/80...\n[TRAINER DEBUG] Episode 80 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 80 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 40: 80 episodes, success rate: 0.0%, avg reward: -0.50"
          ],
          "has_action_error": false
        },
        {
          "task_id": "932a1061",
          "task_type": "click-menu",
          "benchmark": "miniwob",
          "goal": "Select Bathsheba>Shayla>Jacquetta",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "noop()",
            "noop()",
            "click('21')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "edf42063",
          "task_type": "click-menu",
          "benchmark": "miniwob",
          "goal": "Select Ruperta>Faustina>Arliene",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f076ca65",
          "task_type": "click-menu",
          "benchmark": "miniwob",
          "goal": "Select Margalit>Elmira>Jaquelyn",
          "success": false,
          "reward": 0.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "click('33')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c9c42451",
          "task_type": "click-menu",
          "benchmark": "miniwob",
          "goal": "Select Elbertina>Oona",
          "success": false,
          "reward": 0.0,
          "task_length": 1,
          "actions": [
            "click('19')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "051da4d7",
          "task_type": "click-menu",
          "benchmark": "miniwob",
          "goal": "Select Clovis>Lisetta>Phylys",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "57a39940",
          "task_type": "click-menu",
          "benchmark": "miniwob",
          "goal": "Select Cathe>Nicolette>Vinnie",
          "success": false,
          "reward": 0.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "click('41')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b2138a99",
          "task_type": "click-menu",
          "benchmark": "miniwob",
          "goal": "Select Brittaney>Melissa",
          "success": false,
          "reward": 0.0,
          "task_length": 1,
          "actions": [
            "click('19')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "cc4e1859",
          "task_type": "click-menu",
          "benchmark": "miniwob",
          "goal": "Select Ethelyn",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('19')\n[TRAINER DEBUG] Creating 34 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/34...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/34...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/34...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/34...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/34...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/34...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/34...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/34...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/34...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/34...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/34...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/34...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/34...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/34...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/34...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/34...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/34...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/34...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/34...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/34...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/34...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/34...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/34...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/34...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/34...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/34...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/34...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/34...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/34...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/34...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/34...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/34...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/34...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/34...\n[TRAINER DEBUG] Episode 34 reward: 2.45\n[TRAINER DEBUG] Getting ref model logprobs for 34 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 41: 34 episodes, success rate: 2.9%, avg reward: -0.41"
          ],
          "has_action_error": false
        }
      ]
    },
    {
      "step": 65,
      "num_tasks": 32,
      "success_rate": 0.14814814814814814,
      "avg_reward": 0.14814814814814814,
      "results": [
        {
          "task_id": "33b93916",
          "task_type": "read-table",
          "benchmark": "miniwob",
          "goal": "Enter the value of Religion into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "478be588",
          "task_type": "read-table",
          "benchmark": "miniwob",
          "goal": "Enter the value of Country into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d5b7df20",
          "task_type": "read-table",
          "benchmark": "miniwob",
          "goal": "Enter the value of First name into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('28', 'Kaylee')",
            "noop()",
            "click('32')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "8b1dbe09",
          "task_type": "read-table",
          "benchmark": "miniwob",
          "goal": "Enter the value of Gender into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "553105b3",
          "task_type": "read-table",
          "benchmark": "miniwob",
          "goal": "Enter the value of Color into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "fill('16', 'Color')",
            "click('32')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "933aed7d",
          "task_type": "read-table",
          "benchmark": "miniwob",
          "goal": "Enter the value of Color into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 5,
          "actions": [
            "click('19')",
            "noop()",
            "noop()",
            "noop()",
            "click('32')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c2050a26",
          "task_type": "read-table",
          "benchmark": "miniwob",
          "goal": "Enter the value of Gender into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7695d1e4",
          "task_type": "enter-text",
          "benchmark": "miniwob",
          "goal": "Enter \"Emile\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "fill(14, \"Emile\")",
            "click(15)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "fill('14', 'Emile')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "fbe5a691",
          "task_type": "click-pie-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"w\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "74501871",
          "task_type": "click-tab-2-hard",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"vitae\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "033de8af",
          "task_type": "click-dialog-2",
          "benchmark": "miniwob",
          "goal": "Click the button in the dialog box labeled \"Cancel\".",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('27')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bb2ee274",
          "task_type": "click-button-sequence",
          "benchmark": "miniwob",
          "goal": "Click button ONE, then click button TWO.",
          "success": false,
          "reward": 0.0,
          "task_length": 9,
          "actions": [
            "click('12')",
            "click('13')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('12')",
            "noop()",
            "click('13')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "64e6a6d2",
          "task_type": "resize-textarea",
          "benchmark": "miniwob",
          "goal": "Resize the textarea so that the width is smaller than its initial size then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c72f9bd5",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ee47e56c",
          "task_type": "highlight-text-2",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the 2nd paragraph and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5d57f47f",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select YmF2t and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "select(21, 'YmF2t')",
            "click(24)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "check('21')",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "daf82d0a",
          "task_type": "read-table-2",
          "benchmark": "miniwob",
          "goal": "Enter the value that corresponds with each label into the form and submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3d1746a2",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/10/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 6,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "type('17', '12/10/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "40bb890f",
          "task_type": "simple-algebra",
          "benchmark": "miniwob",
          "goal": "Solve for x and type your answer into the textbox. Press Submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a42b97d7",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bfd31eda",
          "task_type": "click-collapsible",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 8,
          "actions": [
            "noop()",
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3b5be4a2",
          "task_type": "click-button",
          "benchmark": "miniwob",
          "goal": "Click on the \"submit\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('19')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "83a0bfa0",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"ac.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "083f513e",
          "task_type": "click-color",
          "benchmark": "miniwob",
          "goal": "Click on the olive colored box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "316c4856",
          "task_type": "scroll-text-2",
          "benchmark": "miniwob",
          "goal": "Scroll the textarea to the top of the text hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "186e6763",
          "task_type": "count-shape",
          "benchmark": "miniwob",
          "goal": "How many green os are there?",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a49ac001",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select NzChj6 and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 3,
          "actions": [
            "noop()",
            "click('21')",
            "click('27')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "15c5ba31",
          "task_type": "click-menu",
          "benchmark": "miniwob",
          "goal": "Select Karalee>Delilah>Gratiana",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b2a2b2f6",
          "task_type": "highlight-text-2",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the 3rd paragraph and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0e746df9",
          "task_type": "count-sides",
          "benchmark": "miniwob",
          "goal": "Press the button that correctly denotes how many sides the shape has.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('14')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "dd4f67d1",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Dominica, Norway from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ebd422ec",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"Canc\u00c3\u00a9l\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('15')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ea36e696",
          "task_type": "count-sides",
          "benchmark": "miniwob",
          "goal": "Press the button that correctly denotes how many sides the shape has.",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('14')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d4447191",
          "task_type": "email-inbox-reply",
          "benchmark": "miniwob",
          "goal": "Find the email by Marne and reply to them with the text \"Nisi.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "click('36')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c04c8f9a",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b09453e4",
          "task_type": "use-autocomplete-nodelay",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"Ukra\".",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('17', 'Ukra')",
            "noop()",
            "click('18')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "1bc6c6a1",
          "task_type": "use-autocomplete",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"To\" and ends with \"ga\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "176435c4",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "type([15], '14')",
            "noop()",
            "noop()",
            "click([16])",
            "noop()",
            "noop()",
            "type([15], '14')",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "f8aaa583",
          "task_type": "enter-time",
          "benchmark": "miniwob",
          "goal": "Enter 1:31 AM as the time and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3f42828b",
          "task_type": "enter-time",
          "benchmark": "miniwob",
          "goal": "Enter 7:42 AM as the time and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c4070bb3",
          "task_type": "enter-time",
          "benchmark": "miniwob",
          "goal": "Enter 12:29 AM as the time and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "08638005",
          "task_type": "enter-time",
          "benchmark": "miniwob",
          "goal": "Enter 7:11 AM as the time and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0af56b7d",
          "task_type": "enter-time",
          "benchmark": "miniwob",
          "goal": "Enter 9:49 AM as the time and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1d4d243d",
          "task_type": "enter-time",
          "benchmark": "miniwob",
          "goal": "Enter 11:00 AM as the time and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7a0d8f3e",
          "task_type": "enter-time",
          "benchmark": "miniwob",
          "goal": "Enter 11:05 PM as the time and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a4ec878e",
          "task_type": "enter-time",
          "benchmark": "miniwob",
          "goal": "Enter 6:19 PM as the time and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 80 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/80...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/80...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/80...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/80...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/80...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/80...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/80...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/80...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/80...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/80...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/80...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/80...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/80...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/80...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/80...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/80...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/80...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/80...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/80...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/80...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/80...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/80...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/80...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/80...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/80...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/80...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/80...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/80...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/80...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/80...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/80...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/80...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/80...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/80...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/80...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/80...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/80...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/80...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/80...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/80...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/80...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/80...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/80...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/80...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/80...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/80...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/80...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/80...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/80...\u001b[34m[ReferenceModel-1/2] 2025-12-25 23:20:37 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-0/2] 2025-12-25 23:20:37 INFO\u001b[0m [GC] Performing periodic GC collection took 0.02 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.500, std=0.000\n[ADVANTAGES DEBUG] advantages: min=0.00, max=0.00\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 23:20:39 INFO\u001b[0m [GC] Performing periodic GC collection took 0.08 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 23:20:40 INFO\u001b[0m Pushing weights for policy version 68\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 23:20:42 INFO\u001b[0m Completed weights push in 2.42 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 23:20:42 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 23:20:48 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 23:20:48 INFO\u001b[0m Weight update completed (now v68)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 68 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 51.0\n  buffer/sample/avg_sampled_policy_age: 0.125\n  buffer/sample/demand_to_size_ratio: 0.10256410256410256\n  buffer/sample/max_sampled_policy_age: 1.0\n  generator/generate/count_requests: 78.0\n  generator/generate/count_sequences_completed: 78.0\n  generator_perf/generate/total_duration_avg_s: 0.9889397027431391\n  generator_perf/generate/total_duration_max_s: 4.008060546875\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.4778148364275694\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.490533828735352\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 0.5802041310817003\n  reference_perf/forward/total_duration_max_s: 0.5819631293416023\n  reward/avg_reward: -0.5\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.0\n  rl_trainer/learning_rate: 8.677354709418838e-06\n  rl_trainer/loss: 0.025096558034420013\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.5305473580956459\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.5305473580956459\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013866402208805084\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013866402208805084\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.15105429664254189\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.15105429664254189\n  rl_trainer_perf/step/total_duration_avg_s: 0.6954768858850002\n  rl_trainer_perf/step/total_duration_max_s: 0.6954768858850002\n  task/avg_length: 10.0\n  task/avg_reward: 0.0\n  task/count_completed: 8.0\n  task/success_rate: 0.0\n  train/avg_reward: -0.5\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n==============================\n\n\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/80...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/80...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/80...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/80...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/80...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/80...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/80...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/80...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/80...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/80...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/80...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/80...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/80...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/80...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/80...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/80...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 66/80...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/80...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/80...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 69/80...\n[TRAINER DEBUG] Episode 69 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 70/80...\n[TRAINER DEBUG] Episode 70 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 71/80...\n[TRAINER DEBUG] Episode 71 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 72/80...\n[TRAINER DEBUG] Episode 72 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 73/80...\n[TRAINER DEBUG] Episode 73 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 74/80...\n[TRAINER DEBUG] Episode 74 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 75/80...\n[TRAINER DEBUG] Episode 75 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 76/80...\n[TRAINER DEBUG] Episode 76 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 77/80...\n[TRAINER DEBUG] Episode 77 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 78/80...\n[TRAINER DEBUG] Episode 78 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 79/80...\n[TRAINER DEBUG] Episode 79 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 80/80...\n[TRAINER DEBUG] Episode 80 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 80 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 43: 80 episodes, success rate: 0.0%, avg reward: -0.50"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3825e503",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e7a12d4b",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "414cd42b",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bb38fb52",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a38125b0",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type('[18]', 'vir')",
            "noop()",
            "click('[19]')",
            "noop()",
            "noop()",
            "noop()",
            "fill('[18]', 'vir')",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "dd3ad3d7",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e468a1f0",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e500f499",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 80 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/80...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/80...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/80...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/80...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/80...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/80...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/80...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/80...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/80...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/80...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/80...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/80...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/80...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/80...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/80...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/80...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/80...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/80...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/80...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/80...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/80...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/80...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/80...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/80...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/80...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/80...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/80...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/80...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/80...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/80...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/80...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/80...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/80...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/80...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/80...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/80...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/80...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/80...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/80...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/80...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/80...\n[TRAINER DEBUG] Episode 41 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 42/80...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/80...\n[TRAINER DEBUG] Episode 43 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 44/80...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/80...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/80...\n[TRAINER DEBUG] Episode 46 reward: -0.5\u001b[34m[ReferenceModel-0/2] 2025-12-25 23:24:16 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-1/2] 2025-12-25 23:24:16 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.6, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.504, std=0.019\n[ADVANTAGES DEBUG] advantages: min=-5.01, max=0.20\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 23:24:17 INFO\u001b[0m [GC] Performing periodic GC collection took 0.10 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 23:24:18 INFO\u001b[0m Pushing weights for policy version 70\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 23:24:20 INFO\u001b[0m Completed weights push in 2.42 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 23:24:22 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 23:24:28 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 23:24:28 INFO\u001b[0m Weight update completed (now v70)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 70 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 71.0\n  buffer/sample/avg_sampled_policy_age: 0.0\n  buffer/sample/demand_to_size_ratio: 0.10256410256410256\n  buffer/sample/max_sampled_policy_age: 0.0\n  generator/generate/count_requests: 78.0\n  generator/generate/count_sequences_completed: 78.0\n  generator_perf/generate/total_duration_avg_s: 0.9771439252021986\n  generator_perf/generate/total_duration_max_s: 3.918535888671875\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.9501440543681383\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.490509033203125\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 0.5935182562097907\n  reference_perf/forward/total_duration_max_s: 0.5989647004753351\n  reward/avg_reward: -0.50375\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.300000000000004\n  rl_trainer/learning_rate: 8.637274549098197e-06\n  rl_trainer/loss: -0.11772198230028152\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.546743206679821\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.546743206679821\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013780379667878151\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013780379667878151\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.15098597668111324\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.15098597668111324\n  rl_trainer_perf/step/total_duration_avg_s: 0.711518831551075\n  rl_trainer_perf/step/total_duration_max_s: 0.711518831551075\n  task/avg_length: 10.0\n  task/avg_reward: 0.0\n  task/count_completed: 8.0\n  task/success_rate: 0.0\n  train/avg_reward: -0.5037499999999999\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n==============================\n\n\n[TRAINER DEBUG] Computing reward for episode 47/80...\n[TRAINER DEBUG] Episode 47 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 48/80...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/80...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/80...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/80...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/80...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/80...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/80...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/80...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/80...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/80...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/80...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/80...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/80...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/80...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/80...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/80...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/80...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/80...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 66/80...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/80...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/80...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 69/80...\n[TRAINER DEBUG] Episode 69 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 70/80...\n[TRAINER DEBUG] Episode 70 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 71/80...\n[TRAINER DEBUG] Episode 71 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 72/80...\n[TRAINER DEBUG] Episode 72 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 73/80...\n[TRAINER DEBUG] Episode 73 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 74/80...\n[TRAINER DEBUG] Episode 74 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 75/80...\n[TRAINER DEBUG] Episode 75 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 76/80...\n[TRAINER DEBUG] Episode 76 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 77/80...\n[TRAINER DEBUG] Episode 77 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 78/80...\n[TRAINER DEBUG] Episode 78 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 79/80...\n[TRAINER DEBUG] Episode 79 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 80/80...\n[TRAINER DEBUG] Episode 80 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 80 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 44: 80 episodes, success rate: 0.0%, avg reward: -0.50"
          ],
          "has_action_error": false
        }
      ]
    },
    {
      "step": 70,
      "num_tasks": 32,
      "success_rate": 0.33962264150943394,
      "avg_reward": 0.33962264150943394,
      "results": [
        {
          "task_id": "eec62bdd",
          "task_type": "enter-text-2",
          "benchmark": "miniwob",
          "goal": "Type \"ASHLEA\" in all lower case letters in the text input and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type(14, \"ashlea\")",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "1af36bff",
          "task_type": "enter-text-2",
          "benchmark": "miniwob",
          "goal": "Type \"SERGIO\" in all lower case letters in the text input and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type(14, \"sergio\")",
            "noop()",
            "click(15)",
            "noop()",
            "noop()",
            "noop()",
            "fill(14, \"sergio\")",
            "noop()",
            "click(15)",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "574e3852",
          "task_type": "enter-text-2",
          "benchmark": "miniwob",
          "goal": "Type \"kanesha\" in all upper case letters in the text input and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "fill('14', 'KANESHA')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7af702e3",
          "task_type": "enter-text-2",
          "benchmark": "miniwob",
          "goal": "Type \"emile\" in all upper case letters in the text input and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type(14, \"emile\")",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "78a3e740",
          "task_type": "enter-text-2",
          "benchmark": "miniwob",
          "goal": "Type \"leonie\" in all upper case letters in the text input and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type(14, \"leonie\")",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "e04f83d1",
          "task_type": "enter-text-2",
          "benchmark": "miniwob",
          "goal": "Type \"cheree\" in all upper case letters in the text input and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type(14, \"cheree\")",
            "noop()",
            "click(15)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "21ec8030",
          "task_type": "enter-text-2",
          "benchmark": "miniwob",
          "goal": "Type \"NATHALIE\" in all lower case letters in the text input and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type(14, 'nathalie')",
            "noop()",
            "click(15)",
            "noop()",
            "noop()",
            "noop()",
            "fill(14, \"nathalie\")",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "efc8f76d",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select 40O and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('30')",
            "click('33')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1a3ec4b9",
          "task_type": "simple-algebra",
          "benchmark": "miniwob",
          "goal": "Solve for x and type your answer into the textbox. Press Submit when done.",
          "success": true,
          "reward": 1.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "fill('16', '88')",
            "click('17')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6dff7d35",
          "task_type": "click-widget",
          "benchmark": "miniwob",
          "goal": "Click on a \"radio\" widget.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('15')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ee47cf3a",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a blue letter",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f02b1332",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00c3\u2013K\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('18')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d2de1d9b",
          "task_type": "read-table",
          "benchmark": "miniwob",
          "goal": "Enter the value of Gender into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "noop()",
            "noop()",
            "click('32')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "af0c4fdb",
          "task_type": "use-colorwheel",
          "benchmark": "miniwob",
          "goal": "Select gray with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f0fcf7e2",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": true,
          "reward": 1.0,
          "task_length": 3,
          "actions": [
            "noop()",
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "121b5262",
          "task_type": "enter-time",
          "benchmark": "miniwob",
          "goal": "Enter 12:02 PM as the time and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d9873307",
          "task_type": "drag-circle",
          "benchmark": "miniwob",
          "goal": "Drag the circle up then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "50caf6fe",
          "task_type": "click-color",
          "benchmark": "miniwob",
          "goal": "Click on the cyan colored box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "57a1bb62",
          "task_type": "click-menu-2",
          "benchmark": "miniwob",
          "goal": "Click the \"Menu\" button, and then find and click on the item with the \"ui-icon-play\" icon.",
          "success": true,
          "reward": 1.0,
          "task_length": 5,
          "actions": [
            "click('16')",
            "noop()",
            "click('22')",
            "noop()",
            "click('32')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c91e7f51",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select vpcnvl and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 4,
          "actions": [
            "select('21', 'vpcnvl')",
            "noop()",
            "noop()",
            "click('24')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "b9cd395c",
          "task_type": "drag-circle",
          "benchmark": "miniwob",
          "goal": "Drag the circle down then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1aaa49b7",
          "task_type": "click-tab-2-easy",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"ac\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "feaf3e83",
          "task_type": "drag-box",
          "benchmark": "miniwob",
          "goal": "Drag the smaller box so that it is completely inside the larger box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5af49bfa",
          "task_type": "count-sides",
          "benchmark": "miniwob",
          "goal": "Press the button that correctly denotes how many sides the shape has.",
          "success": false,
          "reward": 0.0,
          "task_length": 7,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('14')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c72fdf4b",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #457ce5 with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5f927414",
          "task_type": "click-shades",
          "benchmark": "miniwob",
          "goal": "Select all the shades of red and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "aa3e3e73",
          "task_type": "drag-circle",
          "benchmark": "miniwob",
          "goal": "Drag the circle up then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "517d2ee6",
          "task_type": "email-inbox-delete",
          "benchmark": "miniwob",
          "goal": "Find the email by Letta and click the trash icon to delete it.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f258bb66",
          "task_type": "click-tab",
          "benchmark": "miniwob",
          "goal": "Click on Tab #1.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('18')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "46a841cc",
          "task_type": "enter-password",
          "benchmark": "miniwob",
          "goal": "Enter the password \"x3phP\" into both text fields and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7f7254e5",
          "task_type": "click-checkboxes-transfer",
          "benchmark": "miniwob",
          "goal": "Select uXpKBx, Iviog and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "54aa2ae0",
          "task_type": "click-color",
          "benchmark": "miniwob",
          "goal": "Click on the blue colored box.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8621a49e",
          "task_type": "click-collapsible-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a5997d26",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"adipiscing.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "50292269",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00e5\u008f\u2013\u00e6\u00b6\u02c6\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "63abf353",
          "task_type": "enter-text",
          "benchmark": "miniwob",
          "goal": "Enter \"Livia\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('14', 'Livia')",
            "noop()",
            "click('15')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "7fa4f6e4",
          "task_type": "simple-algebra",
          "benchmark": "miniwob",
          "goal": "Solve for x and type your answer into the textbox. Press Submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "65b5797d",
          "task_type": "email-inbox-delete",
          "benchmark": "miniwob",
          "goal": "Find the email by Kinna and click the trash icon to delete it.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3539ac91",
          "task_type": "click-menu-2",
          "benchmark": "miniwob",
          "goal": "Click the \"Menu\" button, and then find and click on the item labeled \"Next\".",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "click('16')",
            "noop()",
            "click('41')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e98c4e9e",
          "task_type": "click-menu-2",
          "benchmark": "miniwob",
          "goal": "Click the \"Menu\" button, and then find and click on the item with the \"ui-icon-seek-end\" icon.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ae47e178",
          "task_type": "click-menu-2",
          "benchmark": "miniwob",
          "goal": "Click the \"Menu\" button, and then find and click on the item with the \"ui-icon-zoomout\" icon.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('16')",
            "click('44')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6d75a691",
          "task_type": "click-menu-2",
          "benchmark": "miniwob",
          "goal": "Click the \"Menu\" button, and then find and click on the item with the \"ui-icon-seek-start\" icon.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('22')",
            "noop()",
            "noop()",
            "noop()",
            "click('16')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "cc164f4a",
          "task_type": "click-menu-2",
          "benchmark": "miniwob",
          "goal": "Click the \"Menu\" button, and then find and click on the item labeled \"Next\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b743c5b8",
          "task_type": "click-menu-2",
          "benchmark": "miniwob",
          "goal": "Click the \"Menu\" button, and then find and click on the item labeled \"Zoom In\".",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('16')",
            "click('41')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3cedfffe",
          "task_type": "click-menu-2",
          "benchmark": "miniwob",
          "goal": "Click the \"Menu\" button, and then find and click on the item labeled \"Zoom Out\".",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('16')",
            "click('44')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "be8ec77c",
          "task_type": "click-menu-2",
          "benchmark": "miniwob",
          "goal": "Click the \"Menu\" button, and then find and click on the item labeled \"Stop\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 49 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/49...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/49...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/49...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/49...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/49...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/49...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/49...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/49...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/49...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/49...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/49...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/49...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/49...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/49...\n[TRAINER DEBUG] Episode 14 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 15/49...\n[TRAINER DEBUG] Episode 15 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 16/49...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/49...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/49...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/49...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/49...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/49...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/49...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/49...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/49...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/49...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/49...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/49...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/49...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/49...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/49...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/49...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/49...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/49...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/49...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/49...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/49...\n[TRAINER DEBUG] Episode 36 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 37/49...\n[TRAINER DEBUG] Episode 37 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 38/49...\n[TRAINER DEBUG] Episode 38 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 39/49...\n[TRAINER DEBUG] Episode 39 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 40/49...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/49...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/49...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/49...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/49...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/49...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/49...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/49...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/49...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/49...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 49 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 46: 49 episodes, success rate: 12.2%, avg reward: -0.14"
          ],
          "has_action_error": false
        },
        {
          "task_id": "708f41da",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": true,
          "reward": 1.0,
          "task_length": 7,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f8a1434f",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2e44b178",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f8e3337c",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "04f8ffd8",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "404b0455",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": true,
          "reward": 1.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6ebe46bd",
          "task_type": "click-dialog",
          "benchmark": "miniwob",
          "goal": "Close the dialog box by clicking the \"x\".",
          "success": true,
          "reward": 1.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "click('20')\n[TRAINER DEBUG] Creating 34 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/34...\n[TRAINER DEBUG] Episode 1 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 2/34...\n[TRAINER DEBUG] Episode 2 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 3/34...\n[TRAINER DEBUG] Episode 3 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 4/34...\n[TRAINER DEBUG] Episode 4 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 5/34...\n[TRAINER DEBUG] Episode 5 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 6/34...\n[TRAINER DEBUG] Episode 6 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 7/34...\n[TRAINER DEBUG] Episode 7 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 8/34...\n[TRAINER DEBUG] Episode 8 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 9/34...\n[TRAINER DEBUG] Episode 9 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 10/34...\n[TRAINER DEBUG] Episode 10 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 11/34...\n[TRAINER DEBUG] Episode 11 reward: 2.15\n[TRAINER DEBUG] Computing reward for episode 12/34...\n[TRAINER DEBUG] Episode 12 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 13/34...\n[TRAINER DEBUG] Episode 13 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 14/34...\n[TRAINER DEBUG] Episode 14 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 15/34...\n[TRAINER DEBUG] Episode 15 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 16/34...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/34...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/34...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/34...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/34...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/34...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/34...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/34...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/34...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/34...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/34...\n[TRAINER DEBUG] Episode 26 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 27/34...\n[TRAINER DEBUG] Episode 27 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 28/34...\n[TRAINER DEBUG] Episode 28 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 29/34...\n[TRAINER DEBUG] Episode 29 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 30/34...\n[TRAINER DEBUG] Episode 30 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 31/34...\n[TRAINER DEBUG] Episode 31 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 32/34...\n[TRAINER DEBUG] Episode 32 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 33/34...\n[TRAINER DEBUG] Episode 33 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 34/34...\n[TRAINER DEBUG] Episode 34 reward: 2.3\n[TRAINER DEBUG] Getting ref model logprobs for 34 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 47: 34 episodes, success rate: 70.6%, avg reward: 1.46"
          ],
          "has_action_error": false
        }
      ]
    },
    {
      "step": 75,
      "num_tasks": 32,
      "success_rate": 0.16666666666666666,
      "avg_reward": 0.16666666666666666,
      "results": [
        {
          "task_id": "93ea7eed",
          "task_type": "guess-number",
          "benchmark": "miniwob",
          "goal": "Guess the number between 0-9 and press Submit. Use the feedback below to find the right number.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "970491ab",
          "task_type": "guess-number",
          "benchmark": "miniwob",
          "goal": "Guess the number between 0-9 and press Submit. Use the feedback below to find the right number.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "138a5e7b",
          "task_type": "guess-number",
          "benchmark": "miniwob",
          "goal": "Guess the number between 0-9 and press Submit. Use the feedback below to find the right number.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "01ec5bd2",
          "task_type": "guess-number",
          "benchmark": "miniwob",
          "goal": "Guess the number between 0-9 and press Submit. Use the feedback below to find the right number.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5ff078c7",
          "task_type": "guess-number",
          "benchmark": "miniwob",
          "goal": "Guess the number between 0-9 and press Submit. Use the feedback below to find the right number.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "83e654d8",
          "task_type": "guess-number",
          "benchmark": "miniwob",
          "goal": "Guess the number between 0-9 and press Submit. Use the feedback below to find the right number.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "93d2c75c",
          "task_type": "guess-number",
          "benchmark": "miniwob",
          "goal": "Guess the number between 0-9 and press Submit. Use the feedback below to find the right number.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bf013693",
          "task_type": "click-tab-2-medium",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"et\".",
          "success": true,
          "reward": 1.0,
          "task_length": 8,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('21')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "88027c5e",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "669690fb",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": true,
          "reward": 1.0,
          "task_length": 10,
          "actions": [
            "type([15], \"-1\")",
            "noop()",
            "click([16])",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "fill('15', '-1')",
            "click('16')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "5f2289be",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/11/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 6,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "fill('17', '12/11/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "0515b7d9",
          "task_type": "click-link",
          "benchmark": "miniwob",
          "goal": "Click on the link \"Sem\".",
          "success": false,
          "reward": 0.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "584c4ac6",
          "task_type": "enter-text-dynamic",
          "benchmark": "miniwob",
          "goal": "Enter \"KdV61\" into the text field and press Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 3,
          "actions": [
            "noop()",
            "fill('14', 'KdV61')",
            "click('15')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f8031f15",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a red b",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bb02b3db",
          "task_type": "email-inbox-reply",
          "benchmark": "miniwob",
          "goal": "Find the email by Kathryne and reply to them with the text \"Posuere eu sed faucibus.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "209e2c6a",
          "task_type": "use-slider",
          "benchmark": "miniwob",
          "goal": "Select 26 with the slider and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2ff0e323",
          "task_type": "copy-paste-2",
          "benchmark": "miniwob",
          "goal": "Copy the text from the 1st text area below and paste it into the text input, then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "47fc2028",
          "task_type": "click-tab-2-hard",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"Nulla\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('19')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ed0548be",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d2128d66",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #101b0e with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5cd86b8f",
          "task_type": "click-tab-2-easy",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"interdum\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c39e49e3",
          "task_type": "use-slider",
          "benchmark": "miniwob",
          "goal": "Select -98 with the slider and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c2b435da",
          "task_type": "use-spinner",
          "benchmark": "miniwob",
          "goal": "Select -8 with the spinner and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1fa4ff8f",
          "task_type": "drag-circle",
          "benchmark": "miniwob",
          "goal": "Drag the circle down then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "431b64e9",
          "task_type": "click-menu-2",
          "benchmark": "miniwob",
          "goal": "Click the \"Menu\" button, and then find and click on the item with the \"ui-icon-zoomout\" icon.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('16')",
            "click('44')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e0cd9142",
          "task_type": "drag-single-shape",
          "benchmark": "miniwob",
          "goal": "Drag the item down then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bf3e6556",
          "task_type": "enter-time",
          "benchmark": "miniwob",
          "goal": "Enter 4:03 PM as the time and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "90b876cd",
          "task_type": "enter-password",
          "benchmark": "miniwob",
          "goal": "Enter the password \"a8\" into both text fields and press submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "524fae25",
          "task_type": "click-tab",
          "benchmark": "miniwob",
          "goal": "Click on Tab #3.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8071f99f",
          "task_type": "click-tab",
          "benchmark": "miniwob",
          "goal": "Click on Tab #1.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('18')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2c28e1f1",
          "task_type": "click-color",
          "benchmark": "miniwob",
          "goal": "Click on the blue colored box.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b6684340",
          "task_type": "click-test",
          "benchmark": "miniwob",
          "goal": "Click the button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f1ffe8bb",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"cras\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click(16)",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "22933317",
          "task_type": "click-color",
          "benchmark": "miniwob",
          "goal": "Click on the orange colored box.",
          "success": false,
          "reward": 0.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5421c7b3",
          "task_type": "use-slider-2",
          "benchmark": "miniwob",
          "goal": "Set the sliders to the combination [1,13,0] and submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "df198e9c",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Christmas Island, Japan from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "click('23')",
            "click('25')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3a446be0",
          "task_type": "click-shades",
          "benchmark": "miniwob",
          "goal": "Select all the shades of green and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e6bf51c7",
          "task_type": "click-pie",
          "benchmark": "miniwob",
          "goal": "Expand the pie menu below and click on the item labeled \"y\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('[13]')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "8c8c1b5d",
          "task_type": "copy-paste",
          "benchmark": "miniwob",
          "goal": "Copy the text in the textarea below, paste it into the textbox and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e82168d8",
          "task_type": "copy-paste",
          "benchmark": "miniwob",
          "goal": "Copy the text in the textarea below, paste it into the textbox and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ce023a6c",
          "task_type": "copy-paste",
          "benchmark": "miniwob",
          "goal": "Copy the text in the textarea below, paste it into the textbox and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d89f9202",
          "task_type": "copy-paste",
          "benchmark": "miniwob",
          "goal": "Copy the text in the textarea below, paste it into the textbox and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2d887bc6",
          "task_type": "copy-paste",
          "benchmark": "miniwob",
          "goal": "Copy the text in the textarea below, paste it into the textbox and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8fad2bb7",
          "task_type": "copy-paste",
          "benchmark": "miniwob",
          "goal": "Copy the text in the textarea below, paste it into the textbox and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "aad87cb0",
          "task_type": "copy-paste",
          "benchmark": "miniwob",
          "goal": "Copy the text in the textarea below, paste it into the textbox and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8f08ec92",
          "task_type": "copy-paste",
          "benchmark": "miniwob",
          "goal": "Copy the text in the textarea below, paste it into the textbox and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 80 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/80...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/80...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/80...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/80...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/80...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/80...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/80...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/80...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/80...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/80...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/80...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/80...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/80...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/80...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/80...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/80...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/80...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/80...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/80...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/80...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/80...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/80...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/80...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/80...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/80...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/80...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/80...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/80...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/80...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/80...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/80...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/80...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/80...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/80...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/80...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/80...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/80...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/80...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/80...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/80...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/80...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/80...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/80...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/80...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/80...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/80...\n[TRAINER DEBUG] Episode 46 reward: -0.5\u001b[34m[ReferenceModel-0/2] 2025-12-25 23:58:12 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-1/2] 2025-12-25 23:58:12 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.500, std=0.000\n[ADVANTAGES DEBUG] advantages: min=0.00, max=0.00\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 23:58:13 INFO\u001b[0m [GC] Performing periodic GC collection took 0.10 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 23:58:14 INFO\u001b[0m Pushing weights for policy version 78\n\u001b[34m[TitanTrainer-0/1] 2025-12-25 23:58:16 INFO\u001b[0m Completed weights push in 2.49 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-25 23:58:19 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-25 23:58:23 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-25 23:58:23 INFO\u001b[0m Weight update completed (now v78)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 78 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 72.0\n  buffer/sample/avg_sampled_policy_age: 0.0\n  buffer/sample/demand_to_size_ratio: 1.0526315789473684\n  buffer/sample/max_sampled_policy_age: 0.0\n  generator/generate/count_requests: 79.0\n  generator/generate/count_sequences_completed: 79.0\n  generator_perf/generate/total_duration_avg_s: 0.9809166792857499\n  generator_perf/generate/total_duration_max_s: 4.2845771484375\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 1.0009040776640177\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.490509033203125\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 0.5819260068237782\n  reference_perf/forward/total_duration_max_s: 0.5845871362835169\n  reward/avg_reward: -0.5\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.0\n  rl_trainer/learning_rate: 8.476953907815632e-06\n  rl_trainer/loss: 0.04940143972635269\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.5502845104783773\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.5502845104783773\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.0136712696403265\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.0136712696403265\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.15078220143914223\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.15078220143914223\n  rl_trainer_perf/step/total_duration_avg_s: 0.71473996527493\n  rl_trainer_perf/step/total_duration_max_s: 0.71473996527493\n  task/avg_length: 9.11111111111111\n  task/avg_reward: 0.0\n  task/count_completed: 9.0\n  task/success_rate: 0.0\n  train/avg_reward: -0.5\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n==============================\n\n\n[TRAINER DEBUG] Computing reward for episode 47/80...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/80...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/80...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/80...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/80...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/80...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/80...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/80...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/80...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/80...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/80...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/80...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/80...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/80...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/80...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/80...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/80...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/80...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/80...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 66/80...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/80...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/80...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 69/80...\n[TRAINER DEBUG] Episode 69 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 70/80...\n[TRAINER DEBUG] Episode 70 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 71/80...\n[TRAINER DEBUG] Episode 71 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 72/80...\n[TRAINER DEBUG] Episode 72 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 73/80...\n[TRAINER DEBUG] Episode 73 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 74/80...\n[TRAINER DEBUG] Episode 74 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 75/80...\n[TRAINER DEBUG] Episode 75 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 76/80...\n[TRAINER DEBUG] Episode 76 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 77/80...\n[TRAINER DEBUG] Episode 77 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 78/80...\n[TRAINER DEBUG] Episode 78 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 79/80...\n[TRAINER DEBUG] Episode 79 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 80/80...\n[TRAINER DEBUG] Episode 80 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 80 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 49: 80 episodes, success rate: 0.0%, avg reward: -0.50"
          ],
          "has_action_error": false
        },
        {
          "task_id": "aa33e8da",
          "task_type": "enter-text",
          "benchmark": "miniwob",
          "goal": "Enter \"Cierra\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "type('14', 'Cierra')",
            "click('15')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "14125ad2",
          "task_type": "enter-text",
          "benchmark": "miniwob",
          "goal": "Enter \"Rex\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "type(14, \"Rex\")",
            "click(15)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "fill(14, \"Rex\")",
            "noop()",
            "click(15)"
          ],
          "has_action_error": true
        },
        {
          "task_id": "4a07dc30",
          "task_type": "enter-text",
          "benchmark": "miniwob",
          "goal": "Enter \"Alan\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('14', 'Alan')",
            "noop()",
            "click('15')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "6d92ea23",
          "task_type": "enter-text",
          "benchmark": "miniwob",
          "goal": "Enter \"Jerald\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('14', 'Jerald')",
            "noop()",
            "click('15')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "78bedacd",
          "task_type": "enter-text",
          "benchmark": "miniwob",
          "goal": "Enter \"Tora\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "fill(14, \"Tora\")",
            "click(15)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "f1c1b210",
          "task_type": "enter-text",
          "benchmark": "miniwob",
          "goal": "Enter \"Nieves\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "type(14, \"Nieves\")",
            "click(15)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "fill(14, \"Nieves\")",
            "noop()",
            "click(15)"
          ],
          "has_action_error": true
        },
        {
          "task_id": "a8e21cb6",
          "task_type": "enter-text",
          "benchmark": "miniwob",
          "goal": "Enter \"Agustina\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('14', 'Agustina')",
            "noop()",
            "click('15')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "1a097682",
          "task_type": "enter-text",
          "benchmark": "miniwob",
          "goal": "Enter \"Jess\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('14', 'Jess')",
            "noop()",
            "click('15')\n[TRAINER DEBUG] Creating 44 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/44...\n[TRAINER DEBUG] Episode 1 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 2/44...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/44...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/44...\n[TRAINER DEBUG] Episode 4 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 5/44...\n[TRAINER DEBUG] Episode 5 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 6/44...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/44...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/44...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/44...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/44...\n[TRAINER DEBUG] Episode 10 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 11/44...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/44...\n[TRAINER DEBUG] Episode 12 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 13/44...\n[TRAINER DEBUG] Episode 13 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 14/44...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/44...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/44...\n[TRAINER DEBUG] Episode 16 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 17/44...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/44...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/44...\n[TRAINER DEBUG] Episode 19 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 20/44...\n[TRAINER DEBUG] Episode 20 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 21/44...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/44...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/44...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/44...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/44...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/44...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/44...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/44...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/44...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/44...\n[TRAINER DEBUG] Episode 30 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 31/44...\n[TRAINER DEBUG] Episode 31 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 32/44...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/44...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/44...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/44...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/44...\n[TRAINER DEBUG] Episode 36 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 37/44...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/44...\n[TRAINER DEBUG] Episode 38 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 39/44...\n[TRAINER DEBUG] Episode 39 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 40/44...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/44...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/44...\n[TRAINER DEBUG] Episode 42 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 43/44...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/44...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 44 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 50: 44 episodes, success rate: 0.0%, avg reward: -0.53"
          ],
          "has_action_error": true
        }
      ]
    },
    {
      "step": 80,
      "num_tasks": 32,
      "success_rate": 0.2727272727272727,
      "avg_reward": 0.2727272727272727,
      "results": [
        {
          "task_id": "25337d53",
          "task_type": "resize-textarea",
          "benchmark": "miniwob",
          "goal": "Resize the textarea so that the height is larger than its initial size then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a85aa1fc",
          "task_type": "resize-textarea",
          "benchmark": "miniwob",
          "goal": "Resize the textarea so that the height is smaller than its initial size then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "4f44ac1f",
          "task_type": "resize-textarea",
          "benchmark": "miniwob",
          "goal": "Resize the textarea so that the height is larger than its initial size then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a925e7f6",
          "task_type": "resize-textarea",
          "benchmark": "miniwob",
          "goal": "Resize the textarea so that the width is larger than its initial size then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "40082446",
          "task_type": "resize-textarea",
          "benchmark": "miniwob",
          "goal": "Resize the textarea so that the width is larger than its initial size then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "acb7af79",
          "task_type": "resize-textarea",
          "benchmark": "miniwob",
          "goal": "Resize the textarea so that the width is smaller than its initial size then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "94f01e54",
          "task_type": "resize-textarea",
          "benchmark": "miniwob",
          "goal": "Resize the textarea so that the width is smaller than its initial size then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ebd5265b",
          "task_type": "read-table",
          "benchmark": "miniwob",
          "goal": "Enter the value of Religion into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5d57013c",
          "task_type": "simple-arithmetic",
          "benchmark": "miniwob",
          "goal": "Solve the math problem and type your answer into the textbox. Press submit when done.",
          "success": true,
          "reward": 1.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "fill('15', '3')",
            "click('16')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ae2fa07d",
          "task_type": "count-shape",
          "benchmark": "miniwob",
          "goal": "How many large blue 0s are there?",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "78d25967",
          "task_type": "drag-circle",
          "benchmark": "miniwob",
          "goal": "Drag the circle left then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3f549a8d",
          "task_type": "use-slider",
          "benchmark": "miniwob",
          "goal": "Select -93 with the slider and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6a4988c1",
          "task_type": "unicode-test",
          "benchmark": "miniwob",
          "goal": "Click on the \"\u00e5\u008f\u2013\u00e6\u00b6\u02c6\" button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a2cccdf3",
          "task_type": "enter-date",
          "benchmark": "miniwob",
          "goal": "Enter 04/28/2015 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "4f5057f9",
          "task_type": "copy-paste",
          "benchmark": "miniwob",
          "goal": "Copy the text in the textarea below, paste it into the textbox and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "844addcf",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select ty and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "select('27', 'ty')",
            "click('30')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "04f6c83e",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"pellentesque\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ede9abf1",
          "task_type": "click-collapsible",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 7,
          "actions": [
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "87105f75",
          "task_type": "resize-textarea",
          "benchmark": "miniwob",
          "goal": "Resize the textarea so that the width is smaller than its initial size then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b0df61c3",
          "task_type": "click-tab-2-medium",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"Sed\".",
          "success": false,
          "reward": 0.0,
          "task_length": 8,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "689ff9c2",
          "task_type": "click-button-sequence",
          "benchmark": "miniwob",
          "goal": "Click button ONE, then click button TWO.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('12')",
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a8d88137",
          "task_type": "use-slider",
          "benchmark": "miniwob",
          "goal": "Select 80 with the slider and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1bc193c0",
          "task_type": "text-editor",
          "benchmark": "miniwob",
          "goal": "Using the text editor, give the text Nec the style underlined and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d924647a",
          "task_type": "text-editor",
          "benchmark": "miniwob",
          "goal": "Using the text editor, give everything the style italics and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f1645bff",
          "task_type": "count-sides",
          "benchmark": "miniwob",
          "goal": "Press the button that correctly denotes how many sides the shape has.",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('14')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "225b7e67",
          "task_type": "highlight-text",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the paragraph below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "dd6d8565",
          "task_type": "guess-number",
          "benchmark": "miniwob",
          "goal": "Guess the number between 0-9 and press Submit. Use the feedback below to find the right number.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7850bef3",
          "task_type": "find-word",
          "benchmark": "miniwob",
          "goal": "Find the 11th word in the paragraph, type that into the textbox and press \"Submit\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2343d4fd",
          "task_type": "scroll-text-2",
          "benchmark": "miniwob",
          "goal": "Scroll the textarea to the bottom of the text hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "scroll('down')",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "d5e6b95f",
          "task_type": "text-editor",
          "benchmark": "miniwob",
          "goal": "Using the text editor, give the text Turpis the style italics and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3aa2f7c6",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Iceland from the scroll list and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "click('14')",
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9bcbd140",
          "task_type": "scroll-text-2",
          "benchmark": "miniwob",
          "goal": "Scroll the textarea to the bottom of the text hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "16eb3722",
          "task_type": "scroll-text",
          "benchmark": "miniwob",
          "goal": "Find the last word in the text area, enter it into the text field and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d0e9951b",
          "task_type": "use-autocomplete-nodelay",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"Mo\".",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('17', 'Mo')",
            "noop()",
            "click('18')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "83e6459b",
          "task_type": "simple-algebra",
          "benchmark": "miniwob",
          "goal": "Solve for x and type your answer into the textbox. Press Submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "fill([16], '55')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "47133bda",
          "task_type": "focus-text-2",
          "benchmark": "miniwob",
          "goal": "Focus into the 1st input textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d8bb7559",
          "task_type": "drag-single-shape",
          "benchmark": "miniwob",
          "goal": "Drag the item down then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "badf7a36",
          "task_type": "click-dialog-2",
          "benchmark": "miniwob",
          "goal": "Click the button in the dialog box labeled \"Cancel\".",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('27')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8b8e6073",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f47ca32d",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3b196a40",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ee35a224",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5981b9fb",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "93171566",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a398b368",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "9130ebc1",
          "task_type": "text-transform",
          "benchmark": "miniwob",
          "goal": "Type the text below into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()\n[TRAINER DEBUG] Creating 80 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/80...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/80...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/80...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/80...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/80...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/80...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/80...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/80...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/80...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/80...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/80...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/80...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/80...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/80...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/80...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/80...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/80...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/80...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/80...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/80...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/80...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/80...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/80...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/80...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/80...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/80...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/80...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/80...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/80...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/80...\n[TRAINER DEBUG] Episode 30 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 31/80...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/80...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/80...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/80...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/80...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/80...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/80...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/80...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/80...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/80...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/80...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/80...\n[TRAINER DEBUG] Episode 42 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 43/80...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/80...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/80...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/80...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/80...\n[TRAINER DEBUG] Episode 47 reward: -0.5\u001b[34m[ReferenceModel-0/2] 2025-12-26 00:18:14 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-1/2] 2025-12-26 00:18:14 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.500, std=0.000\n[ADVANTAGES DEBUG] advantages: min=0.00, max=0.00\n\u001b[34m[TitanTrainer-0/1] 2025-12-26 00:18:15 INFO\u001b[0m [GC] Performing periodic GC collection took 0.16 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-26 00:18:16 INFO\u001b[0m Pushing weights for policy version 83\n\u001b[34m[TitanTrainer-0/1] 2025-12-26 00:18:18 INFO\u001b[0m Completed weights push in 2.45 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-26 00:18:18 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-26 00:18:23 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-26 00:18:23 INFO\u001b[0m Weight update completed (now v83)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 83 ===\n  buffer/add/count_episodes_added: 80.0\n  buffer/evict/sum_episodes_evicted: 73.0\n  buffer/sample/avg_sampled_policy_age: 0.125\n  buffer/sample/demand_to_size_ratio: 0.10666666666666667\n  buffer/sample/max_sampled_policy_age: 1.0\n  generator/generate/count_requests: 78.0\n  generator/generate/count_sequences_completed: 78.0\n  generator_perf/generate/total_duration_avg_s: 0.9733224514692258\n  generator_perf/generate/total_duration_max_s: 4.050919189453125\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.1176876351237297\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 14.490509033203125\n  reference_perf/forward/memory_peak_max_gb: 36.414329528808594\n  reference_perf/forward/total_duration_avg_s: 0.5795742925256491\n  reference_perf/forward/total_duration_max_s: 0.5889773406088352\n  reward/avg_reward: -0.5\n  reward/success_rate: 0.0\n  reward/sum_reward: -40.0\n  rl_trainer/learning_rate: 8.376753507014028e-06\n  rl_trainer/loss: 0.0866841971874237\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.45214653201401234\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.45214653201401234\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013816621154546738\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013816621154546738\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.15131673216819763\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.15131673216819763\n  rl_trainer_perf/step/total_duration_avg_s: 0.6172824557870626\n  rl_trainer_perf/step/total_duration_max_s: 0.6172824557870626\n  task/avg_length: 8.2\n  task/avg_reward: 0.2\n  task/count_completed: 10.0\n  task/success_rate: 0.2\n  train/avg_reward: -0.5\n  train/episodes_per_rollout: 80.0\n  train/success_rate: 0.0\n==============================\n\n\n[TRAINER DEBUG] Computing reward for episode 48/80...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/80...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/80...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/80...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/80...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/80...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/80...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/80...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/80...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/80...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 58/80...\n[TRAINER DEBUG] Episode 58 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 59/80...\n[TRAINER DEBUG] Episode 59 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 60/80...\n[TRAINER DEBUG] Episode 60 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 61/80...\n[TRAINER DEBUG] Episode 61 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 62/80...\n[TRAINER DEBUG] Episode 62 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 63/80...\n[TRAINER DEBUG] Episode 63 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 64/80...\n[TRAINER DEBUG] Episode 64 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 65/80...\n[TRAINER DEBUG] Episode 65 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 66/80...\n[TRAINER DEBUG] Episode 66 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 67/80...\n[TRAINER DEBUG] Episode 67 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 68/80...\n[TRAINER DEBUG] Episode 68 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 69/80...\n[TRAINER DEBUG] Episode 69 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 70/80...\n[TRAINER DEBUG] Episode 70 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 71/80...\n[TRAINER DEBUG] Episode 71 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 72/80...\n[TRAINER DEBUG] Episode 72 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 73/80...\n[TRAINER DEBUG] Episode 73 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 74/80...\n[TRAINER DEBUG] Episode 74 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 75/80...\n[TRAINER DEBUG] Episode 75 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 76/80...\n[TRAINER DEBUG] Episode 76 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 77/80...\n[TRAINER DEBUG] Episode 77 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 78/80...\n[TRAINER DEBUG] Episode 78 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 79/80...\n[TRAINER DEBUG] Episode 79 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 80/80...\n[TRAINER DEBUG] Episode 80 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 80 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 52: 80 episodes, success rate: 0.0%, avg reward: -0.50"
          ],
          "has_action_error": false
        },
        {
          "task_id": "da146ff8",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "46559230",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d10116f8",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "597a7cb1",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "28f97689",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('13')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('13')",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "a4b5598c",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "11c03493",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7a2582f5",
          "task_type": "click-test-transfer",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')\n[TRAINER DEBUG] Creating 17 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/17...\n[TRAINER DEBUG] Episode 1 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 2/17...\n[TRAINER DEBUG] Episode 2 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 3/17...\n[TRAINER DEBUG] Episode 3 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 4/17...\n[TRAINER DEBUG] Episode 4 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 5/17...\n[TRAINER DEBUG] Episode 5 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 6/17...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/17...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/17...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/17...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/17...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/17...\n[TRAINER DEBUG] Episode 11 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 12/17...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/17...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/17...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/17...\n[TRAINER DEBUG] Episode 15 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 16/17...\n[TRAINER DEBUG] Episode 16 reward: 2.45\n[TRAINER DEBUG] Computing reward for episode 17/17...\n[TRAINER DEBUG] Episode 17 reward: 2.45\n[TRAINER DEBUG] Getting ref model logprobs for 17 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 53: 17 episodes, success rate: 41.2%, avg reward: 0.70"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7c3b2c5f",
          "task_type": "click-button-sequence",
          "benchmark": "miniwob",
          "goal": "Click button ONE, then click button TWO.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('12')",
            "click('13')"
          ],
          "has_action_error": false
        }
      ]
    },
    {
      "step": 85,
      "num_tasks": 32,
      "success_rate": 0.3018867924528302,
      "avg_reward": 0.3018867924528302,
      "results": [
        {
          "task_id": "6e60c74f",
          "task_type": "click-button-sequence",
          "benchmark": "miniwob",
          "goal": "Click button ONE, then click button TWO.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('12')",
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "24d427f6",
          "task_type": "click-button-sequence",
          "benchmark": "miniwob",
          "goal": "Click button ONE, then click button TWO.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('12')",
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e5c93468",
          "task_type": "click-button-sequence",
          "benchmark": "miniwob",
          "goal": "Click button ONE, then click button TWO.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('12')",
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ba41375d",
          "task_type": "click-button-sequence",
          "benchmark": "miniwob",
          "goal": "Click button ONE, then click button TWO.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('12')",
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ac1cbb95",
          "task_type": "click-button-sequence",
          "benchmark": "miniwob",
          "goal": "Click button ONE, then click button TWO.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('12')",
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "3633a034",
          "task_type": "click-button-sequence",
          "benchmark": "miniwob",
          "goal": "Click button ONE, then click button TWO.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('12')",
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "4cc20f6a",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a green item",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "74d06d0c",
          "task_type": "identify-shape",
          "benchmark": "miniwob",
          "goal": "Click the button that best describes the figure below.",
          "success": true,
          "reward": 1.0,
          "task_length": 9,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('18')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "926a47f1",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Denmark, Gibraltar from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "017c791c",
          "task_type": "enter-text",
          "benchmark": "miniwob",
          "goal": "Enter \"Cheree\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "type('14', 'Cheree')",
            "click('15')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "f4e772c0",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select 7kd and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('27')",
            "click('30')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7fa02bab",
          "task_type": "click-button-sequence",
          "benchmark": "miniwob",
          "goal": "Click button ONE, then click button TWO.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('12')",
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "56986d6f",
          "task_type": "click-widget",
          "benchmark": "miniwob",
          "goal": "Click on a \"text\" widget.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('14')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "a35be507",
          "task_type": "drag-single-shape",
          "benchmark": "miniwob",
          "goal": "Drag the item left then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0ae004d1",
          "task_type": "choose-list",
          "benchmark": "miniwob",
          "goal": "Select Mongolia from the list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "select('15', 'Mongolia')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "b8f73a59",
          "task_type": "click-dialog-2",
          "benchmark": "miniwob",
          "goal": "Click the button in the dialog box labeled \"x\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2fa2b0e0",
          "task_type": "guess-number",
          "benchmark": "miniwob",
          "goal": "Guess the number between 0-9 and press Submit. Use the feedback below to find the right number.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8c80e8a2",
          "task_type": "choose-date-medium",
          "benchmark": "miniwob",
          "goal": "Select 11/10/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d466dbfd",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"maecenas\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6a60e961",
          "task_type": "use-autocomplete-nodelay",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"Mexi\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type([17], \"Mexi\")",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click([18])",
            "fill([17], \"Mexi\")",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "3227a5a7",
          "task_type": "click-link",
          "benchmark": "miniwob",
          "goal": "Click on the link \"tellus\".",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "194b8945",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select QT and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "noop()",
            "check('30')",
            "click('33')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "acbefed9",
          "task_type": "click-tab-2-medium",
          "benchmark": "miniwob",
          "goal": "Switch between the tabs to find and click on the link \"amet\".",
          "success": true,
          "reward": 1.0,
          "task_length": 8,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ac2e2ec8",
          "task_type": "use-autocomplete",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"Yem\".",
          "success": false,
          "reward": 0.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "type('17', 'Yem')",
            "noop()",
            "click('18')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "13e160c1",
          "task_type": "drag-single-shape",
          "benchmark": "miniwob",
          "goal": "Drag the item down then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "43244bfa",
          "task_type": "email-inbox-delete",
          "benchmark": "miniwob",
          "goal": "Find the email by Andie and click the trash icon to delete it.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c435f6f9",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a triangle",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c5ff176e",
          "task_type": "click-test",
          "benchmark": "miniwob",
          "goal": "Click the button.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d8f4e4fe",
          "task_type": "highlight-text",
          "benchmark": "miniwob",
          "goal": "Highlight the text in the paragraph below and click submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "1e47424c",
          "task_type": "copy-paste-2",
          "benchmark": "miniwob",
          "goal": "Copy the text from the 2nd text area below and paste it into the text input, then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "87c039b5",
          "task_type": "email-inbox-reply",
          "benchmark": "miniwob",
          "goal": "Find the email by Kristyn and reply to them with the text \"Vel enim, vivamus ut.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "69c2b645",
          "task_type": "guess-number",
          "benchmark": "miniwob",
          "goal": "Guess the number between 0-9 and press Submit. Use the feedback below to find the right number.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ccbd0f1c",
          "task_type": "scroll-text",
          "benchmark": "miniwob",
          "goal": "Find the last word in the text area, enter it into the text field and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b57ca61c",
          "task_type": "click-collapsible-2",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"mi.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "ac1b0078",
          "task_type": "click-shades",
          "benchmark": "miniwob",
          "goal": "Select all the shades of green and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "dec45936",
          "task_type": "focus-text-2",
          "benchmark": "miniwob",
          "goal": "Focus into the 3rd input textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('14')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "c12360a6",
          "task_type": "enter-text-dynamic",
          "benchmark": "miniwob",
          "goal": "Enter \"Z7Nf\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('14', 'Z7Nf')",
            "noop()",
            "click('15')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "f64bb1b1",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/27/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('17', '12/27/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "d70dd422",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/05/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "af3a7948",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/02/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('17', '12/02/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "55cbf0b5",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/14/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 5,
          "actions": [
            "noop()",
            "noop()",
            "fill('17', '12/14/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "09ece682",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/16/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "fill('[17]', '12/16/2016')",
            "noop()",
            "click('[19]')",
            "noop()",
            "noop()",
            "noop()",
            "fill('[17]', '12/16/2016')",
            "noop()",
            "click('[19]')",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "c7ff1881",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/09/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('17', '12/09/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "0b20a2f2",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/25/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 5,
          "actions": [
            "noop()",
            "noop()",
            "fill('17', '12/25/2016')",
            "noop()",
            "click('19')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "74738f77",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/15/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 5,
          "actions": [
            "noop()",
            "noop()",
            "type('17', '12/15/2016')",
            "noop()",
            "click('19')\n[TRAINER DEBUG] Creating 44 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/44...\n[TRAINER DEBUG] Episode 1 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 2/44...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/44...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/44...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/44...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/44...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/44...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/44...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/44...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/44...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/44...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/44...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/44...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/44...\n[TRAINER DEBUG] Episode 14 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 15/44...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/44...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/44...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/44...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/44...\n[TRAINER DEBUG] Episode 19 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 20/44...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/44...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/44...\n[TRAINER DEBUG] Episode 22 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 23/44...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/44...\n[TRAINER DEBUG] Episode 24 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 25/44...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/44...\n[TRAINER DEBUG] Episode 26 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 27/44...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/44...\n[TRAINER DEBUG] Episode 28 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 29/44...\n[TRAINER DEBUG] Episode 29 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 30/44...\n[TRAINER DEBUG] Episode 30 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 31/44...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/44...\n[TRAINER DEBUG] Episode 32 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 33/44...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/44...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/44...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/44...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/44...\n[TRAINER DEBUG] Episode 37 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 38/44...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/44...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/44...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/44...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/44...\n[TRAINER DEBUG] Episode 42 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 43/44...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/44...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 44 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 55: 44 episodes, success rate: 0.0%, avg reward: -0.52"
          ],
          "has_action_error": true
        },
        {
          "task_id": "0b89b32f",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Netherlands from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "afc2dde1",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Morganne from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "click('[16]')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "82d10449",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Mauritania from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 8,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "select('19', 'Mauritania')",
            "noop()",
            "click('25')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "d75db234",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select French Guiana from the scroll list and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('19')",
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "d81e7a5a",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Marchelle, Gabriel from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0af33071",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Breanne from the scroll list and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('14')",
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "57291c36",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Colombia, Cape Verde from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8303bc1b",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Turkmenistan from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 5,
          "actions": [
            "noop()",
            "noop()",
            "select('21', 'Turkmenistan')",
            "noop()",
            "click('24')\n[TRAINER DEBUG] Creating 57 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/57...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/57...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/57...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/57...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/57...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/57...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/57...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/57...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/57...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/57...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/57...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/57...\n[TRAINER DEBUG] Episode 12 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 13/57...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/57...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/57...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/57...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/57...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/57...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/57...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/57...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/57...\n[TRAINER DEBUG] Episode 21 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 22/57...\n[TRAINER DEBUG] Episode 22 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 23/57...\n[TRAINER DEBUG] Episode 23 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 24/57...\n[TRAINER DEBUG] Episode 24 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 25/57...\n[TRAINER DEBUG] Episode 25 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 26/57...\n[TRAINER DEBUG] Episode 26 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 27/57...\n[TRAINER DEBUG] Episode 27 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 28/57...\n[TRAINER DEBUG] Episode 28 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 29/57...\n[TRAINER DEBUG] Episode 29 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 30/57...\n[TRAINER DEBUG] Episode 30 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 31/57...\n[TRAINER DEBUG] Episode 31 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 32/57...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/57...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/57...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/57...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/57...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/57...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/57...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/57...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/57...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/57...\n[TRAINER DEBUG] Episode 41 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 42/57...\n[TRAINER DEBUG] Episode 42 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 43/57...\n[TRAINER DEBUG] Episode 43 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 44/57...\n[TRAINER DEBUG] Episode 44 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 45/57...\n[TRAINER DEBUG] Episode 45 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 46/57...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/57...\n[TRAINER DEBUG] Episode 47 reward: -0.5\u001b[34m[ReferenceModel-0/2] 2025-12-26 00:33:49 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-1/2] 2025-12-26 00:33:49 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5, 2.4, 2.4, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, 2.4, 2.4, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.6, -0.5, -0.5]\n[ADVANTAGES DEBUG] mean=-0.302, std=0.749\n[ADVANTAGES DEBUG] advantages: min=-0.40, max=3.61\n\u001b[34m[TitanTrainer-0/1] 2025-12-26 00:33:51 INFO\u001b[0m [GC] Performing periodic GC collection took 0.10 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-26 00:33:51 INFO\u001b[0m Pushing weights for policy version 89\n\u001b[34m[TitanTrainer-0/1] 2025-12-26 00:33:54 INFO\u001b[0m Completed weights push in 2.38 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-26 00:33:56 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-26 00:34:00 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-26 00:34:00 INFO\u001b[0m Weight update completed (now v89)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 89 ===\n  buffer/add/count_episodes_added: 57.0\n  buffer/evict/sum_episodes_evicted: 42.0\n  buffer/sample/avg_sampled_policy_age: 0.125\n  buffer/sample/demand_to_size_ratio: 0.14545454545454545\n  buffer/sample/max_sampled_policy_age: 1.0\n  generator/generate/count_requests: 55.0\n  generator/generate/count_sequences_completed: 55.0\n  generator_perf/generate/total_duration_avg_s: 0.9627955871582036\n  generator_perf/generate/total_duration_max_s: 4.33258740234375\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.9420146159827709\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 10.32450008392334\n  reference_perf/forward/memory_peak_max_gb: 26.416075706481934\n  reference_perf/forward/total_duration_avg_s: 0.5704016834497452\n  reference_perf/forward/total_duration_max_s: 0.5738773625344038\n  reward/avg_reward: -0.3017543859649123\n  reward/success_rate: 0.07017543859649122\n  reward/sum_reward: -17.2\n  rl_trainer/learning_rate: 8.256513026052104e-06\n  rl_trainer/loss: -0.15831539034843445\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.5516546294093132\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.5516546294093132\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013769986107945442\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013769986107945442\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.14838938415050507\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.14838938415050507\n  rl_trainer_perf/step/total_duration_avg_s: 0.7138225547969341\n  rl_trainer_perf/step/total_duration_max_s: 0.7138225547969341\n  task/avg_length: 7.125\n  task/avg_reward: 0.25\n  task/count_completed: 8.0\n  task/success_rate: 0.25\n  train/avg_reward: -0.3017543859649123\n  train/episodes_per_rollout: 57.0\n  train/success_rate: 0.07017543859649122\n==============================\n\n\n[TRAINER DEBUG] Computing reward for episode 48/57...\n[TRAINER DEBUG] Episode 48 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 49/57...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/57...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/57...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/57...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/57...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/57...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/57...\n[TRAINER DEBUG] Episode 55 reward: -0.6\n[TRAINER DEBUG] Computing reward for episode 56/57...\n[TRAINER DEBUG] Episode 56 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 57/57...\n[TRAINER DEBUG] Episode 57 reward: -0.5\n[TRAINER DEBUG] Getting ref model logprobs for 57 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 56: 57 episodes, success rate: 7.0%, avg reward: -0.30"
          ],
          "has_action_error": true
        }
      ]
    },
    {
      "step": 90,
      "num_tasks": 32,
      "success_rate": 0.21739130434782608,
      "avg_reward": 0.21739130434782608,
      "results": [
        {
          "task_id": "744a6e90",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"dignissim\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7db515e7",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"viverra\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f6f0502a",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"tortor\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click(16)",
            "noop()",
            "noop()",
            "click(23)",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "bfc6e510",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"id.\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "click('16')",
            "noop()",
            "noop()",
            "noop()",
            "click('27')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f16ceeeb",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"consectetur\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('16')",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "25b125bc",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"non\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click(16)",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "65edd119",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"suscipit\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "5ebbcaaf",
          "task_type": "click-checkboxes-large",
          "benchmark": "miniwob",
          "goal": "Select uKCy6, cVZJs, AiBdx5X, elxb, bI6Hi, Uju, 0GXpRK, ZUMgWH, vjdt, oICa, sKFmDu and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "543bf951",
          "task_type": "click-collapsible-2-nodelay",
          "benchmark": "miniwob",
          "goal": "Expand the sections below, to find and click on the link \"at\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "eab71577",
          "task_type": "enter-text",
          "benchmark": "miniwob",
          "goal": "Enter \"Jess\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "type('14', 'Jess')",
            "noop()",
            "click('15')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "4b1cef23",
          "task_type": "enter-text-2",
          "benchmark": "miniwob",
          "goal": "Type \"KARRIE\" in all lower case letters in the text input and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "type(14, \"karrie\")",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "3edb8e65",
          "task_type": "click-checkboxes",
          "benchmark": "miniwob",
          "goal": "Select W5Y0yAW and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "check('W5Y0yAW')",
            "noop()",
            "click('Submit')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "check('W5Y0yAW')",
            "click('Submit')",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "cf247df0",
          "task_type": "click-menu-2",
          "benchmark": "miniwob",
          "goal": "Click the \"Menu\" button, and then find and click on the item with the \"ui-icon-zoomout\" icon.",
          "success": true,
          "reward": 1.0,
          "task_length": 3,
          "actions": [
            "click('16')",
            "noop()",
            "click('44')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6ccd7229",
          "task_type": "email-inbox-important",
          "benchmark": "miniwob",
          "goal": "Find the email by Nessy and click the star icon to mark it as important.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "b8e5d279",
          "task_type": "simple-algebra",
          "benchmark": "miniwob",
          "goal": "Solve for x and type your answer into the textbox. Press Submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type(16, '1')",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "ab1a67cc",
          "task_type": "guess-number",
          "benchmark": "miniwob",
          "goal": "Guess the number between 0-9 and press Submit. Use the feedback below to find the right number.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "178e5595",
          "task_type": "choose-date-easy",
          "benchmark": "miniwob",
          "goal": "Select 12/02/2016 as the date and hit submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bd0c8949",
          "task_type": "simple-algebra",
          "benchmark": "miniwob",
          "goal": "Solve for x and type your answer into the textbox. Press Submit when done.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "7645623f",
          "task_type": "use-slider",
          "benchmark": "miniwob",
          "goal": "Select -95 with the slider and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "70fce065",
          "task_type": "click-test-2",
          "benchmark": "miniwob",
          "goal": "Click button ONE.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8a97ad3c",
          "task_type": "enter-text-2",
          "benchmark": "miniwob",
          "goal": "Type \"BEAULAH\" in all lower case letters in the text input and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "type(14, \"beaulah\")",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "5b314e22",
          "task_type": "focus-text",
          "benchmark": "miniwob",
          "goal": "Focus into the textbox.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('12')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "236a8ebd",
          "task_type": "click-collapsible",
          "benchmark": "miniwob",
          "goal": "Expand the section below and click submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 3,
          "actions": [
            "noop()",
            "click('16')",
            "click('22')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "e05257a8",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Switzerland from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 3,
          "actions": [
            "select('16', 'Switzerland')",
            "noop()",
            "click('25')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "d279a15d",
          "task_type": "use-slider-2",
          "benchmark": "miniwob",
          "goal": "Set the sliders to the combination [20,4,9] and submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "eb541eb6",
          "task_type": "use-colorwheel",
          "benchmark": "miniwob",
          "goal": "Select yellow with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6e18efa4",
          "task_type": "use-colorwheel-2",
          "benchmark": "miniwob",
          "goal": "Select the following color #bbd298 with the color picker and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "0aa93ffd",
          "task_type": "click-tab",
          "benchmark": "miniwob",
          "goal": "Click on Tab #2.",
          "success": true,
          "reward": 1.0,
          "task_length": 1,
          "actions": [
            "click('20')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "2a64281a",
          "task_type": "email-inbox-important",
          "benchmark": "miniwob",
          "goal": "Find the email by Gilberta and click the star icon to mark it as important.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "8f5f3698",
          "task_type": "scroll-text",
          "benchmark": "miniwob",
          "goal": "Find the last word in the text area, enter it into the text field and hit Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "cc45d133",
          "task_type": "enter-text",
          "benchmark": "miniwob",
          "goal": "Enter \"Cheree\" into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "type('14', 'Cheree')",
            "click('15')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "22987290",
          "task_type": "copy-paste-2",
          "benchmark": "miniwob",
          "goal": "Copy the text from the 3rd text area below and paste it into the text input, then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "f47a95a6",
          "task_type": "click-shape",
          "benchmark": "miniwob",
          "goal": "Click on a shape",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "noop()",
            "click('13')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "bd2897b6",
          "task_type": "click-option",
          "benchmark": "miniwob",
          "goal": "Select bn and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 2,
          "actions": [
            "select('21')",
            "click('24')"
          ],
          "has_action_error": true
        },
        {
          "task_id": "4f1c8db4",
          "task_type": "drag-single-shape",
          "benchmark": "miniwob",
          "goal": "Drag the item down then press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "36e4ec12",
          "task_type": "read-table",
          "benchmark": "miniwob",
          "goal": "Enter the value of Country into the text field and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "fill('[16] cell \"Country\"', 'Country')",
            "click('[32] button \"Submit\"')",
            "noop()"
          ],
          "has_action_error": true
        },
        {
          "task_id": "cd994447",
          "task_type": "copy-paste",
          "benchmark": "miniwob",
          "goal": "Copy the text in the textarea below, paste it into the textbox and press Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "17273290",
          "task_type": "use-autocomplete",
          "benchmark": "miniwob",
          "goal": "Enter an item that starts with \"Cong\" and ends with \"go\".",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "903fb965",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Ranique, Aurora from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6047ba59",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Netherlands, Luxembourg from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6ca3c3ee",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Bel from the scroll list and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 3,
          "actions": [
            "noop()",
            "click('18')",
            "click('23')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "cec1d2d5",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Timor-Leste from the scroll list and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 8,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "click('15')",
            "click('24')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "43b28dda",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Kristyn, Pandora from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "6b3aa10a",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Ileane from the scroll list and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 4,
          "actions": [
            "noop()",
            "noop()",
            "click('21')",
            "click('23')"
          ],
          "has_action_error": false
        },
        {
          "task_id": "072bd4b3",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Oman, Grenada from the scroll list and click Submit.",
          "success": false,
          "reward": 0.0,
          "task_length": 10,
          "actions": [
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()",
            "noop()"
          ],
          "has_action_error": false
        },
        {
          "task_id": "752498d0",
          "task_type": "click-scroll-list",
          "benchmark": "miniwob",
          "goal": "Select Erna from the scroll list and click Submit.",
          "success": true,
          "reward": 1.0,
          "task_length": 2,
          "actions": [
            "click('18')",
            "click('25')\n[TRAINER DEBUG] Creating 57 episodes...\n[TRAINER DEBUG] Computing reward for episode 1/57...\n[TRAINER DEBUG] Episode 1 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 2/57...\n[TRAINER DEBUG] Episode 2 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 3/57...\n[TRAINER DEBUG] Episode 3 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 4/57...\n[TRAINER DEBUG] Episode 4 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 5/57...\n[TRAINER DEBUG] Episode 5 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 6/57...\n[TRAINER DEBUG] Episode 6 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 7/57...\n[TRAINER DEBUG] Episode 7 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 8/57...\n[TRAINER DEBUG] Episode 8 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 9/57...\n[TRAINER DEBUG] Episode 9 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 10/57...\n[TRAINER DEBUG] Episode 10 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 11/57...\n[TRAINER DEBUG] Episode 11 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 12/57...\n[TRAINER DEBUG] Episode 12 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 13/57...\n[TRAINER DEBUG] Episode 13 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 14/57...\n[TRAINER DEBUG] Episode 14 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 15/57...\n[TRAINER DEBUG] Episode 15 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 16/57...\n[TRAINER DEBUG] Episode 16 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 17/57...\n[TRAINER DEBUG] Episode 17 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 18/57...\n[TRAINER DEBUG] Episode 18 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 19/57...\n[TRAINER DEBUG] Episode 19 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 20/57...\n[TRAINER DEBUG] Episode 20 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 21/57...\n[TRAINER DEBUG] Episode 21 reward: 2.35\n[TRAINER DEBUG] Computing reward for episode 22/57...\n[TRAINER DEBUG] Episode 22 reward: 2.35\n[TRAINER DEBUG] Computing reward for episode 23/57...\n[TRAINER DEBUG] Episode 23 reward: 2.35\n[TRAINER DEBUG] Computing reward for episode 24/57...\n[TRAINER DEBUG] Episode 24 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 25/57...\n[TRAINER DEBUG] Episode 25 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 26/57...\n[TRAINER DEBUG] Episode 26 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 27/57...\n[TRAINER DEBUG] Episode 27 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 28/57...\n[TRAINER DEBUG] Episode 28 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 29/57...\n[TRAINER DEBUG] Episode 29 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 30/57...\n[TRAINER DEBUG] Episode 30 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 31/57...\n[TRAINER DEBUG] Episode 31 reward: 2.1\n[TRAINER DEBUG] Computing reward for episode 32/57...\n[TRAINER DEBUG] Episode 32 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 33/57...\n[TRAINER DEBUG] Episode 33 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 34/57...\n[TRAINER DEBUG] Episode 34 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 35/57...\n[TRAINER DEBUG] Episode 35 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 36/57...\n[TRAINER DEBUG] Episode 36 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 37/57...\n[TRAINER DEBUG] Episode 37 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 38/57...\n[TRAINER DEBUG] Episode 38 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 39/57...\n[TRAINER DEBUG] Episode 39 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 40/57...\n[TRAINER DEBUG] Episode 40 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 41/57...\n[TRAINER DEBUG] Episode 41 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 42/57...\n[TRAINER DEBUG] Episode 42 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 43/57...\n[TRAINER DEBUG] Episode 43 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 44/57...\n[TRAINER DEBUG] Episode 44 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 45/57...\n[TRAINER DEBUG] Episode 45 reward: 2.3\n[TRAINER DEBUG] Computing reward for episode 46/57...\n[TRAINER DEBUG] Episode 46 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 47/57...\n[TRAINER DEBUG] Episode 47 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 48/57...\n[TRAINER DEBUG] Episode 48 reward: -0.5\u001b[34m[ReferenceModel-1/2] 2025-12-26 00:50:33 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n\u001b[34m[ReferenceModel-0/2] 2025-12-26 00:50:33 INFO\u001b[0m [GC] Performing periodic GC collection took 0.01 seconds\n[ADVANTAGES DEBUG] rewards: [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, 2.35, 2.35, 2.35, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, 2.1, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, 2.3, 2.3, 2.3, 2.3, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, 2.4, 2.4]\n[ADVANTAGES DEBUG] mean=0.313, std=1.260\n[ADVANTAGES DEBUG] advantages: min=-0.65, max=1.66\n\u001b[34m[TitanTrainer-0/1] 2025-12-26 00:50:35 INFO\u001b[0m [GC] Performing periodic GC collection took 0.09 seconds\n\u001b[34m[TitanTrainer-0/1] 2025-12-26 00:50:36 INFO\u001b[0m Pushing weights for policy version 93\n\u001b[34m[TitanTrainer-0/1] 2025-12-26 00:50:38 INFO\u001b[0m Completed weights push in 2.48 seconds\n\u001b[34m[GeneratorWorker-0/1] 2025-12-26 00:50:38 INFO\u001b[0m [PolicyWorker] update weights from torchstore.\nINFO 12-26 00:50:44 [block_pool.py:321] Successfully reset prefix cache\n\u001b[34m[Generator-0/1] 2025-12-26 00:50:44 INFO\u001b[0m Weight update completed (now v93)\nINFO:forge.util.logging: === [global_reduce] - METRICS STEP 93 ===\n  buffer/add/count_episodes_added: 57.0\n  buffer/evict/sum_episodes_evicted: 68.0\n  buffer/sample/avg_sampled_policy_age: 0.125\n  buffer/sample/demand_to_size_ratio: 0.14545454545454545\n  buffer/sample/max_sampled_policy_age: 1.0\n  generator/generate/count_requests: 54.0\n  generator/generate/count_sequences_completed: 54.0\n  generator_perf/generate/total_duration_avg_s: 0.9649763081868491\n  generator_perf/generate/total_duration_max_s: 4.4623505859375\n  generator_perf/update_weights/avg_waiting_for_generation_duration_s: 0.37699197232723236\n  generator_perf/update_weights/sum_pending_gen_requests: 1.0\n  reference_perf/forward/count_forward_passes: 2.0\n  reference_perf/forward/memory_delta_end_start_avg_gb: 10.32446575164795\n  reference_perf/forward/memory_peak_max_gb: 26.416075706481934\n  reference_perf/forward/total_duration_avg_s: 0.5762965139001608\n  reference_perf/forward/total_duration_max_s: 0.5777937714010477\n  reward/avg_reward: 0.31315789473684214\n  reward/success_rate: 0.2982456140350877\n  reward/sum_reward: 17.85\n  rl_trainer/learning_rate: 8.176352705410823e-06\n  rl_trainer/loss: 0.17047137022018433\n  rl_trainer_perf/step/forward_backward/duration_avg_s: 0.5407052934169769\n  rl_trainer_perf/step/forward_backward/duration_max_s: 0.5407052934169769\n  rl_trainer_perf/step/memory_delta_end_start_avg_gb: 0.00010156631469726562\n  rl_trainer_perf/step/memory_peak_max_gb: 22.124167442321777\n  rl_trainer_perf/step/optimizer_step/duration_avg_s: 0.013719374313950539\n  rl_trainer_perf/step/optimizer_step/duration_max_s: 0.013719374313950539\n  rl_trainer_perf/step/save_checkpoint/duration_avg_s: 0.15104220993816853\n  rl_trainer_perf/step/save_checkpoint/duration_max_s: 0.15104220993816853\n  rl_trainer_perf/step/total_duration_avg_s: 0.7054690103977919\n  rl_trainer_perf/step/total_duration_max_s: 0.7054690103977919\n  task/avg_length: 7.125\n  task/avg_reward: 0.5\n  task/count_completed: 8.0\n  task/success_rate: 0.5\n  train/avg_reward: 0.31315789473684214\n  train/episodes_per_rollout: 57.0\n  train/success_rate: 0.2982456140350877\n==============================\n\n\n[TRAINER DEBUG] Computing reward for episode 49/57...\n[TRAINER DEBUG] Episode 49 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 50/57...\n[TRAINER DEBUG] Episode 50 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 51/57...\n[TRAINER DEBUG] Episode 51 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 52/57...\n[TRAINER DEBUG] Episode 52 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 53/57...\n[TRAINER DEBUG] Episode 53 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 54/57...\n[TRAINER DEBUG] Episode 54 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 55/57...\n[TRAINER DEBUG] Episode 55 reward: -0.5\n[TRAINER DEBUG] Computing reward for episode 56/57...\n[TRAINER DEBUG] Episode 56 reward: 2.4\n[TRAINER DEBUG] Computing reward for episode 57/57...\n[TRAINER DEBUG] Episode 57 reward: 2.4\n[TRAINER DEBUG] Getting ref model logprobs for 57 episodes...\n[TRAINER DEBUG] Got ref logprobs\n[TRAINER DEBUG] Computing advantages...\n[TRAINER DEBUG] Adding episodes to replay buffer...\n[TRAINER DEBUG] Episodes added to replay buffer\nRollout 58: 57 episodes, success rate: 29.8%, avg reward: 0.31"
          ],
          "has_action_error": false
        }
      ]
    }
  ],
  "improved_tasks": {
    "enter-text-dynamic": {
      "first_failure_step": 0,
      "first_success_step": 75,
      "improvement_steps": 75,
      "success_progression": [
        [
          0,
          0.0
        ],
        [
          5,
          0.0
        ],
        [
          25,
          0.0
        ],
        [
          45,
          0.0
        ],
        [
          75,
          1.0
        ],
        [
          85,
          0.0
        ]
      ],
      "total_attempts": 7,
      "total_successes": 1
    },
    "click-test-transfer": {
      "first_failure_step": 25,
      "first_success_step": 40,
      "improvement_steps": 15,
      "success_progression": [
        [
          0,
          1.0
        ],
        [
          10,
          1.0
        ],
        [
          25,
          0.8888888888888888
        ],
        [
          40,
          1.0
        ],
        [
          50,
          0.0
        ],
        [
          75,
          1.0
        ],
        [
          80,
          0.875
        ]
      ],
      "total_attempts": 22,
      "total_successes": 19
    },
    "click-shape": {
      "first_failure_step": 0,
      "first_success_step": 90,
      "improvement_steps": 90,
      "success_progression": [
        [
          0,
          0.0
        ],
        [
          5,
          0.0
        ],
        [
          15,
          0.0
        ],
        [
          30,
          0.0
        ],
        [
          50,
          0.0
        ],
        [
          55,
          0.0
        ],
        [
          60,
          0.0
        ],
        [
          70,
          0.0
        ],
        [
          75,
          0.0
        ],
        [
          85,
          0.0
        ],
        [
          90,
          1.0
        ]
      ],
      "total_attempts": 20,
      "total_successes": 1
    },
    "click-collapsible": {
      "first_failure_step": 0,
      "first_success_step": 10,
      "improvement_steps": 10,
      "success_progression": [
        [
          0,
          0.0
        ],
        [
          10,
          0.5
        ],
        [
          20,
          0.5
        ],
        [
          25,
          1.0
        ],
        [
          30,
          1.0
        ],
        [
          55,
          1.0
        ],
        [
          65,
          1.0
        ],
        [
          80,
          1.0
        ],
        [
          90,
          1.0
        ]
      ],
      "total_attempts": 17,
      "total_successes": 11
    },
    "click-checkboxes-transfer": {
      "first_failure_step": 5,
      "first_success_step": 35,
      "improvement_steps": 30,
      "success_progression": [
        [
          5,
          0.0
        ],
        [
          25,
          0.0
        ],
        [
          35,
          1.0
        ],
        [
          50,
          0.0
        ],
        [
          60,
          1.0
        ],
        [
          70,
          0.0
        ]
      ],
      "total_attempts": 6,
      "total_successes": 2
    },
    "click-checkboxes": {
      "first_failure_step": 5,
      "first_success_step": 10,
      "improvement_steps": 5,
      "success_progression": [
        [
          5,
          0.0
        ],
        [
          10,
          0.5
        ],
        [
          25,
          0.0
        ],
        [
          35,
          0.0
        ],
        [
          45,
          1.0
        ],
        [
          50,
          0.375
        ],
        [
          90,
          0.0
        ]
      ],
      "total_attempts": 15,
      "total_successes": 5
    },
    "click-scroll-list": {
      "first_failure_step": 5,
      "first_success_step": 45,
      "improvement_steps": 40,
      "success_progression": [
        [
          5,
          0.0
        ],
        [
          15,
          0.0
        ],
        [
          20,
          0.0
        ],
        [
          30,
          0.0
        ],
        [
          35,
          0.0
        ],
        [
          45,
          0.125
        ],
        [
          65,
          0.0
        ],
        [
          75,
          0.0
        ],
        [
          80,
          1.0
        ],
        [
          85,
          0.2222222222222222
        ],
        [
          90,
          0.4444444444444444
        ]
      ],
      "total_attempts": 34,
      "total_successes": 8
    },
    "simple-algebra": {
      "first_failure_step": 5,
      "first_success_step": 70,
      "improvement_steps": 65,
      "success_progression": [
        [
          5,
          0.0
        ],
        [
          15,
          0.0
        ],
        [
          20,
          0.0
        ],
        [
          25,
          0.0
        ],
        [
          50,
          0.0
        ],
        [
          55,
          0.0
        ],
        [
          60,
          0.0
        ],
        [
          65,
          0.0
        ],
        [
          70,
          0.5
        ],
        [
          80,
          0.0
        ],
        [
          90,
          0.0
        ]
      ],
      "total_attempts": 14,
      "total_successes": 1
    },
    "use-colorwheel-2": {
      "first_failure_step": 10,
      "first_success_step": 45,
      "improvement_steps": 35,
      "success_progression": [
        [
          5,
          1.0
        ],
        [
          10,
          0.0
        ],
        [
          15,
          0.0
        ],
        [
          25,
          0.0
        ],
        [
          35,
          0.0
        ],
        [
          45,
          1.0
        ],
        [
          60,
          0.0
        ],
        [
          70,
          0.0
        ],
        [
          75,
          0.0
        ],
        [
          90,
          0.0
        ]
      ],
      "total_attempts": 19,
      "total_successes": 3
    },
    "click-dialog": {
      "first_failure_step": 45,
      "first_success_step": 45,
      "improvement_steps": 0,
      "success_progression": [
        [
          5,
          1.0
        ],
        [
          40,
          1.0
        ],
        [
          45,
          0.7777777777777778
        ],
        [
          60,
          0.0
        ],
        [
          65,
          1.0
        ],
        [
          70,
          0.875
        ]
      ],
      "total_attempts": 22,
      "total_successes": 18
    },
    "click-link": {
      "first_failure_step": 5,
      "first_success_step": 85,
      "improvement_steps": 80,
      "success_progression": [
        [
          5,
          0.0
        ],
        [
          15,
          0.0
        ],
        [
          30,
          0.0
        ],
        [
          75,
          0.0
        ],
        [
          85,
          1.0
        ]
      ],
      "total_attempts": 5,
      "total_successes": 1
    },
    "identify-shape": {
      "first_failure_step": 5,
      "first_success_step": 5,
      "improvement_steps": 0,
      "success_progression": [
        [
          5,
          0.5
        ],
        [
          20,
          0.0
        ],
        [
          30,
          1.0
        ],
        [
          40,
          0.0
        ],
        [
          50,
          0.0
        ],
        [
          60,
          0.0
        ],
        [
          65,
          0.0
        ],
        [
          85,
          1.0
        ]
      ],
      "total_attempts": 19,
      "total_successes": 6
    },
    "click-menu-2": {
      "first_failure_step": 10,
      "first_success_step": 15,
      "improvement_steps": 5,
      "success_progression": [
        [
          10,
          0.0
        ],
        [
          15,
          1.0
        ],
        [
          25,
          0.0
        ],
        [
          45,
          0.0
        ],
        [
          60,
          1.0
        ],
        [
          70,
          0.4444444444444444
        ],
        [
          75,
          1.0
        ],
        [
          90,
          1.0
        ]
      ],
      "total_attempts": 16,
      "total_successes": 8
    },
    "click-collapsible-nodelay": {
      "first_failure_step": 10,
      "first_success_step": 30,
      "improvement_steps": 20,
      "success_progression": [
        [
          10,
          0.0
        ],
        [
          30,
          0.5
        ],
        [
          35,
          1.0
        ],
        [
          50,
          0.0
        ],
        [
          55,
          1.0
        ],
        [
          70,
          0.0
        ]
      ],
      "total_attempts": 7,
      "total_successes": 3
    },
    "click-button-sequence": {
      "first_failure_step": 65,
      "first_success_step": 80,
      "improvement_steps": 15,
      "success_progression": [
        [
          10,
          1.0
        ],
        [
          55,
          1.0
        ],
        [
          65,
          0.0
        ],
        [
          80,
          1.0
        ],
        [
          85,
          1.0
        ]
      ],
      "total_attempts": 12,
      "total_successes": 11
    },
    "click-tab-2-medium": {
      "first_failure_step": 10,
      "first_success_step": 30,
      "improvement_steps": 20,
      "success_progression": [
        [
          10,
          0.0
        ],
        [
          15,
          0.0
        ],
        [
          20,
          0.0
        ],
        [
          25,
          0.0
        ],
        [
          30,
          1.0
        ],
        [
          35,
          0.0
        ],
        [
          40,
          0.0
        ],
        [
          75,
          1.0
        ],
        [
          80,
          0.0
        ],
        [
          85,
          1.0
        ]
      ],
      "total_attempts": 15,
      "total_successes": 4
    },
    "click-menu": {
      "first_failure_step": 15,
      "first_success_step": 30,
      "improvement_steps": 15,
      "success_progression": [
        [
          15,
          0.0
        ],
        [
          20,
          0.0
        ],
        [
          30,
          1.0
        ],
        [
          45,
          0.0
        ],
        [
          60,
          0.2222222222222222
        ],
        [
          65,
          0.0
        ]
      ],
      "total_attempts": 15,
      "total_successes": 3
    },
    "click-color": {
      "first_failure_step": 20,
      "first_success_step": 70,
      "improvement_steps": 50,
      "success_progression": [
        [
          20,
          0.14285714285714285
        ],
        [
          35,
          0.0
        ],
        [
          45,
          0.0
        ],
        [
          60,
          0.0
        ],
        [
          65,
          0.0
        ],
        [
          70,
          0.5
        ],
        [
          75,
          0.0
        ]
      ],
      "total_attempts": 16,
      "total_successes": 2
    },
    "click-option": {
      "first_failure_step": 20,
      "first_success_step": 35,
      "improvement_steps": 15,
      "success_progression": [
        [
          20,
          0.0
        ],
        [
          30,
          0.0
        ],
        [
          35,
          0.375
        ],
        [
          45,
          0.0
        ],
        [
          65,
          0.5
        ],
        [
          70,
          0.5
        ],
        [
          80,
          0.0
        ],
        [
          85,
          0.5
        ],
        [
          90,
          0.0
        ]
      ],
      "total_attempts": 19,
      "total_successes": 6
    },
    "click-widget": {
      "first_failure_step": 30,
      "first_success_step": 50,
      "improvement_steps": 20,
      "success_progression": [
        [
          20,
          1.0
        ],
        [
          30,
          0.0
        ],
        [
          50,
          1.0
        ],
        [
          60,
          0.0
        ],
        [
          70,
          1.0
        ],
        [
          85,
          1.0
        ]
      ],
      "total_attempts": 7,
      "total_successes": 5
    },
    "count-sides": {
      "first_failure_step": 25,
      "first_success_step": 65,
      "improvement_steps": 40,
      "success_progression": [
        [
          25,
          0.0
        ],
        [
          30,
          0.0
        ],
        [
          50,
          0.0
        ],
        [
          60,
          0.0
        ],
        [
          65,
          0.5
        ],
        [
          70,
          0.0
        ],
        [
          80,
          0.0
        ]
      ],
      "total_attempts": 8,
      "total_successes": 1
    },
    "click-dialog-2": {
      "first_failure_step": 25,
      "first_success_step": 45,
      "improvement_steps": 20,
      "success_progression": [
        [
          25,
          0.0
        ],
        [
          45,
          1.0
        ],
        [
          65,
          1.0
        ],
        [
          80,
          1.0
        ],
        [
          85,
          0.0
        ]
      ],
      "total_attempts": 5,
      "total_successes": 3
    },
    "simple-arithmetic": {
      "first_failure_step": 25,
      "first_success_step": 55,
      "improvement_steps": 30,
      "success_progression": [
        [
          25,
          0.0
        ],
        [
          40,
          0.0
        ],
        [
          45,
          0.0
        ],
        [
          50,
          0.0
        ],
        [
          55,
          0.125
        ],
        [
          60,
          0.0
        ],
        [
          65,
          0.0
        ],
        [
          75,
          1.0
        ],
        [
          80,
          1.0
        ]
      ],
      "total_attempts": 29,
      "total_successes": 3
    }
  },
  "regressed_tasks": {
    "enter-text-dynamic": {
      "first_success_step": 75,
      "first_failure_step": 85,
      "regression_at_step": 85
    },
    "click-test-transfer": {
      "first_success_step": 0,
      "first_failure_step": 25,
      "regression_at_step": 25
    },
    "click-collapsible": {
      "first_success_step": 10,
      "first_failure_step": 20,
      "regression_at_step": 20
    },
    "click-checkboxes-transfer": {
      "first_success_step": 35,
      "first_failure_step": 50,
      "regression_at_step": 50
    },
    "click-checkboxes": {
      "first_success_step": 10,
      "first_failure_step": 25,
      "regression_at_step": 25
    },
    "click-scroll-list": {
      "first_success_step": 45,
      "first_failure_step": 45,
      "regression_at_step": 45
    },
    "simple-algebra": {
      "first_success_step": 70,
      "first_failure_step": 70,
      "regression_at_step": 70
    },
    "use-colorwheel-2": {
      "first_success_step": 5,
      "first_failure_step": 10,
      "regression_at_step": 10
    },
    "click-dialog": {
      "first_success_step": 5,
      "first_failure_step": 45,
      "regression_at_step": 45
    },
    "identify-shape": {
      "first_success_step": 5,
      "first_failure_step": 5,
      "regression_at_step": 5
    },
    "click-menu-2": {
      "first_success_step": 15,
      "first_failure_step": 25,
      "regression_at_step": 25
    },
    "click-collapsible-nodelay": {
      "first_success_step": 30,
      "first_failure_step": 50,
      "regression_at_step": 50
    },
    "click-button-sequence": {
      "first_success_step": 10,
      "first_failure_step": 65,
      "regression_at_step": 65
    },
    "click-tab-2-medium": {
      "first_success_step": 30,
      "first_failure_step": 35,
      "regression_at_step": 35
    },
    "click-menu": {
      "first_success_step": 30,
      "first_failure_step": 45,
      "regression_at_step": 45
    },
    "choose-list": {
      "first_success_step": 15,
      "first_failure_step": 20,
      "regression_at_step": 20
    },
    "click-color": {
      "first_success_step": 20,
      "first_failure_step": 20,
      "regression_at_step": 20
    },
    "click-option": {
      "first_success_step": 35,
      "first_failure_step": 35,
      "regression_at_step": 35
    },
    "click-widget": {
      "first_success_step": 20,
      "first_failure_step": 30,
      "regression_at_step": 30
    },
    "count-sides": {
      "first_success_step": 65,
      "first_failure_step": 65,
      "regression_at_step": 65
    },
    "click-dialog-2": {
      "first_success_step": 45,
      "first_failure_step": 85,
      "regression_at_step": 85
    },
    "simple-arithmetic": {
      "first_success_step": 55,
      "first_failure_step": 60,
      "regression_at_step": 60
    }
  },
  "task_type_stats": {
    "enter-text-dynamic": {
      "total_attempts": 7,
      "total_successes": 1,
      "overall_success_rate": 0.14285714285714285,
      "success_rate_by_step": {
        "0": 0.0,
        "5": 0.0,
        "25": 0.0,
        "45": 0.0,
        "75": 1.0,
        "85": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 85
    },
    "click-test-transfer": {
      "total_attempts": 22,
      "total_successes": 19,
      "overall_success_rate": 0.8636363636363636,
      "success_rate_by_step": {
        "0": 1.0,
        "10": 1.0,
        "25": 0.8888888888888888,
        "40": 1.0,
        "50": 0.0,
        "75": 1.0,
        "80": 0.875
      },
      "first_seen_step": 0,
      "last_seen_step": 80
    },
    "email-inbox-noscroll": {
      "total_attempts": 18,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "15": 0.0,
        "35": 0.0,
        "55": 0.0,
        "60": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 60
    },
    "enter-time": {
      "total_attempts": 15,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "15": 0.0,
        "40": 0.0,
        "65": 0.0,
        "70": 0.0,
        "75": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 75
    },
    "click-shape": {
      "total_attempts": 20,
      "total_successes": 1,
      "overall_success_rate": 0.05,
      "success_rate_by_step": {
        "0": 0.0,
        "5": 0.0,
        "15": 0.0,
        "30": 0.0,
        "50": 0.0,
        "55": 0.0,
        "60": 0.0,
        "70": 0.0,
        "75": 0.0,
        "85": 0.0,
        "90": 1.0
      },
      "first_seen_step": 0,
      "last_seen_step": 90
    },
    "find-word": {
      "total_attempts": 8,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "20": 0.0,
        "25": 0.0,
        "35": 0.0,
        "40": 0.0,
        "45": 0.0,
        "55": 0.0,
        "80": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 80
    },
    "text-editor": {
      "total_attempts": 8,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "25": 0.0,
        "30": 0.0,
        "45": 0.0,
        "50": 0.0,
        "80": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 80
    },
    "click-pie-nodelay": {
      "total_attempts": 10,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "15": 0.0,
        "25": 0.0,
        "30": 0.0,
        "35": 0.0,
        "50": 0.0,
        "55": 0.0,
        "65": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 65
    },
    "click-collapsible": {
      "total_attempts": 17,
      "total_successes": 11,
      "overall_success_rate": 0.6470588235294118,
      "success_rate_by_step": {
        "0": 0.0,
        "10": 0.5,
        "20": 0.5,
        "25": 1.0,
        "30": 1.0,
        "55": 1.0,
        "65": 1.0,
        "80": 1.0,
        "90": 1.0
      },
      "first_seen_step": 0,
      "last_seen_step": 90
    },
    "resize-textarea": {
      "total_attempts": 15,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "5": 0.0,
        "10": 0.0,
        "45": 0.0,
        "55": 0.0,
        "65": 0.0,
        "80": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 80
    },
    "copy-paste-2": {
      "total_attempts": 10,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "5": 0.0,
        "25": 0.0,
        "40": 0.0,
        "75": 0.0,
        "85": 0.0,
        "90": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 90
    },
    "scroll-text": {
      "total_attempts": 17,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "5": 0.0,
        "10": 0.0,
        "25": 0.0,
        "45": 0.0,
        "55": 0.0,
        "80": 0.0,
        "85": 0.0,
        "90": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 90
    },
    "use-autocomplete": {
      "total_attempts": 15,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "5": 0.0,
        "10": 0.0,
        "15": 0.0,
        "20": 0.0,
        "30": 0.0,
        "35": 0.0,
        "60": 0.0,
        "65": 0.0,
        "85": 0.0,
        "90": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 90
    },
    "read-table-2": {
      "total_attempts": 8,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "5": 0.0,
        "30": 0.0,
        "50": 0.0,
        "55": 0.0,
        "65": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 65
    },
    "choose-date-easy": {
      "total_attempts": 34,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "5": 0.0,
        "30": 0.0,
        "40": 0.0,
        "50": 0.0,
        "60": 0.0,
        "65": 0.0,
        "75": 0.0,
        "85": 0.0,
        "90": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 90
    },
    "email-inbox-delete": {
      "total_attempts": 14,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "20": 0.0,
        "35": 0.0,
        "45": 0.0,
        "55": 0.0,
        "70": 0.0,
        "85": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 85
    },
    "use-spinner": {
      "total_attempts": 17,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "15": 0.0,
        "40": 0.0,
        "55": 0.0,
        "60": 0.0,
        "75": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 75
    },
    "enter-text": {
      "total_attempts": 16,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "15": 0.0,
        "60": 0.0,
        "65": 0.0,
        "70": 0.0,
        "75": 0.0,
        "85": 0.0,
        "90": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 90
    },
    "highlight-text": {
      "total_attempts": 9,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "10": 0.0,
        "15": 0.0,
        "20": 0.0,
        "30": 0.0,
        "45": 0.0,
        "50": 0.0,
        "80": 0.0,
        "85": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 85
    },
    "copy-paste": {
      "total_attempts": 17,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "5": 0.0,
        "10": 0.0,
        "20": 0.0,
        "35": 0.0,
        "60": 0.0,
        "75": 0.0,
        "80": 0.0,
        "90": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 90
    },
    "email-inbox-reply": {
      "total_attempts": 7,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "10": 0.0,
        "25": 0.0,
        "50": 0.0,
        "65": 0.0,
        "75": 0.0,
        "85": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 85
    },
    "email-inbox-important": {
      "total_attempts": 13,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "10": 0.0,
        "35": 0.0,
        "40": 0.0,
        "90": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 90
    },
    "click-shades": {
      "total_attempts": 17,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "20": 0.0,
        "30": 0.0,
        "45": 0.0,
        "55": 0.0,
        "70": 0.0,
        "75": 0.0,
        "85": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 85
    },
    "enter-date": {
      "total_attempts": 11,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "0": 0.0,
        "20": 0.0,
        "50": 0.0,
        "80": 0.0
      },
      "first_seen_step": 0,
      "last_seen_step": 80
    },
    "click-tab-2-hard": {
      "total_attempts": 15,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "5": 0.0,
        "10": 0.0,
        "15": 0.0,
        "20": 0.0,
        "30": 0.0,
        "55": 0.0,
        "65": 0.0,
        "75": 0.0
      },
      "first_seen_step": 5,
      "last_seen_step": 75
    },
    "click-checkboxes-soft": {
      "total_attempts": 6,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "5": 0.0,
        "15": 0.0,
        "25": 0.0,
        "40": 0.0,
        "55": 0.0
      },
      "first_seen_step": 5,
      "last_seen_step": 55
    },
    "click-checkboxes-transfer": {
      "total_attempts": 6,
      "total_successes": 2,
      "overall_success_rate": 0.3333333333333333,
      "success_rate_by_step": {
        "5": 0.0,
        "25": 0.0,
        "35": 1.0,
        "50": 0.0,
        "60": 1.0,
        "70": 0.0
      },
      "first_seen_step": 5,
      "last_seen_step": 70
    },
    "click-checkboxes": {
      "total_attempts": 15,
      "total_successes": 5,
      "overall_success_rate": 0.3333333333333333,
      "success_rate_by_step": {
        "5": 0.0,
        "10": 0.5,
        "25": 0.0,
        "35": 0.0,
        "45": 1.0,
        "50": 0.375,
        "90": 0.0
      },
      "first_seen_step": 5,
      "last_seen_step": 90
    },
    "unicode-test": {
      "total_attempts": 25,
      "total_successes": 25,
      "overall_success_rate": 1.0,
      "success_rate_by_step": {
        "5": 1.0,
        "25": 1.0,
        "30": 1.0,
        "35": 1.0,
        "40": 1.0,
        "50": 1.0,
        "65": 1.0,
        "70": 1.0,
        "80": 1.0
      },
      "first_seen_step": 5,
      "last_seen_step": 80
    },
    "click-tab-2-easy": {
      "total_attempts": 16,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "5": 0.0,
        "10": 0.0,
        "35": 0.0,
        "50": 0.0,
        "70": 0.0,
        "75": 0.0
      },
      "first_seen_step": 5,
      "last_seen_step": 75
    },
    "click-scroll-list": {
      "total_attempts": 34,
      "total_successes": 8,
      "overall_success_rate": 0.23529411764705882,
      "success_rate_by_step": {
        "5": 0.0,
        "15": 0.0,
        "20": 0.0,
        "30": 0.0,
        "35": 0.0,
        "45": 0.125,
        "65": 0.0,
        "75": 0.0,
        "80": 1.0,
        "85": 0.2222222222222222,
        "90": 0.4444444444444444
      },
      "first_seen_step": 5,
      "last_seen_step": 90
    },
    "simple-algebra": {
      "total_attempts": 14,
      "total_successes": 1,
      "overall_success_rate": 0.07142857142857142,
      "success_rate_by_step": {
        "5": 0.0,
        "15": 0.0,
        "20": 0.0,
        "25": 0.0,
        "50": 0.0,
        "55": 0.0,
        "60": 0.0,
        "65": 0.0,
        "70": 0.5,
        "80": 0.0,
        "90": 0.0
      },
      "first_seen_step": 5,
      "last_seen_step": 90
    },
    "use-slider": {
      "total_attempts": 12,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "5": 0.0,
        "10": 0.0,
        "15": 0.0,
        "35": 0.0,
        "45": 0.0,
        "55": 0.0,
        "75": 0.0,
        "80": 0.0,
        "90": 0.0
      },
      "first_seen_step": 5,
      "last_seen_step": 90
    },
    "use-colorwheel-2": {
      "total_attempts": 19,
      "total_successes": 3,
      "overall_success_rate": 0.15789473684210525,
      "success_rate_by_step": {
        "5": 1.0,
        "10": 0.0,
        "15": 0.0,
        "25": 0.0,
        "35": 0.0,
        "45": 1.0,
        "60": 0.0,
        "70": 0.0,
        "75": 0.0,
        "90": 0.0
      },
      "first_seen_step": 5,
      "last_seen_step": 90
    },
    "click-dialog": {
      "total_attempts": 22,
      "total_successes": 18,
      "overall_success_rate": 0.8181818181818182,
      "success_rate_by_step": {
        "5": 1.0,
        "40": 1.0,
        "45": 0.7777777777777778,
        "60": 0.0,
        "65": 1.0,
        "70": 0.875
      },
      "first_seen_step": 5,
      "last_seen_step": 70
    },
    "click-link": {
      "total_attempts": 5,
      "total_successes": 1,
      "overall_success_rate": 0.2,
      "success_rate_by_step": {
        "5": 0.0,
        "15": 0.0,
        "30": 0.0,
        "75": 0.0,
        "85": 1.0
      },
      "first_seen_step": 5,
      "last_seen_step": 85
    },
    "click-checkboxes-large": {
      "total_attempts": 3,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "5": 0.0,
        "30": 0.0,
        "90": 0.0
      },
      "first_seen_step": 5,
      "last_seen_step": 90
    },
    "scroll-text-2": {
      "total_attempts": 5,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "5": 0.0,
        "25": 0.0,
        "65": 0.0,
        "80": 0.0
      },
      "first_seen_step": 5,
      "last_seen_step": 80
    },
    "enter-text-2": {
      "total_attempts": 14,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "5": 0.0,
        "20": 0.0,
        "30": 0.0,
        "40": 0.0,
        "50": 0.0,
        "70": 0.0,
        "90": 0.0
      },
      "first_seen_step": 5,
      "last_seen_step": 90
    },
    "click-tab-2": {
      "total_attempts": 6,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "5": 0.0,
        "25": 0.0,
        "30": 0.0,
        "35": 0.0,
        "60": 0.0
      },
      "first_seen_step": 5,
      "last_seen_step": 60
    },
    "identify-shape": {
      "total_attempts": 19,
      "total_successes": 6,
      "overall_success_rate": 0.3157894736842105,
      "success_rate_by_step": {
        "5": 0.5,
        "20": 0.0,
        "30": 1.0,
        "40": 0.0,
        "50": 0.0,
        "60": 0.0,
        "65": 0.0,
        "85": 1.0
      },
      "first_seen_step": 5,
      "last_seen_step": 85
    },
    "click-pie": {
      "total_attempts": 13,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "10": 0.0,
        "15": 0.0,
        "35": 0.0,
        "40": 0.0,
        "55": 0.0,
        "75": 0.0
      },
      "first_seen_step": 10,
      "last_seen_step": 75
    },
    "drag-single-shape": {
      "total_attempts": 12,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "10": 0.0,
        "25": 0.0,
        "40": 0.0,
        "45": 0.0,
        "50": 0.0,
        "75": 0.0,
        "80": 0.0,
        "85": 0.0,
        "90": 0.0
      },
      "first_seen_step": 10,
      "last_seen_step": 90
    },
    "click-test-2": {
      "total_attempts": 4,
      "total_successes": 4,
      "overall_success_rate": 1.0,
      "success_rate_by_step": {
        "10": 1.0,
        "50": 1.0,
        "60": 1.0,
        "90": 1.0
      },
      "first_seen_step": 10,
      "last_seen_step": 90
    },
    "guess-number": {
      "total_attempts": 16,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "10": 0.0,
        "15": 0.0,
        "30": 0.0,
        "50": 0.0,
        "75": 0.0,
        "80": 0.0,
        "85": 0.0,
        "90": 0.0
      },
      "first_seen_step": 10,
      "last_seen_step": 90
    },
    "highlight-text-2": {
      "total_attempts": 18,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "10": 0.0,
        "15": 0.0,
        "20": 0.0,
        "35": 0.0,
        "40": 0.0,
        "45": 0.0,
        "60": 0.0,
        "65": 0.0
      },
      "first_seen_step": 10,
      "last_seen_step": 65
    },
    "click-menu-2": {
      "total_attempts": 16,
      "total_successes": 8,
      "overall_success_rate": 0.5,
      "success_rate_by_step": {
        "10": 0.0,
        "15": 1.0,
        "25": 0.0,
        "45": 0.0,
        "60": 1.0,
        "70": 0.4444444444444444,
        "75": 1.0,
        "90": 1.0
      },
      "first_seen_step": 10,
      "last_seen_step": 90
    },
    "use-slider-2": {
      "total_attempts": 10,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "10": 0.0,
        "35": 0.0,
        "40": 0.0,
        "45": 0.0,
        "50": 0.0,
        "55": 0.0,
        "75": 0.0,
        "90": 0.0
      },
      "first_seen_step": 10,
      "last_seen_step": 90
    },
    "click-collapsible-2-nodelay": {
      "total_attempts": 19,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "10": 0.0,
        "15": 0.0,
        "20": 0.0,
        "70": 0.0,
        "90": 0.0
      },
      "first_seen_step": 10,
      "last_seen_step": 90
    },
    "text-transform": {
      "total_attempts": 27,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "10": 0.0,
        "30": 0.0,
        "35": 0.0,
        "40": 0.0,
        "65": 0.0,
        "80": 0.0
      },
      "first_seen_step": 10,
      "last_seen_step": 80
    },
    "click-collapsible-nodelay": {
      "total_attempts": 7,
      "total_successes": 3,
      "overall_success_rate": 0.42857142857142855,
      "success_rate_by_step": {
        "10": 0.0,
        "30": 0.5,
        "35": 1.0,
        "50": 0.0,
        "55": 1.0,
        "70": 0.0
      },
      "first_seen_step": 10,
      "last_seen_step": 70
    },
    "click-button-sequence": {
      "total_attempts": 12,
      "total_successes": 11,
      "overall_success_rate": 0.9166666666666666,
      "success_rate_by_step": {
        "10": 1.0,
        "55": 1.0,
        "65": 0.0,
        "80": 1.0,
        "85": 1.0
      },
      "first_seen_step": 10,
      "last_seen_step": 85
    },
    "click-tab": {
      "total_attempts": 8,
      "total_successes": 8,
      "overall_success_rate": 1.0,
      "success_rate_by_step": {
        "10": 1.0,
        "40": 1.0,
        "45": 1.0,
        "60": 1.0,
        "70": 1.0,
        "75": 1.0,
        "90": 1.0
      },
      "first_seen_step": 10,
      "last_seen_step": 90
    },
    "click-tab-2-medium": {
      "total_attempts": 15,
      "total_successes": 4,
      "overall_success_rate": 0.26666666666666666,
      "success_rate_by_step": {
        "10": 0.0,
        "15": 0.0,
        "20": 0.0,
        "25": 0.0,
        "30": 1.0,
        "35": 0.0,
        "40": 0.0,
        "75": 1.0,
        "80": 0.0,
        "85": 1.0
      },
      "first_seen_step": 10,
      "last_seen_step": 85
    },
    "click-collapsible-2": {
      "total_attempts": 30,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "10": 0.0,
        "15": 0.0,
        "20": 0.0,
        "25": 0.0,
        "30": 0.0,
        "45": 0.0,
        "50": 0.0,
        "65": 0.0,
        "75": 0.0,
        "80": 0.0,
        "85": 0.0
      },
      "first_seen_step": 10,
      "last_seen_step": 85
    },
    "focus-text": {
      "total_attempts": 32,
      "total_successes": 32,
      "overall_success_rate": 1.0,
      "success_rate_by_step": {
        "10": 1.0,
        "15": 1.0,
        "20": 1.0,
        "25": 1.0,
        "30": 1.0,
        "50": 1.0,
        "55": 1.0,
        "65": 1.0,
        "75": 1.0,
        "90": 1.0
      },
      "first_seen_step": 10,
      "last_seen_step": 90
    },
    "click-menu": {
      "total_attempts": 15,
      "total_successes": 3,
      "overall_success_rate": 0.2,
      "success_rate_by_step": {
        "15": 0.0,
        "20": 0.0,
        "30": 1.0,
        "45": 0.0,
        "60": 0.2222222222222222,
        "65": 0.0
      },
      "first_seen_step": 15,
      "last_seen_step": 65
    },
    "email-inbox-star-reply": {
      "total_attempts": 10,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "15": 0.0,
        "40": 0.0,
        "50": 0.0,
        "55": 0.0
      },
      "first_seen_step": 15,
      "last_seen_step": 55
    },
    "read-table": {
      "total_attempts": 14,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "15": 0.0,
        "25": 0.0,
        "60": 0.0,
        "65": 0.0,
        "70": 0.0,
        "80": 0.0,
        "90": 0.0
      },
      "first_seen_step": 15,
      "last_seen_step": 90
    },
    "choose-list": {
      "total_attempts": 7,
      "total_successes": 1,
      "overall_success_rate": 0.14285714285714285,
      "success_rate_by_step": {
        "15": 1.0,
        "20": 0.0,
        "30": 0.0,
        "35": 0.0,
        "55": 0.0,
        "60": 0.0,
        "85": 0.0
      },
      "first_seen_step": 15,
      "last_seen_step": 85
    },
    "click-button": {
      "total_attempts": 4,
      "total_successes": 4,
      "overall_success_rate": 1.0,
      "success_rate_by_step": {
        "15": 1.0,
        "20": 1.0,
        "35": 1.0,
        "65": 1.0
      },
      "first_seen_step": 15,
      "last_seen_step": 65
    },
    "drag-circle": {
      "total_attempts": 14,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "15": 0.0,
        "25": 0.0,
        "35": 0.0,
        "40": 0.0,
        "45": 0.0,
        "60": 0.0,
        "70": 0.0,
        "75": 0.0,
        "80": 0.0
      },
      "first_seen_step": 15,
      "last_seen_step": 80
    },
    "click-color": {
      "total_attempts": 16,
      "total_successes": 2,
      "overall_success_rate": 0.125,
      "success_rate_by_step": {
        "20": 0.14285714285714285,
        "35": 0.0,
        "45": 0.0,
        "60": 0.0,
        "65": 0.0,
        "70": 0.5,
        "75": 0.0
      },
      "first_seen_step": 20,
      "last_seen_step": 75
    },
    "drag-box": {
      "total_attempts": 16,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "20": 0.0,
        "40": 0.0,
        "45": 0.0,
        "55": 0.0,
        "60": 0.0,
        "70": 0.0
      },
      "first_seen_step": 20,
      "last_seen_step": 70
    },
    "click-option": {
      "total_attempts": 19,
      "total_successes": 6,
      "overall_success_rate": 0.3157894736842105,
      "success_rate_by_step": {
        "20": 0.0,
        "30": 0.0,
        "35": 0.375,
        "45": 0.0,
        "65": 0.5,
        "70": 0.5,
        "80": 0.0,
        "85": 0.5,
        "90": 0.0
      },
      "first_seen_step": 20,
      "last_seen_step": 90
    },
    "click-widget": {
      "total_attempts": 7,
      "total_successes": 5,
      "overall_success_rate": 0.7142857142857143,
      "success_rate_by_step": {
        "20": 1.0,
        "30": 0.0,
        "50": 1.0,
        "60": 0.0,
        "70": 1.0,
        "85": 1.0
      },
      "first_seen_step": 20,
      "last_seen_step": 85
    },
    "enter-password": {
      "total_attempts": 12,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "20": 0.0,
        "30": 0.0,
        "45": 0.0,
        "60": 0.0,
        "70": 0.0,
        "75": 0.0
      },
      "first_seen_step": 20,
      "last_seen_step": 75
    },
    "use-colorwheel": {
      "total_attempts": 8,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "20": 0.0,
        "35": 0.0,
        "40": 0.0,
        "45": 0.0,
        "50": 0.0,
        "55": 0.0,
        "70": 0.0,
        "90": 0.0
      },
      "first_seen_step": 20,
      "last_seen_step": 90
    },
    "choose-date-medium": {
      "total_attempts": 14,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "20": 0.0,
        "25": 0.0,
        "30": 0.0,
        "40": 0.0,
        "50": 0.0,
        "85": 0.0
      },
      "first_seen_step": 20,
      "last_seen_step": 85
    },
    "count-sides": {
      "total_attempts": 8,
      "total_successes": 1,
      "overall_success_rate": 0.125,
      "success_rate_by_step": {
        "25": 0.0,
        "30": 0.0,
        "50": 0.0,
        "60": 0.0,
        "65": 0.5,
        "70": 0.0,
        "80": 0.0
      },
      "first_seen_step": 25,
      "last_seen_step": 80
    },
    "count-shape": {
      "total_attempts": 7,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "25": 0.0,
        "40": 0.0,
        "55": 0.0,
        "60": 0.0,
        "65": 0.0,
        "80": 0.0
      },
      "first_seen_step": 25,
      "last_seen_step": 80
    },
    "click-dialog-2": {
      "total_attempts": 5,
      "total_successes": 3,
      "overall_success_rate": 0.6,
      "success_rate_by_step": {
        "25": 0.0,
        "45": 1.0,
        "65": 1.0,
        "80": 1.0,
        "85": 0.0
      },
      "first_seen_step": 25,
      "last_seen_step": 85
    },
    "simple-arithmetic": {
      "total_attempts": 29,
      "total_successes": 3,
      "overall_success_rate": 0.10344827586206896,
      "success_rate_by_step": {
        "25": 0.0,
        "40": 0.0,
        "45": 0.0,
        "50": 0.0,
        "55": 0.125,
        "60": 0.0,
        "65": 0.0,
        "75": 1.0,
        "80": 1.0
      },
      "first_seen_step": 25,
      "last_seen_step": 80
    },
    "focus-text-2": {
      "total_attempts": 6,
      "total_successes": 6,
      "overall_success_rate": 1.0,
      "success_rate_by_step": {
        "35": 1.0,
        "40": 1.0,
        "45": 1.0,
        "60": 1.0,
        "80": 1.0,
        "85": 1.0
      },
      "first_seen_step": 35,
      "last_seen_step": 85
    },
    "use-autocomplete-nodelay": {
      "total_attempts": 6,
      "total_successes": 0,
      "overall_success_rate": 0.0,
      "success_rate_by_step": {
        "40": 0.0,
        "45": 0.0,
        "55": 0.0,
        "65": 0.0,
        "80": 0.0,
        "85": 0.0
      },
      "first_seen_step": 40,
      "last_seen_step": 85
    },
    "click-test": {
      "total_attempts": 3,
      "total_successes": 3,
      "overall_success_rate": 1.0,
      "success_rate_by_step": {
        "45": 1.0,
        "75": 1.0,
        "85": 1.0
      },
      "first_seen_step": 45,
      "last_seen_step": 85
    }
  },
  "noop_analysis": {
    "enter-text-dynamic": {
      "total_attempts": 7,
      "all_noop_count": 0,
      "noop_rate": 0.0
    },
    "click-test-transfer": {
      "total_attempts": 22,
      "all_noop_count": 0,
      "noop_rate": 0.0
    },
    "email-inbox-noscroll": {
      "total_attempts": 18,
      "all_noop_count": 12,
      "noop_rate": 0.6666666666666666
    },
    "enter-time": {
      "total_attempts": 15,
      "all_noop_count": 14,
      "noop_rate": 0.9333333333333333
    },
    "click-shape": {
      "total_attempts": 20,
      "all_noop_count": 13,
      "noop_rate": 0.65
    },
    "find-word": {
      "total_attempts": 8,
      "all_noop_count": 7,
      "noop_rate": 0.875
    },
    "text-editor": {
      "total_attempts": 8,
      "all_noop_count": 8,
      "noop_rate": 1.0
    },
    "click-pie-nodelay": {
      "total_attempts": 10,
      "all_noop_count": 8,
      "noop_rate": 0.8
    },
    "click-collapsible": {
      "total_attempts": 17,
      "all_noop_count": 0,
      "noop_rate": 0.0
    },
    "resize-textarea": {
      "total_attempts": 15,
      "all_noop_count": 15,
      "noop_rate": 1.0
    },
    "copy-paste-2": {
      "total_attempts": 10,
      "all_noop_count": 10,
      "noop_rate": 1.0
    },
    "scroll-text": {
      "total_attempts": 17,
      "all_noop_count": 16,
      "noop_rate": 0.9411764705882353
    },
    "use-autocomplete": {
      "total_attempts": 15,
      "all_noop_count": 9,
      "noop_rate": 0.6
    },
    "read-table-2": {
      "total_attempts": 8,
      "all_noop_count": 8,
      "noop_rate": 1.0
    },
    "choose-date-easy": {
      "total_attempts": 34,
      "all_noop_count": 2,
      "noop_rate": 0.058823529411764705
    },
    "email-inbox-delete": {
      "total_attempts": 14,
      "all_noop_count": 13,
      "noop_rate": 0.9285714285714286
    },
    "use-spinner": {
      "total_attempts": 17,
      "all_noop_count": 13,
      "noop_rate": 0.7647058823529411
    },
    "enter-text": {
      "total_attempts": 16,
      "all_noop_count": 0,
      "noop_rate": 0.0
    },
    "highlight-text": {
      "total_attempts": 9,
      "all_noop_count": 9,
      "noop_rate": 1.0
    },
    "copy-paste": {
      "total_attempts": 17,
      "all_noop_count": 16,
      "noop_rate": 0.9411764705882353
    },
    "email-inbox-reply": {
      "total_attempts": 7,
      "all_noop_count": 4,
      "noop_rate": 0.5714285714285714
    },
    "email-inbox-important": {
      "total_attempts": 13,
      "all_noop_count": 12,
      "noop_rate": 0.9230769230769231
    },
    "click-shades": {
      "total_attempts": 17,
      "all_noop_count": 16,
      "noop_rate": 0.9411764705882353
    },
    "enter-date": {
      "total_attempts": 11,
      "all_noop_count": 10,
      "noop_rate": 0.9090909090909091
    },
    "click-tab-2-hard": {
      "total_attempts": 15,
      "all_noop_count": 9,
      "noop_rate": 0.6
    },
    "click-checkboxes-soft": {
      "total_attempts": 6,
      "all_noop_count": 5,
      "noop_rate": 0.8333333333333334
    },
    "click-checkboxes-transfer": {
      "total_attempts": 6,
      "all_noop_count": 2,
      "noop_rate": 0.3333333333333333
    },
    "click-checkboxes": {
      "total_attempts": 15,
      "all_noop_count": 4,
      "noop_rate": 0.26666666666666666
    },
    "unicode-test": {
      "total_attempts": 25,
      "all_noop_count": 0,
      "noop_rate": 0.0
    },
    "click-tab-2-easy": {
      "total_attempts": 16,
      "all_noop_count": 14,
      "noop_rate": 0.875
    },
    "click-scroll-list": {
      "total_attempts": 34,
      "all_noop_count": 15,
      "noop_rate": 0.4411764705882353
    },
    "simple-algebra": {
      "total_attempts": 14,
      "all_noop_count": 5,
      "noop_rate": 0.35714285714285715
    },
    "use-slider": {
      "total_attempts": 12,
      "all_noop_count": 12,
      "noop_rate": 1.0
    },
    "use-colorwheel-2": {
      "total_attempts": 19,
      "all_noop_count": 16,
      "noop_rate": 0.8421052631578947
    },
    "click-dialog": {
      "total_attempts": 22,
      "all_noop_count": 3,
      "noop_rate": 0.13636363636363635
    },
    "click-link": {
      "total_attempts": 5,
      "all_noop_count": 0,
      "noop_rate": 0.0
    },
    "click-checkboxes-large": {
      "total_attempts": 3,
      "all_noop_count": 3,
      "noop_rate": 1.0
    },
    "scroll-text-2": {
      "total_attempts": 5,
      "all_noop_count": 4,
      "noop_rate": 0.8
    },
    "enter-text-2": {
      "total_attempts": 14,
      "all_noop_count": 0,
      "noop_rate": 0.0
    },
    "click-tab-2": {
      "total_attempts": 6,
      "all_noop_count": 5,
      "noop_rate": 0.8333333333333334
    },
    "identify-shape": {
      "total_attempts": 19,
      "all_noop_count": 12,
      "noop_rate": 0.631578947368421
    },
    "click-pie": {
      "total_attempts": 13,
      "all_noop_count": 4,
      "noop_rate": 0.3076923076923077
    },
    "drag-single-shape": {
      "total_attempts": 12,
      "all_noop_count": 12,
      "noop_rate": 1.0
    },
    "click-test-2": {
      "total_attempts": 4,
      "all_noop_count": 0,
      "noop_rate": 0.0
    },
    "guess-number": {
      "total_attempts": 16,
      "all_noop_count": 16,
      "noop_rate": 1.0
    },
    "highlight-text-2": {
      "total_attempts": 18,
      "all_noop_count": 17,
      "noop_rate": 0.9444444444444444
    },
    "click-menu-2": {
      "total_attempts": 16,
      "all_noop_count": 0,
      "noop_rate": 0.0
    },
    "use-slider-2": {
      "total_attempts": 10,
      "all_noop_count": 10,
      "noop_rate": 1.0
    },
    "click-collapsible-2-nodelay": {
      "total_attempts": 19,
      "all_noop_count": 8,
      "noop_rate": 0.42105263157894735
    },
    "text-transform": {
      "total_attempts": 27,
      "all_noop_count": 21,
      "noop_rate": 0.7777777777777778
    },
    "click-collapsible-nodelay": {
      "total_attempts": 7,
      "all_noop_count": 0,
      "noop_rate": 0.0
    },
    "click-button-sequence": {
      "total_attempts": 12,
      "all_noop_count": 0,
      "noop_rate": 0.0
    },
    "click-tab": {
      "total_attempts": 8,
      "all_noop_count": 0,
      "noop_rate": 0.0
    },
    "click-tab-2-medium": {
      "total_attempts": 15,
      "all_noop_count": 2,
      "noop_rate": 0.13333333333333333
    },
    "click-collapsible-2": {
      "total_attempts": 30,
      "all_noop_count": 17,
      "noop_rate": 0.5666666666666667
    },
    "focus-text": {
      "total_attempts": 32,
      "all_noop_count": 0,
      "noop_rate": 0.0
    },
    "click-menu": {
      "total_attempts": 15,
      "all_noop_count": 5,
      "noop_rate": 0.3333333333333333
    },
    "email-inbox-star-reply": {
      "total_attempts": 10,
      "all_noop_count": 4,
      "noop_rate": 0.4
    },
    "read-table": {
      "total_attempts": 14,
      "all_noop_count": 8,
      "noop_rate": 0.5714285714285714
    },
    "choose-list": {
      "total_attempts": 7,
      "all_noop_count": 0,
      "noop_rate": 0.0
    },
    "click-button": {
      "total_attempts": 4,
      "all_noop_count": 0,
      "noop_rate": 0.0
    },
    "drag-circle": {
      "total_attempts": 14,
      "all_noop_count": 11,
      "noop_rate": 0.7857142857142857
    },
    "click-color": {
      "total_attempts": 16,
      "all_noop_count": 11,
      "noop_rate": 0.6875
    },
    "drag-box": {
      "total_attempts": 16,
      "all_noop_count": 15,
      "noop_rate": 0.9375
    },
    "click-option": {
      "total_attempts": 19,
      "all_noop_count": 0,
      "noop_rate": 0.0
    },
    "click-widget": {
      "total_attempts": 7,
      "all_noop_count": 2,
      "noop_rate": 0.2857142857142857
    },
    "enter-password": {
      "total_attempts": 12,
      "all_noop_count": 12,
      "noop_rate": 1.0
    },
    "use-colorwheel": {
      "total_attempts": 8,
      "all_noop_count": 8,
      "noop_rate": 1.0
    },
    "choose-date-medium": {
      "total_attempts": 14,
      "all_noop_count": 1,
      "noop_rate": 0.07142857142857142
    },
    "count-sides": {
      "total_attempts": 8,
      "all_noop_count": 3,
      "noop_rate": 0.375
    },
    "count-shape": {
      "total_attempts": 7,
      "all_noop_count": 7,
      "noop_rate": 1.0
    },
    "click-dialog-2": {
      "total_attempts": 5,
      "all_noop_count": 2,
      "noop_rate": 0.4
    },
    "simple-arithmetic": {
      "total_attempts": 29,
      "all_noop_count": 2,
      "noop_rate": 0.06896551724137931
    },
    "focus-text-2": {
      "total_attempts": 6,
      "all_noop_count": 0,
      "noop_rate": 0.0
    },
    "use-autocomplete-nodelay": {
      "total_attempts": 6,
      "all_noop_count": 1,
      "noop_rate": 0.16666666666666666
    },
    "click-test": {
      "total_attempts": 3,
      "all_noop_count": 0,
      "noop_rate": 0.0
    }
  }
}